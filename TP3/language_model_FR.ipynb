{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30626a2",
   "metadata": {},
   "source": [
    "# NLP Lab : Modèles de langue\n",
    "\n",
    "Dans ce tp, nous allons constuire les briques principales du modèle GPT2 et entrainer un petit modèle sur des poèmes de Victor Hugo. \n",
    "\n",
    "Les questions sont posées dans ce notebook, puis pour executer l'entrainement, il faudra modifier le ficher `gpt_single_head.py` aussi disponible dans le reposository git.\n",
    "\n",
    "\n",
    "## Données\n",
    "\n",
    "Les données d'entrainement sont un recueil de poèmes de Victor Hugo issu du site [gutenberg.org](https://www.gutenberg.org/). Elles sont disponibles dans le répertoire `data`.\n",
    "\n",
    "Afin de réduire la complexité du modèle, nous allons modéliser le texte au niveau caractère. Les modèles de language modélisent généralement des séquences de sous-mots en utilisant des [tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) (BPE, SentencePiece, WordPiece)\n",
    "\n",
    "Questions :\n",
    ">* En utilisant [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), afficher le nombre de caractères différents dans le texte et la fréquence de chaque caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4d4ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the file: 285222\n",
      "Number of character in counter: 285222\n",
      "101 different characters\n",
      "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "with open('data/hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Number of characters in the file: {len(text)}')\n",
    "##  YOUR CODE HERE\n",
    "\n",
    "counter = collections.Counter()\n",
    "\n",
    "for word in text:\n",
    "    counter[word]+=1\n",
    "\n",
    "chars=counter.keys()\n",
    "###\n",
    "\n",
    "print (f'Number of character in counter: {sum(counter.values())}')\n",
    "print (f'{len(chars)} different characters')\n",
    "print (counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b661f",
   "metadata": {},
   "source": [
    "### Encodage / décodage\n",
    "Afin de transformer le texte en vecteur pour le réseau de neurones, il faut encoder chaque caractère avec un entier. Les fonctions suivantes opérent l'encodage et le décodage des caractères :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d9c974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: transform a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: transform a list of integers into a string\n",
    "\n",
    "\n",
    "# test that your encoder/decoder is coherent\n",
    "testString = \"\\nDemain, dès l'aube\"\n",
    "assert decode(encode (testString)) ==  testString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a633d",
   "metadata": {},
   "source": [
    "### Découpage Train/Validation\n",
    "\n",
    "L'objectif étant de prédire des poèmes, il ne faut pas mélanger les lignes aléatoirement. Il faut garder l'ordre des lignes dans le texte et uniquement prendre les premiers 90% pour entrainer et les 10% restants pour contrôler l'apprentissage. \n",
    "\n",
    "Questions :\n",
    "> * Découper en `train_data` (90%) et `val_data` (10%) en utilisant du slicing sur data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf5b7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "## YOUR CODE HERE\n",
    "\n",
    "train_data = data[:int(data.shape[0]*0.9)]\n",
    "val_data = data[train_data.shape[0]:]\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa498280",
   "metadata": {},
   "source": [
    "### Contexte\n",
    "\n",
    "Le modèle de langue possède comme paramètre la taille maximale du contexte à considérer pour faire la prédiction du prochain caractère. Ce contexte est appelé `block_size`. Les données d'apprentissage sont donc des séquences de caractères consécutifs, issues de l'ensemble d'entraînenement tirées aléatoirement et de longueur `block_size`.\n",
    "\n",
    "Si le caractère de début de la séquence est `i`, la séquence de contexte est donc :\n",
    "``` x = data[i:i+block_size]```\n",
    "et la valeur à prédire à chaque position dans le contexte est le caractère suivant :\n",
    "```y = [data[i+1:i+block_size+1]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97a262bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([190166])\n",
      "context is > < target is >l<\n",
      "context is > l< target is >u<\n",
      "context is > lu< target is >i<\n",
      "context is > lui< target is > <\n",
      "context is > lui < target is >l<\n",
      "context is > lui l< target is >'<\n",
      "context is > lui l'< target is >a<\n",
      "context is > lui l'a< target is >m<\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "i  = torch.randint(len(data) - block_size, (1,))\n",
    "print (i)\n",
    "x = train_data[i:i+block_size]\n",
    "y = train_data[i+1:i+1+block_size]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print (f'context is >{decode(context.tolist())}< target is >{decode([target.tolist()])}<')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81464",
   "metadata": {},
   "source": [
    "### Définition des batchs\n",
    "\n",
    "Les batchs d'entrainement sont constitués de plusieurs séquences de caractères tirées aléatoirement dans `train_data`. Pour choisir aléatoirement une séquence à mettre dans le batch, il faut tirer aléatoirement un point de départ dans `train_data` et extraire les `block_size` caractères suivants. Lors du tirage du point de départ, faire attention à laisser suffisamment de caractères après le point de départ pour avoir une séquence de `block_size` caractères.\n",
    "\n",
    "Questions :\n",
    "> * Créer les batchs `x` en tirant `batch_size` séquences de longeur `block_size` à  partir d'un point `i` tiré aléatoirement. Empiler les exemples avec `torch.stack`.\n",
    "> * Créer les batchs `y` en ajoutant le caractère suivant la séquence `x`. Empiler les exemples avec `torch.stack`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9be91965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35., 14., 37., 35., 32., 29., 53., 65.],\n",
       "         [39., 32., 44., 14., 53., 52., 14., 44.],\n",
       "         [32., 50., 62., 35., 43., 42.,  3., 14.],\n",
       "         [44., 43., 14., 53., 35., 43., 14., 57.]]),\n",
       " tensor([[14., 37., 35., 32., 29., 53., 65.,  3.],\n",
       "         [32., 44., 14., 53., 52., 14., 44., 35.],\n",
       "         [50., 62., 35., 43., 42.,  3., 14., 14.],\n",
       "         [43., 14., 53., 35., 43., 14., 57., 53.]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "batch_size = 4\n",
    "torch.manual_seed(2023)\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ## YOUR CODE HERE\n",
    "    # select batch_size starting points in the data, store them in a list called starting_points\n",
    "    starting_points = list(torch.randint(high=len(data)- block_size, size=(batch_size,)))\n",
    "    # x is the sequence of integer starting at each starting point and of length block_size\n",
    "    # y is the character after each starting position\n",
    "    x = torch.zeros(1,block_size)\n",
    "    y = torch.zeros(1,block_size)\n",
    "    for i in range(batch_size):\n",
    "        u = train_data[int(starting_points[i]):int(starting_points[i])+block_size].reshape(1,block_size)\n",
    "        v = train_data[int(starting_points[i])+1:int(starting_points[i])+1+block_size].reshape(1,block_size)\n",
    "        x = torch.vstack((x,u))\n",
    "        y = torch.vstack((y,v))\n",
    "    x = x[1:,:]\n",
    "    y = y[1:,:]\n",
    "    ### \n",
    "    # send data and target to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507313b",
   "metadata": {},
   "source": [
    "### Premier modèle : un bigramme \n",
    "\n",
    "Le premier modèle que nous allons implémenter est un modèle bigramme. Il prédit le caractère suivant uniquement en fonction du caractère courant. Il est possible de stocker ce modèle dans une simple matrice : pour chaque caractère (en ligne), on stocke la distribution de probabilités sur l'ensemble des caractères suivants (en colonne). On peut donc le stocker dans une simple couche [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
    "\n",
    "Questions :\n",
    "> * Dans le constructeur, définir une couche Embedding de taille `vocab_size` par `vocab_size`.\n",
    "> * Dans forward, appliquer la couche d'embedding au batch de idx (`x`).\n",
    "> * Dans forward, définir la loss comme la `cross_entropy` entre la prédiction et target (`y`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b7a7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# use a gpu if we have one\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # we use a simple vocab_size times vocab_size tensor to store the probabilities \n",
    "        # of each token given a single token as context in nn.Embedding\n",
    "        # YOUR CODE HERE\n",
    "        self.embedding = nn.Embedding(vocab_size,vocab_size)\n",
    "        ## \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (Batch,Time) tensor of integers\n",
    "        # YOUR CODE HERE\n",
    "        idx = idx.clone().detach().long()\n",
    "\n",
    "        logits = self.embedding(idx)\n",
    "\n",
    "        ## \n",
    "   \n",
    "        # don't compute loss if we don't have targets\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # change the shape of the logits and target to match what is needed for CrossEntropyLoss\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "            Batch, Time, Channels = logits.shape\n",
    "            logits = logits.view(Batch*Time, Channels).float()\n",
    "            targets = targets.view(Batch*Time).long()\n",
    "            \n",
    "            # negative log likelihood between prediction and target\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(logits,targets)\n",
    "\n",
    "            ## \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "vocab_size=101\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# send the model to device\n",
    "m = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3cffb",
   "metadata": {},
   "source": [
    "#### Modèle avant entraînement\n",
    "Le modèle n'a pas encore été entrainé, il est juste initialisé, mais on peut calculer la loss sur un batch aléatoire. Les poids étant initialisés avec une distribution normale N(0,1) sur chaque dimension, la loss attendue après l'initialisation devrait être proche de `-ln(1/vocab_size)` (l'entropie est maximale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62343cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 101])\n",
      "loss attendue 4.61512051684126\n",
      "loss calculée 5.3546857833862305\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "print (logits.shape)\n",
    "print (f'loss attendue {-math.log(1.0/101)}')\n",
    "print (f'loss calculée {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d44b50",
   "metadata": {},
   "source": [
    "Pour utiliser le modèle en prédiction, il faut lui fournir un premier caractère pour amorcer la séquence : c'est le prompt. Dans notre cas, on peut initialiser la génération avec le caractère de retour à la ligne pour débuter une nouvelle phrase.\n",
    "\n",
    "Questions :\n",
    "> * Créer un prompt avec un tenseur de taille (1,1) contenant l'entier correspondant au caractère `\\n`.\n",
    "> * Générer une séquence de caractères de taille 100 à partir de ce prompt avec les fonctions `m.generate` et `decode`.\n",
    "> * Comment est la phrase générée ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "320d3ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (encode(['\\n']))\n",
    "## YOUR CODE HERE\n",
    "\n",
    "prompt = torch.tensor([encode(['\\n'])])\n",
    "\n",
    "\n",
    "prompt.shape\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630d8c3",
   "metadata": {},
   "source": [
    "### Entrainement\n",
    "\n",
    "Pour l'entrainement, nous utilisons un optimiseur [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) avec un learning rate de 1e-3. Une itération d'apprentissage consiste en \n",
    "- générer un batch\n",
    "- appliquer le réseau de neurones (forward) et calculer la loss (`model(xb, yb)`)\n",
    "- calculer le gradient (après avoir remis à zero le gradient cumulé) (`loss.backward()`)\n",
    "- mettre à jour les paramètres (`optimizer.step()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05831b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.0675, val loss 5.1427\n",
      "step 10: train loss 5.1048, val loss 5.1271\n",
      "step 20: train loss 5.0806, val loss 5.1517\n",
      "step 30: train loss 5.0686, val loss 5.0700\n",
      "step 40: train loss 5.0815, val loss 5.1055\n",
      "step 50: train loss 5.0608, val loss 5.0947\n",
      "step 60: train loss 5.0171, val loss 5.1370\n",
      "step 70: train loss 5.1344, val loss 5.0176\n",
      "step 80: train loss 5.0600, val loss 5.1014\n",
      "step 90: train loss 5.0345, val loss 5.0418\n",
      "step 100: train loss 5.0460, val loss 5.0424\n",
      "step 110: train loss 5.0475, val loss 5.0219\n",
      "step 120: train loss 5.0118, val loss 5.0610\n",
      "step 130: train loss 4.9687, val loss 5.0528\n",
      "step 140: train loss 5.0220, val loss 5.0231\n",
      "step 150: train loss 5.0061, val loss 5.0494\n",
      "step 160: train loss 4.9233, val loss 5.0424\n",
      "step 170: train loss 5.0106, val loss 4.9616\n",
      "step 180: train loss 4.9061, val loss 4.9335\n",
      "step 190: train loss 4.9040, val loss 4.9281\n",
      "step 200: train loss 4.8412, val loss 4.9660\n",
      "step 210: train loss 4.9297, val loss 4.9355\n",
      "step 220: train loss 4.9349, val loss 4.9472\n",
      "step 230: train loss 4.9095, val loss 4.9060\n",
      "step 240: train loss 4.9267, val loss 4.9378\n",
      "step 250: train loss 4.8866, val loss 4.8888\n",
      "step 260: train loss 4.8551, val loss 4.9392\n",
      "step 270: train loss 4.9023, val loss 4.8130\n",
      "step 280: train loss 4.8586, val loss 4.9038\n",
      "step 290: train loss 4.9190, val loss 4.9710\n",
      "step 300: train loss 4.8749, val loss 4.8906\n",
      "step 310: train loss 4.8365, val loss 4.8974\n",
      "step 320: train loss 4.8365, val loss 4.8754\n",
      "step 330: train loss 4.8481, val loss 4.8691\n",
      "step 340: train loss 4.8069, val loss 4.8574\n",
      "step 350: train loss 4.8277, val loss 4.9178\n",
      "step 360: train loss 4.8586, val loss 4.8008\n",
      "step 370: train loss 4.8158, val loss 4.8139\n",
      "step 380: train loss 4.8453, val loss 4.7700\n",
      "step 390: train loss 4.8249, val loss 4.8645\n",
      "step 400: train loss 4.8026, val loss 4.7394\n",
      "step 410: train loss 4.7200, val loss 4.7951\n",
      "step 420: train loss 4.7412, val loss 4.7917\n",
      "step 430: train loss 4.7679, val loss 4.7489\n",
      "step 440: train loss 4.7091, val loss 4.7794\n",
      "step 450: train loss 4.7130, val loss 4.7379\n",
      "step 460: train loss 4.7127, val loss 4.7346\n",
      "step 470: train loss 4.7253, val loss 4.6506\n",
      "step 480: train loss 4.6798, val loss 4.7593\n",
      "step 490: train loss 4.6890, val loss 4.7413\n",
      "step 500: train loss 4.6724, val loss 4.6804\n",
      "step 510: train loss 4.6205, val loss 4.7241\n",
      "step 520: train loss 4.6589, val loss 4.7111\n",
      "step 530: train loss 4.6273, val loss 4.7123\n",
      "step 540: train loss 4.6364, val loss 4.6797\n",
      "step 550: train loss 4.6699, val loss 4.6776\n",
      "step 560: train loss 4.6777, val loss 4.6580\n",
      "step 570: train loss 4.7212, val loss 4.6855\n",
      "step 580: train loss 4.5883, val loss 4.6118\n",
      "step 590: train loss 4.5941, val loss 4.5527\n",
      "step 600: train loss 4.6145, val loss 4.6205\n",
      "step 610: train loss 4.6394, val loss 4.6521\n",
      "step 620: train loss 4.6079, val loss 4.6410\n",
      "step 630: train loss 4.5575, val loss 4.5762\n",
      "step 640: train loss 4.5597, val loss 4.6641\n",
      "step 650: train loss 4.4944, val loss 4.6189\n",
      "step 660: train loss 4.5405, val loss 4.5610\n",
      "step 670: train loss 4.5434, val loss 4.5843\n",
      "step 680: train loss 4.5500, val loss 4.5569\n",
      "step 690: train loss 4.5339, val loss 4.5253\n",
      "step 700: train loss 4.4831, val loss 4.5730\n",
      "step 710: train loss 4.5609, val loss 4.4861\n",
      "step 720: train loss 4.5740, val loss 4.5748\n",
      "step 730: train loss 4.5418, val loss 4.5599\n",
      "step 740: train loss 4.5227, val loss 4.5927\n",
      "step 750: train loss 4.4669, val loss 4.5451\n",
      "step 760: train loss 4.5303, val loss 4.5058\n",
      "step 770: train loss 4.4957, val loss 4.5566\n",
      "step 780: train loss 4.4550, val loss 4.5293\n",
      "step 790: train loss 4.4320, val loss 4.5170\n",
      "step 800: train loss 4.4559, val loss 4.4819\n",
      "step 810: train loss 4.4976, val loss 4.4749\n",
      "step 820: train loss 4.4876, val loss 4.4515\n",
      "step 830: train loss 4.4256, val loss 4.4118\n",
      "step 840: train loss 4.3610, val loss 4.4797\n",
      "step 850: train loss 4.3926, val loss 4.4191\n",
      "step 860: train loss 4.4478, val loss 4.4556\n",
      "step 870: train loss 4.4706, val loss 4.4709\n",
      "step 880: train loss 4.4413, val loss 4.4457\n",
      "step 890: train loss 4.3132, val loss 4.3892\n",
      "step 900: train loss 4.3039, val loss 4.4313\n",
      "step 910: train loss 4.3675, val loss 4.4229\n",
      "step 920: train loss 4.3757, val loss 4.4056\n",
      "step 930: train loss 4.4313, val loss 4.4405\n",
      "step 940: train loss 4.3608, val loss 4.3065\n",
      "step 950: train loss 4.3652, val loss 4.3413\n",
      "step 960: train loss 4.3519, val loss 4.4032\n",
      "step 970: train loss 4.3582, val loss 4.4031\n",
      "step 980: train loss 4.2515, val loss 4.3743\n",
      "step 990: train loss 4.2868, val loss 4.3524\n",
      "step 1000: train loss 4.3432, val loss 4.3663\n",
      "step 1010: train loss 4.2443, val loss 4.3901\n",
      "step 1020: train loss 4.2766, val loss 4.3145\n",
      "step 1030: train loss 4.2703, val loss 4.3414\n",
      "step 1040: train loss 4.3214, val loss 4.3111\n",
      "step 1050: train loss 4.2542, val loss 4.3078\n",
      "step 1060: train loss 4.2751, val loss 4.3099\n",
      "step 1070: train loss 4.2947, val loss 4.3306\n",
      "step 1080: train loss 4.2207, val loss 4.3085\n",
      "step 1090: train loss 4.2943, val loss 4.2505\n",
      "step 1100: train loss 4.3464, val loss 4.2034\n",
      "step 1110: train loss 4.2344, val loss 4.2982\n",
      "step 1120: train loss 4.2766, val loss 4.2688\n",
      "step 1130: train loss 4.2894, val loss 4.2349\n",
      "step 1140: train loss 4.2395, val loss 4.2488\n",
      "step 1150: train loss 4.2213, val loss 4.3138\n",
      "step 1160: train loss 4.1844, val loss 4.1998\n",
      "step 1170: train loss 4.1463, val loss 4.1877\n",
      "step 1180: train loss 4.1838, val loss 4.1982\n",
      "step 1190: train loss 4.1706, val loss 4.2859\n",
      "step 1200: train loss 4.1524, val loss 4.1939\n",
      "step 1210: train loss 4.1684, val loss 4.1991\n",
      "step 1220: train loss 4.1548, val loss 4.2056\n",
      "step 1230: train loss 4.1523, val loss 4.1863\n",
      "step 1240: train loss 4.1257, val loss 4.2110\n",
      "step 1250: train loss 4.1061, val loss 4.1375\n",
      "step 1260: train loss 4.1392, val loss 4.0805\n",
      "step 1270: train loss 4.2063, val loss 4.1803\n",
      "step 1280: train loss 4.0378, val loss 4.1228\n",
      "step 1290: train loss 4.1762, val loss 4.1629\n",
      "step 1300: train loss 4.1211, val loss 4.1729\n",
      "step 1310: train loss 4.0791, val loss 4.1286\n",
      "step 1320: train loss 4.0594, val loss 4.1898\n",
      "step 1330: train loss 4.1171, val loss 4.0725\n",
      "step 1340: train loss 4.0817, val loss 4.1242\n",
      "step 1350: train loss 4.1275, val loss 4.1514\n",
      "step 1360: train loss 4.1009, val loss 4.1050\n",
      "step 1370: train loss 4.1515, val loss 4.1301\n",
      "step 1380: train loss 4.0802, val loss 4.1451\n",
      "step 1390: train loss 4.0425, val loss 4.1400\n",
      "step 1400: train loss 4.0571, val loss 4.0784\n",
      "step 1410: train loss 4.1555, val loss 4.0745\n",
      "step 1420: train loss 4.0962, val loss 3.9914\n",
      "step 1430: train loss 3.9909, val loss 4.1039\n",
      "step 1440: train loss 4.0734, val loss 4.0443\n",
      "step 1450: train loss 4.0818, val loss 4.1011\n",
      "step 1460: train loss 3.9789, val loss 4.0712\n",
      "step 1470: train loss 4.0112, val loss 4.0113\n",
      "step 1480: train loss 3.9334, val loss 4.0352\n",
      "step 1490: train loss 4.0176, val loss 4.0429\n",
      "step 1500: train loss 3.9741, val loss 4.0145\n",
      "step 1510: train loss 3.9626, val loss 4.0410\n",
      "step 1520: train loss 3.9050, val loss 4.0142\n",
      "step 1530: train loss 3.9637, val loss 3.9553\n",
      "step 1540: train loss 4.0325, val loss 3.9631\n",
      "step 1550: train loss 3.9773, val loss 3.9203\n",
      "step 1560: train loss 4.0000, val loss 3.9372\n",
      "step 1570: train loss 3.9626, val loss 3.9341\n",
      "step 1580: train loss 3.9393, val loss 3.9556\n",
      "step 1590: train loss 4.0162, val loss 4.0359\n",
      "step 1600: train loss 3.9706, val loss 3.9920\n",
      "step 1610: train loss 3.9093, val loss 3.9975\n",
      "step 1620: train loss 3.9287, val loss 3.9142\n",
      "step 1630: train loss 3.9082, val loss 3.9341\n",
      "step 1640: train loss 3.9114, val loss 3.9468\n",
      "step 1650: train loss 3.9020, val loss 3.8421\n",
      "step 1660: train loss 3.8930, val loss 3.8957\n",
      "step 1670: train loss 3.9163, val loss 3.9009\n",
      "step 1680: train loss 3.8231, val loss 3.9727\n",
      "step 1690: train loss 3.9250, val loss 3.8720\n",
      "step 1700: train loss 3.8613, val loss 3.9196\n",
      "step 1710: train loss 3.8941, val loss 3.9204\n",
      "step 1720: train loss 3.8622, val loss 3.9141\n",
      "step 1730: train loss 3.9100, val loss 3.8387\n",
      "step 1740: train loss 3.8629, val loss 3.8760\n",
      "step 1750: train loss 3.8088, val loss 3.8847\n",
      "step 1760: train loss 3.8764, val loss 3.8836\n",
      "step 1770: train loss 3.8241, val loss 3.9083\n",
      "step 1780: train loss 3.8298, val loss 3.8897\n",
      "step 1790: train loss 3.8405, val loss 3.8550\n",
      "step 1800: train loss 3.9270, val loss 3.8623\n",
      "step 1810: train loss 3.8744, val loss 3.8762\n",
      "step 1820: train loss 3.8168, val loss 3.8260\n",
      "step 1830: train loss 3.8412, val loss 3.8458\n",
      "step 1840: train loss 3.7889, val loss 3.8121\n",
      "step 1850: train loss 3.7584, val loss 3.8954\n",
      "step 1860: train loss 3.8465, val loss 3.9180\n",
      "step 1870: train loss 3.8089, val loss 3.9012\n",
      "step 1880: train loss 3.7722, val loss 3.8022\n",
      "step 1890: train loss 3.7436, val loss 3.8438\n",
      "step 1900: train loss 3.7786, val loss 3.8345\n",
      "step 1910: train loss 3.8011, val loss 3.7631\n",
      "step 1920: train loss 3.7895, val loss 3.7838\n",
      "step 1930: train loss 3.8272, val loss 3.6958\n",
      "step 1940: train loss 3.7898, val loss 3.7905\n",
      "step 1950: train loss 3.7708, val loss 3.7751\n",
      "step 1960: train loss 3.6970, val loss 3.8639\n",
      "step 1970: train loss 3.7013, val loss 3.7987\n",
      "step 1980: train loss 3.7308, val loss 3.7569\n",
      "step 1990: train loss 3.7735, val loss 3.7504\n",
      "step 2000: train loss 3.7469, val loss 3.7383\n",
      "step 2010: train loss 3.7388, val loss 3.7156\n",
      "step 2020: train loss 3.7029, val loss 3.8019\n",
      "step 2030: train loss 3.6546, val loss 3.7732\n",
      "step 2040: train loss 3.7688, val loss 3.7338\n",
      "step 2050: train loss 3.6582, val loss 3.7448\n",
      "step 2060: train loss 3.7058, val loss 3.7171\n",
      "step 2070: train loss 3.6332, val loss 3.7056\n",
      "step 2080: train loss 3.6732, val loss 3.6961\n",
      "step 2090: train loss 3.6556, val loss 3.7100\n",
      "step 2100: train loss 3.7097, val loss 3.6008\n",
      "step 2110: train loss 3.6853, val loss 3.6980\n",
      "step 2120: train loss 3.6950, val loss 3.6318\n",
      "step 2130: train loss 3.6902, val loss 3.7188\n",
      "step 2140: train loss 3.7080, val loss 3.6945\n",
      "step 2150: train loss 3.6854, val loss 3.7435\n",
      "step 2160: train loss 3.6433, val loss 3.7521\n",
      "step 2170: train loss 3.7049, val loss 3.7016\n",
      "step 2180: train loss 3.6827, val loss 3.7164\n",
      "step 2190: train loss 3.6127, val loss 3.6857\n",
      "step 2200: train loss 3.6225, val loss 3.7126\n",
      "step 2210: train loss 3.6147, val loss 3.7038\n",
      "step 2220: train loss 3.6773, val loss 3.6752\n",
      "step 2230: train loss 3.6199, val loss 3.6988\n",
      "step 2240: train loss 3.5979, val loss 3.6776\n",
      "step 2250: train loss 3.5577, val loss 3.6571\n",
      "step 2260: train loss 3.5983, val loss 3.6331\n",
      "step 2270: train loss 3.5764, val loss 3.5801\n",
      "step 2280: train loss 3.6092, val loss 3.6502\n",
      "step 2290: train loss 3.5773, val loss 3.6062\n",
      "step 2300: train loss 3.5767, val loss 3.5720\n",
      "step 2310: train loss 3.6019, val loss 3.6609\n",
      "step 2320: train loss 3.5865, val loss 3.6227\n",
      "step 2330: train loss 3.6396, val loss 3.6022\n",
      "step 2340: train loss 3.5116, val loss 3.5938\n",
      "step 2350: train loss 3.5852, val loss 3.6327\n",
      "step 2360: train loss 3.5624, val loss 3.5884\n",
      "step 2370: train loss 3.5230, val loss 3.6310\n",
      "step 2380: train loss 3.5483, val loss 3.5957\n",
      "step 2390: train loss 3.4918, val loss 3.5897\n",
      "step 2400: train loss 3.5821, val loss 3.6043\n",
      "step 2410: train loss 3.5174, val loss 3.6033\n",
      "step 2420: train loss 3.5978, val loss 3.5399\n",
      "step 2430: train loss 3.5305, val loss 3.5585\n",
      "step 2440: train loss 3.5204, val loss 3.5641\n",
      "step 2450: train loss 3.5256, val loss 3.5108\n",
      "step 2460: train loss 3.5079, val loss 3.5632\n",
      "step 2470: train loss 3.4796, val loss 3.4791\n",
      "step 2480: train loss 3.5885, val loss 3.5319\n",
      "step 2490: train loss 3.5252, val loss 3.4980\n",
      "step 2500: train loss 3.5150, val loss 3.5301\n",
      "step 2510: train loss 3.4076, val loss 3.5112\n",
      "step 2520: train loss 3.4619, val loss 3.5262\n",
      "step 2530: train loss 3.4597, val loss 3.5750\n",
      "step 2540: train loss 3.3955, val loss 3.4789\n",
      "step 2550: train loss 3.4864, val loss 3.5112\n",
      "step 2560: train loss 3.5110, val loss 3.4697\n",
      "step 2570: train loss 3.4206, val loss 3.5604\n",
      "step 2580: train loss 3.4596, val loss 3.4947\n",
      "step 2590: train loss 3.4292, val loss 3.4746\n",
      "step 2600: train loss 3.5008, val loss 3.4975\n",
      "step 2610: train loss 3.5334, val loss 3.5174\n",
      "step 2620: train loss 3.3789, val loss 3.4739\n",
      "step 2630: train loss 3.4662, val loss 3.5056\n",
      "step 2640: train loss 3.4975, val loss 3.5284\n",
      "step 2650: train loss 3.4855, val loss 3.5053\n",
      "step 2660: train loss 3.3801, val loss 3.4457\n",
      "step 2670: train loss 3.4597, val loss 3.4685\n",
      "step 2680: train loss 3.4548, val loss 3.4344\n",
      "step 2690: train loss 3.4363, val loss 3.5092\n",
      "step 2700: train loss 3.4587, val loss 3.4450\n",
      "step 2710: train loss 3.4369, val loss 3.5051\n",
      "step 2720: train loss 3.4385, val loss 3.4390\n",
      "step 2730: train loss 3.4498, val loss 3.4521\n",
      "step 2740: train loss 3.4156, val loss 3.4273\n",
      "step 2750: train loss 3.4390, val loss 3.4371\n",
      "step 2760: train loss 3.3877, val loss 3.4349\n",
      "step 2770: train loss 3.3997, val loss 3.4563\n",
      "step 2780: train loss 3.4109, val loss 3.3833\n",
      "step 2790: train loss 3.3811, val loss 3.3919\n",
      "step 2800: train loss 3.4142, val loss 3.4280\n",
      "step 2810: train loss 3.4345, val loss 3.3977\n",
      "step 2820: train loss 3.3376, val loss 3.3450\n",
      "step 2830: train loss 3.3496, val loss 3.4071\n",
      "step 2840: train loss 3.3082, val loss 3.4130\n",
      "step 2850: train loss 3.3632, val loss 3.3567\n",
      "step 2860: train loss 3.3776, val loss 3.3733\n",
      "step 2870: train loss 3.4081, val loss 3.4088\n",
      "step 2880: train loss 3.3134, val loss 3.3245\n",
      "step 2890: train loss 3.3762, val loss 3.4059\n",
      "step 2900: train loss 3.3678, val loss 3.3176\n",
      "step 2910: train loss 3.2919, val loss 3.3718\n",
      "step 2920: train loss 3.3444, val loss 3.3789\n",
      "step 2930: train loss 3.3497, val loss 3.4024\n",
      "step 2940: train loss 3.2864, val loss 3.3465\n",
      "step 2950: train loss 3.3214, val loss 3.3034\n",
      "step 2960: train loss 3.3050, val loss 3.3422\n",
      "step 2970: train loss 3.2948, val loss 3.3908\n",
      "step 2980: train loss 3.3265, val loss 3.3912\n",
      "step 2990: train loss 3.2724, val loss 3.3706\n",
      "step 3000: train loss 3.3639, val loss 3.3848\n",
      "step 3010: train loss 3.3405, val loss 3.3375\n",
      "step 3020: train loss 3.2284, val loss 3.3308\n",
      "step 3030: train loss 3.2481, val loss 3.3760\n",
      "step 3040: train loss 3.2094, val loss 3.2814\n",
      "step 3050: train loss 3.3479, val loss 3.3329\n",
      "step 3060: train loss 3.2364, val loss 3.3454\n",
      "step 3070: train loss 3.3172, val loss 3.3689\n",
      "step 3080: train loss 3.3094, val loss 3.2986\n",
      "step 3090: train loss 3.3480, val loss 3.2797\n",
      "step 3100: train loss 3.3280, val loss 3.2571\n",
      "step 3110: train loss 3.3611, val loss 3.3684\n",
      "step 3120: train loss 3.2929, val loss 3.2934\n",
      "step 3130: train loss 3.1950, val loss 3.3073\n",
      "step 3140: train loss 3.3129, val loss 3.2674\n",
      "step 3150: train loss 3.2612, val loss 3.2914\n",
      "step 3160: train loss 3.2596, val loss 3.3149\n",
      "step 3170: train loss 3.1804, val loss 3.3420\n",
      "step 3180: train loss 3.2406, val loss 3.2711\n",
      "step 3190: train loss 3.3023, val loss 3.3136\n",
      "step 3200: train loss 3.2787, val loss 3.2546\n",
      "step 3210: train loss 3.2035, val loss 3.3116\n",
      "step 3220: train loss 3.2149, val loss 3.3314\n",
      "step 3230: train loss 3.2211, val loss 3.2485\n",
      "step 3240: train loss 3.2655, val loss 3.2666\n",
      "step 3250: train loss 3.2197, val loss 3.1729\n",
      "step 3260: train loss 3.2447, val loss 3.2826\n",
      "step 3270: train loss 3.1137, val loss 3.2579\n",
      "step 3280: train loss 3.2569, val loss 3.2165\n",
      "step 3290: train loss 3.2135, val loss 3.3100\n",
      "step 3300: train loss 3.1917, val loss 3.2405\n",
      "step 3310: train loss 3.2343, val loss 3.2143\n",
      "step 3320: train loss 3.3177, val loss 3.2667\n",
      "step 3330: train loss 3.1718, val loss 3.2757\n",
      "step 3340: train loss 3.2434, val loss 3.2558\n",
      "step 3350: train loss 3.1614, val loss 3.2690\n",
      "step 3360: train loss 3.2178, val loss 3.1907\n",
      "step 3370: train loss 3.2049, val loss 3.1831\n",
      "step 3380: train loss 3.1801, val loss 3.1346\n",
      "step 3390: train loss 3.2006, val loss 3.2346\n",
      "step 3400: train loss 3.2034, val loss 3.1942\n",
      "step 3410: train loss 3.2289, val loss 3.2261\n",
      "step 3420: train loss 3.1727, val loss 3.2199\n",
      "step 3430: train loss 3.1626, val loss 3.2679\n",
      "step 3440: train loss 3.2116, val loss 3.1984\n",
      "step 3450: train loss 3.1566, val loss 3.2170\n",
      "step 3460: train loss 3.1737, val loss 3.2237\n",
      "step 3470: train loss 3.1569, val loss 3.2343\n",
      "step 3480: train loss 3.1493, val loss 3.1328\n",
      "step 3490: train loss 3.1113, val loss 3.1477\n",
      "step 3500: train loss 3.2162, val loss 3.1074\n",
      "step 3510: train loss 3.2151, val loss 3.2014\n",
      "step 3520: train loss 3.2280, val loss 3.1451\n",
      "step 3530: train loss 3.0808, val loss 3.2449\n",
      "step 3540: train loss 3.1635, val loss 3.1563\n",
      "step 3550: train loss 3.1526, val loss 3.1561\n",
      "step 3560: train loss 3.1489, val loss 3.1433\n",
      "step 3570: train loss 3.1178, val loss 3.1841\n",
      "step 3580: train loss 3.0786, val loss 3.1416\n",
      "step 3590: train loss 3.1339, val loss 3.1173\n",
      "step 3600: train loss 3.1173, val loss 3.1772\n",
      "step 3610: train loss 3.0663, val loss 3.0932\n",
      "step 3620: train loss 3.1640, val loss 3.2275\n",
      "step 3630: train loss 3.1371, val loss 3.0983\n",
      "step 3640: train loss 3.1605, val loss 3.1456\n",
      "step 3650: train loss 3.0798, val loss 3.0860\n",
      "step 3660: train loss 3.1413, val loss 3.1868\n",
      "step 3670: train loss 3.1470, val loss 3.1420\n",
      "step 3680: train loss 3.1004, val loss 3.1145\n",
      "step 3690: train loss 3.1835, val loss 3.1321\n",
      "step 3700: train loss 3.0608, val loss 3.0660\n",
      "step 3710: train loss 3.1019, val loss 3.1269\n",
      "step 3720: train loss 3.1350, val loss 3.0856\n",
      "step 3730: train loss 3.1914, val loss 3.1500\n",
      "step 3740: train loss 3.0248, val loss 3.1328\n",
      "step 3750: train loss 3.2332, val loss 3.1280\n",
      "step 3760: train loss 3.1610, val loss 3.1922\n",
      "step 3770: train loss 3.1033, val loss 3.0400\n",
      "step 3780: train loss 3.0851, val loss 3.0428\n",
      "step 3790: train loss 3.0663, val loss 3.0373\n",
      "step 3800: train loss 3.0797, val loss 3.1918\n",
      "step 3810: train loss 3.1198, val loss 3.0767\n",
      "step 3820: train loss 3.1108, val loss 3.0722\n",
      "step 3830: train loss 3.0831, val loss 3.0150\n",
      "step 3840: train loss 3.0445, val loss 3.1457\n",
      "step 3850: train loss 3.0461, val loss 3.0577\n",
      "step 3860: train loss 3.1152, val loss 3.0688\n",
      "step 3870: train loss 3.0045, val loss 3.0038\n",
      "step 3880: train loss 3.0863, val loss 3.0821\n",
      "step 3890: train loss 3.0197, val loss 3.0804\n",
      "step 3900: train loss 3.0328, val loss 3.0886\n",
      "step 3910: train loss 3.0487, val loss 3.0811\n",
      "step 3920: train loss 3.0373, val loss 3.1000\n",
      "step 3930: train loss 3.0157, val loss 3.1269\n",
      "step 3940: train loss 3.0352, val loss 3.0693\n",
      "step 3950: train loss 3.0397, val loss 3.0581\n",
      "step 3960: train loss 2.9971, val loss 3.0683\n",
      "step 3970: train loss 3.0537, val loss 3.1029\n",
      "step 3980: train loss 3.0457, val loss 3.0436\n",
      "step 3990: train loss 3.0189, val loss 3.0986\n",
      "step 4000: train loss 3.0277, val loss 3.0490\n",
      "step 4010: train loss 3.1323, val loss 3.0141\n",
      "step 4020: train loss 3.0572, val loss 3.0130\n",
      "step 4030: train loss 3.0637, val loss 2.9876\n",
      "step 4040: train loss 3.0571, val loss 3.0259\n",
      "step 4050: train loss 2.9656, val loss 3.1005\n",
      "step 4060: train loss 3.0279, val loss 3.0588\n",
      "step 4070: train loss 3.0382, val loss 2.9945\n",
      "step 4080: train loss 2.9908, val loss 3.0132\n",
      "step 4090: train loss 2.9852, val loss 3.0345\n",
      "step 4100: train loss 2.9734, val loss 2.9832\n",
      "step 4110: train loss 3.0490, val loss 3.0483\n",
      "step 4120: train loss 2.9442, val loss 3.0117\n",
      "step 4130: train loss 3.0790, val loss 3.0603\n",
      "step 4140: train loss 2.9458, val loss 2.9868\n",
      "step 4150: train loss 2.9469, val loss 3.0194\n",
      "step 4160: train loss 2.9721, val loss 3.0860\n",
      "step 4170: train loss 2.9672, val loss 3.0471\n",
      "step 4180: train loss 2.9765, val loss 3.0747\n",
      "step 4190: train loss 3.0277, val loss 3.0617\n",
      "step 4200: train loss 2.9929, val loss 2.9607\n",
      "step 4210: train loss 3.0243, val loss 3.1033\n",
      "step 4220: train loss 2.9185, val loss 2.9831\n",
      "step 4230: train loss 2.9747, val loss 3.0082\n",
      "step 4240: train loss 3.0176, val loss 3.0023\n",
      "step 4250: train loss 2.9797, val loss 2.9671\n",
      "step 4260: train loss 2.9748, val loss 2.9795\n",
      "step 4270: train loss 2.9044, val loss 3.0144\n",
      "step 4280: train loss 2.9015, val loss 3.0395\n",
      "step 4290: train loss 2.8862, val loss 2.9987\n",
      "step 4300: train loss 3.0425, val loss 2.9299\n",
      "step 4310: train loss 2.9994, val loss 2.9719\n",
      "step 4320: train loss 2.9006, val loss 2.9449\n",
      "step 4330: train loss 3.0755, val loss 2.9858\n",
      "step 4340: train loss 2.9062, val loss 3.0495\n",
      "step 4350: train loss 2.9026, val loss 2.9490\n",
      "step 4360: train loss 2.9408, val loss 3.0407\n",
      "step 4370: train loss 2.9611, val loss 3.0084\n",
      "step 4380: train loss 2.9237, val loss 2.9818\n",
      "step 4390: train loss 2.9607, val loss 2.9391\n",
      "step 4400: train loss 2.9725, val loss 2.9438\n",
      "step 4410: train loss 2.9011, val loss 2.9856\n",
      "step 4420: train loss 2.9090, val loss 2.8972\n",
      "step 4430: train loss 2.8672, val loss 2.9189\n",
      "step 4440: train loss 2.8458, val loss 2.9648\n",
      "step 4450: train loss 2.9232, val loss 2.8852\n",
      "step 4460: train loss 2.9226, val loss 2.9328\n",
      "step 4470: train loss 2.9450, val loss 2.9764\n",
      "step 4480: train loss 2.8785, val loss 2.8903\n",
      "step 4490: train loss 2.9467, val loss 2.9101\n",
      "step 4500: train loss 2.9545, val loss 2.9332\n",
      "step 4510: train loss 2.8796, val loss 2.8648\n",
      "step 4520: train loss 2.9814, val loss 2.9270\n",
      "step 4530: train loss 2.8961, val loss 2.9023\n",
      "step 4540: train loss 2.9720, val loss 2.8750\n",
      "step 4550: train loss 2.9161, val loss 2.9276\n",
      "step 4560: train loss 2.9155, val loss 2.9553\n",
      "step 4570: train loss 2.8856, val loss 2.9852\n",
      "step 4580: train loss 2.9098, val loss 2.8983\n",
      "step 4590: train loss 2.8458, val loss 2.9100\n",
      "step 4600: train loss 2.8929, val loss 2.9045\n",
      "step 4610: train loss 2.8822, val loss 2.9619\n",
      "step 4620: train loss 2.8917, val loss 2.9307\n",
      "step 4630: train loss 2.8346, val loss 2.8783\n",
      "step 4640: train loss 2.7973, val loss 2.9890\n",
      "step 4650: train loss 2.9069, val loss 2.8346\n",
      "step 4660: train loss 2.8583, val loss 2.8499\n",
      "step 4670: train loss 2.8092, val loss 2.9086\n",
      "step 4680: train loss 2.9117, val loss 2.8821\n",
      "step 4690: train loss 2.8783, val loss 2.9672\n",
      "step 4700: train loss 2.9258, val loss 2.8628\n",
      "step 4710: train loss 2.9114, val loss 2.9166\n",
      "step 4720: train loss 2.8848, val loss 2.8500\n",
      "step 4730: train loss 2.8372, val loss 2.9062\n",
      "step 4740: train loss 2.8645, val loss 2.8750\n",
      "step 4750: train loss 2.9524, val loss 2.8663\n",
      "step 4760: train loss 2.8561, val loss 2.8611\n",
      "step 4770: train loss 2.8852, val loss 2.9238\n",
      "step 4780: train loss 2.8248, val loss 2.8786\n",
      "step 4790: train loss 2.7653, val loss 2.8886\n",
      "step 4800: train loss 2.9041, val loss 2.8860\n",
      "step 4810: train loss 2.8181, val loss 2.9124\n",
      "step 4820: train loss 2.8125, val loss 2.8733\n",
      "step 4830: train loss 2.8418, val loss 2.9187\n",
      "step 4840: train loss 2.7841, val loss 2.8688\n",
      "step 4850: train loss 2.7801, val loss 2.8048\n",
      "step 4860: train loss 2.9109, val loss 2.8451\n",
      "step 4870: train loss 2.8785, val loss 2.8603\n",
      "step 4880: train loss 2.8385, val loss 2.8492\n",
      "step 4890: train loss 2.8230, val loss 2.8659\n",
      "step 4900: train loss 2.8437, val loss 2.8618\n",
      "step 4910: train loss 2.8790, val loss 2.8837\n",
      "step 4920: train loss 2.8265, val loss 2.8717\n",
      "step 4930: train loss 2.8537, val loss 2.8486\n",
      "step 4940: train loss 2.8100, val loss 2.8807\n",
      "step 4950: train loss 2.7784, val loss 2.9126\n",
      "step 4960: train loss 2.8118, val loss 2.9280\n",
      "step 4970: train loss 2.8682, val loss 2.8150\n",
      "step 4980: train loss 2.8573, val loss 2.8541\n",
      "step 4990: train loss 2.9263, val loss 2.7659\n",
      "step 5000: train loss 2.8260, val loss 2.9040\n",
      "step 5010: train loss 2.7917, val loss 2.8645\n",
      "step 5020: train loss 2.8456, val loss 2.8651\n",
      "step 5030: train loss 2.8871, val loss 2.8451\n",
      "step 5040: train loss 2.7477, val loss 2.8526\n",
      "step 5050: train loss 2.8493, val loss 2.8596\n",
      "step 5060: train loss 2.8445, val loss 2.8317\n",
      "step 5070: train loss 2.8111, val loss 2.8263\n",
      "step 5080: train loss 2.8387, val loss 2.8508\n",
      "step 5090: train loss 2.8521, val loss 2.8027\n",
      "step 5100: train loss 2.8308, val loss 2.7817\n",
      "step 5110: train loss 2.8632, val loss 2.8060\n",
      "step 5120: train loss 2.7864, val loss 2.8453\n",
      "step 5130: train loss 2.7877, val loss 2.7582\n",
      "step 5140: train loss 2.7890, val loss 2.8197\n",
      "step 5150: train loss 2.7979, val loss 2.8399\n",
      "step 5160: train loss 2.7933, val loss 2.8330\n",
      "step 5170: train loss 2.8023, val loss 2.8310\n",
      "step 5180: train loss 2.7792, val loss 2.8243\n",
      "step 5190: train loss 2.8184, val loss 2.7596\n",
      "step 5200: train loss 2.8745, val loss 2.9715\n",
      "step 5210: train loss 2.8612, val loss 2.8077\n",
      "step 5220: train loss 2.7844, val loss 2.8292\n",
      "step 5230: train loss 2.8337, val loss 2.8820\n",
      "step 5240: train loss 2.8023, val loss 2.8157\n",
      "step 5250: train loss 2.8180, val loss 2.8026\n",
      "step 5260: train loss 2.7782, val loss 2.7555\n",
      "step 5270: train loss 2.8235, val loss 2.7638\n",
      "step 5280: train loss 2.7333, val loss 2.7582\n",
      "step 5290: train loss 2.8165, val loss 2.7788\n",
      "step 5300: train loss 2.7800, val loss 2.6848\n",
      "step 5310: train loss 2.8124, val loss 2.7975\n",
      "step 5320: train loss 2.7710, val loss 2.8464\n",
      "step 5330: train loss 2.7827, val loss 2.7929\n",
      "step 5340: train loss 2.7651, val loss 2.8050\n",
      "step 5350: train loss 2.6735, val loss 2.7910\n",
      "step 5360: train loss 2.8639, val loss 2.8179\n",
      "step 5370: train loss 2.7388, val loss 2.7312\n",
      "step 5380: train loss 2.7721, val loss 2.7482\n",
      "step 5390: train loss 2.7848, val loss 2.7890\n",
      "step 5400: train loss 2.7037, val loss 2.7653\n",
      "step 5410: train loss 2.7846, val loss 2.8780\n",
      "step 5420: train loss 2.7824, val loss 2.8192\n",
      "step 5430: train loss 2.6823, val loss 2.7519\n",
      "step 5440: train loss 2.8165, val loss 2.8573\n",
      "step 5450: train loss 2.7534, val loss 2.7877\n",
      "step 5460: train loss 2.8065, val loss 2.8264\n",
      "step 5470: train loss 2.6915, val loss 2.7986\n",
      "step 5480: train loss 2.8077, val loss 2.7203\n",
      "step 5490: train loss 2.8427, val loss 2.7840\n",
      "step 5500: train loss 2.7549, val loss 2.7604\n",
      "step 5510: train loss 2.6128, val loss 2.8167\n",
      "step 5520: train loss 2.6832, val loss 2.7562\n",
      "step 5530: train loss 2.8278, val loss 2.7815\n",
      "step 5540: train loss 2.8076, val loss 2.7627\n",
      "step 5550: train loss 2.7442, val loss 2.7460\n",
      "step 5560: train loss 2.7933, val loss 2.7279\n",
      "step 5570: train loss 2.7012, val loss 2.8080\n",
      "step 5580: train loss 2.6659, val loss 2.7531\n",
      "step 5590: train loss 2.7171, val loss 2.7186\n",
      "step 5600: train loss 2.7379, val loss 2.7443\n",
      "step 5610: train loss 2.8220, val loss 2.7415\n",
      "step 5620: train loss 2.7024, val loss 2.7542\n",
      "step 5630: train loss 2.7086, val loss 2.7378\n",
      "step 5640: train loss 2.7875, val loss 2.8130\n",
      "step 5650: train loss 2.7955, val loss 2.7936\n",
      "step 5660: train loss 2.7453, val loss 2.7365\n",
      "step 5670: train loss 2.6947, val loss 2.7998\n",
      "step 5680: train loss 2.7585, val loss 2.7375\n",
      "step 5690: train loss 2.6896, val loss 2.6511\n",
      "step 5700: train loss 2.7062, val loss 2.8057\n",
      "step 5710: train loss 2.7710, val loss 2.7119\n",
      "step 5720: train loss 2.6728, val loss 2.7070\n",
      "step 5730: train loss 2.7094, val loss 2.6997\n",
      "step 5740: train loss 2.8637, val loss 2.8031\n",
      "step 5750: train loss 2.7443, val loss 2.7564\n",
      "step 5760: train loss 2.7292, val loss 2.7830\n",
      "step 5770: train loss 2.7493, val loss 2.7785\n",
      "step 5780: train loss 2.7551, val loss 2.7888\n",
      "step 5790: train loss 2.6880, val loss 2.7023\n",
      "step 5800: train loss 2.6797, val loss 2.7554\n",
      "step 5810: train loss 2.7205, val loss 2.7079\n",
      "step 5820: train loss 2.6563, val loss 2.7324\n",
      "step 5830: train loss 2.6811, val loss 2.6812\n",
      "step 5840: train loss 2.7441, val loss 2.6524\n",
      "step 5850: train loss 2.6482, val loss 2.6994\n",
      "step 5860: train loss 2.7457, val loss 2.7065\n",
      "step 5870: train loss 2.7465, val loss 2.7262\n",
      "step 5880: train loss 2.7639, val loss 2.6956\n",
      "step 5890: train loss 2.6794, val loss 2.7157\n",
      "step 5900: train loss 2.6526, val loss 2.6999\n",
      "step 5910: train loss 2.7217, val loss 2.7181\n",
      "step 5920: train loss 2.7206, val loss 2.7683\n",
      "step 5930: train loss 2.6557, val loss 2.7423\n",
      "step 5940: train loss 2.7120, val loss 2.6100\n",
      "step 5950: train loss 2.6588, val loss 2.7544\n",
      "step 5960: train loss 2.6652, val loss 2.7043\n",
      "step 5970: train loss 2.7595, val loss 2.6139\n",
      "step 5980: train loss 2.6237, val loss 2.7019\n",
      "step 5990: train loss 2.6830, val loss 2.7498\n",
      "step 6000: train loss 2.6941, val loss 2.7020\n",
      "step 6010: train loss 2.7426, val loss 2.7197\n",
      "step 6020: train loss 2.6320, val loss 2.7567\n",
      "step 6030: train loss 2.6581, val loss 2.7049\n",
      "step 6040: train loss 2.7577, val loss 2.8036\n",
      "step 6050: train loss 2.7348, val loss 2.6795\n",
      "step 6060: train loss 2.6570, val loss 2.7152\n",
      "step 6070: train loss 2.6845, val loss 2.7667\n",
      "step 6080: train loss 2.7637, val loss 2.7249\n",
      "step 6090: train loss 2.6928, val loss 2.6464\n",
      "step 6100: train loss 2.6521, val loss 2.6720\n",
      "step 6110: train loss 2.7205, val loss 2.7152\n",
      "step 6120: train loss 2.6866, val loss 2.7829\n",
      "step 6130: train loss 2.7220, val loss 2.7694\n",
      "step 6140: train loss 2.6955, val loss 2.7471\n",
      "step 6150: train loss 2.7198, val loss 2.7521\n",
      "step 6160: train loss 2.7032, val loss 2.6980\n",
      "step 6170: train loss 2.6806, val loss 2.6819\n",
      "step 6180: train loss 2.6671, val loss 2.6150\n",
      "step 6190: train loss 2.7156, val loss 2.6249\n",
      "step 6200: train loss 2.6031, val loss 2.6983\n",
      "step 6210: train loss 2.7585, val loss 2.6599\n",
      "step 6220: train loss 2.6728, val loss 2.6622\n",
      "step 6230: train loss 2.6565, val loss 2.7272\n",
      "step 6240: train loss 2.6455, val loss 2.7192\n",
      "step 6250: train loss 2.6732, val loss 2.7080\n",
      "step 6260: train loss 2.6338, val loss 2.6895\n",
      "step 6270: train loss 2.7569, val loss 2.7000\n",
      "step 6280: train loss 2.6691, val loss 2.6348\n",
      "step 6290: train loss 2.6609, val loss 2.6030\n",
      "step 6300: train loss 2.6269, val loss 2.7625\n",
      "step 6310: train loss 2.6463, val loss 2.6211\n",
      "step 6320: train loss 2.7164, val loss 2.7160\n",
      "step 6330: train loss 2.5911, val loss 2.6909\n",
      "step 6340: train loss 2.6688, val loss 2.6068\n",
      "step 6350: train loss 2.7345, val loss 2.6997\n",
      "step 6360: train loss 2.6835, val loss 2.7946\n",
      "step 6370: train loss 2.7212, val loss 2.6798\n",
      "step 6380: train loss 2.6706, val loss 2.6281\n",
      "step 6390: train loss 2.5912, val loss 2.6329\n",
      "step 6400: train loss 2.7047, val loss 2.6834\n",
      "step 6410: train loss 2.6290, val loss 2.6607\n",
      "step 6420: train loss 2.7298, val loss 2.7692\n",
      "step 6430: train loss 2.6166, val loss 2.6829\n",
      "step 6440: train loss 2.6745, val loss 2.6785\n",
      "step 6450: train loss 2.6107, val loss 2.6297\n",
      "step 6460: train loss 2.6134, val loss 2.6421\n",
      "step 6470: train loss 2.6709, val loss 2.7000\n",
      "step 6480: train loss 2.6124, val loss 2.6936\n",
      "step 6490: train loss 2.6604, val loss 2.7384\n",
      "step 6500: train loss 2.7041, val loss 2.6959\n",
      "step 6510: train loss 2.7408, val loss 2.6750\n",
      "step 6520: train loss 2.6728, val loss 2.6802\n",
      "step 6530: train loss 2.6475, val loss 2.6543\n",
      "step 6540: train loss 2.6754, val loss 2.6553\n",
      "step 6550: train loss 2.6859, val loss 2.7361\n",
      "step 6560: train loss 2.6382, val loss 2.6575\n",
      "step 6570: train loss 2.7177, val loss 2.7365\n",
      "step 6580: train loss 2.6266, val loss 2.6718\n",
      "step 6590: train loss 2.6550, val loss 2.6352\n",
      "step 6600: train loss 2.6335, val loss 2.6180\n",
      "step 6610: train loss 2.6914, val loss 2.6931\n",
      "step 6620: train loss 2.6727, val loss 2.6835\n",
      "step 6630: train loss 2.6333, val loss 2.7248\n",
      "step 6640: train loss 2.6221, val loss 2.6601\n",
      "step 6650: train loss 2.6336, val loss 2.7475\n",
      "step 6660: train loss 2.6877, val loss 2.6627\n",
      "step 6670: train loss 2.6082, val loss 2.7109\n",
      "step 6680: train loss 2.5763, val loss 2.6199\n",
      "step 6690: train loss 2.7096, val loss 2.6202\n",
      "step 6700: train loss 2.6812, val loss 2.6211\n",
      "step 6710: train loss 2.6465, val loss 2.5834\n",
      "step 6720: train loss 2.6651, val loss 2.5753\n",
      "step 6730: train loss 2.5865, val loss 2.6384\n",
      "step 6740: train loss 2.6150, val loss 2.6493\n",
      "step 6750: train loss 2.5613, val loss 2.6491\n",
      "step 6760: train loss 2.5733, val loss 2.6940\n",
      "step 6770: train loss 2.5662, val loss 2.5961\n",
      "step 6780: train loss 2.6671, val loss 2.6856\n",
      "step 6790: train loss 2.5826, val loss 2.6286\n",
      "step 6800: train loss 2.6099, val loss 2.6013\n",
      "step 6810: train loss 2.6609, val loss 2.6424\n",
      "step 6820: train loss 2.6323, val loss 2.7029\n",
      "step 6830: train loss 2.6416, val loss 2.7309\n",
      "step 6840: train loss 2.4967, val loss 2.6666\n",
      "step 6850: train loss 2.6990, val loss 2.5746\n",
      "step 6860: train loss 2.5563, val loss 2.6251\n",
      "step 6870: train loss 2.6606, val loss 2.6970\n",
      "step 6880: train loss 2.5655, val loss 2.6958\n",
      "step 6890: train loss 2.5929, val loss 2.6610\n",
      "step 6900: train loss 2.6105, val loss 2.6356\n",
      "step 6910: train loss 2.6059, val loss 2.7030\n",
      "step 6920: train loss 2.4872, val loss 2.6475\n",
      "step 6930: train loss 2.6374, val loss 2.6243\n",
      "step 6940: train loss 2.6911, val loss 2.6216\n",
      "step 6950: train loss 2.6563, val loss 2.6221\n",
      "step 6960: train loss 2.6342, val loss 2.6500\n",
      "step 6970: train loss 2.6577, val loss 2.7147\n",
      "step 6980: train loss 2.5332, val loss 2.6299\n",
      "step 6990: train loss 2.6472, val loss 2.6334\n",
      "step 7000: train loss 2.5928, val loss 2.6055\n",
      "step 7010: train loss 2.6375, val loss 2.5912\n",
      "step 7020: train loss 2.6802, val loss 2.6926\n",
      "step 7030: train loss 2.7064, val loss 2.6362\n",
      "step 7040: train loss 2.5934, val loss 2.6031\n",
      "step 7050: train loss 2.6153, val loss 2.6833\n",
      "step 7060: train loss 2.6558, val loss 2.6266\n",
      "step 7070: train loss 2.5344, val loss 2.6016\n",
      "step 7080: train loss 2.5569, val loss 2.6385\n",
      "step 7090: train loss 2.6240, val loss 2.6291\n",
      "step 7100: train loss 2.6815, val loss 2.5835\n",
      "step 7110: train loss 2.6761, val loss 2.6160\n",
      "step 7120: train loss 2.6702, val loss 2.5905\n",
      "step 7130: train loss 2.6303, val loss 2.6732\n",
      "step 7140: train loss 2.6222, val loss 2.6821\n",
      "step 7150: train loss 2.6042, val loss 2.5854\n",
      "step 7160: train loss 2.6257, val loss 2.6019\n",
      "step 7170: train loss 2.6093, val loss 2.6300\n",
      "step 7180: train loss 2.5037, val loss 2.6113\n",
      "step 7190: train loss 2.6170, val loss 2.6539\n",
      "step 7200: train loss 2.5986, val loss 2.6439\n",
      "step 7210: train loss 2.6142, val loss 2.5981\n",
      "step 7220: train loss 2.5433, val loss 2.5446\n",
      "step 7230: train loss 2.5851, val loss 2.5885\n",
      "step 7240: train loss 2.5779, val loss 2.5810\n",
      "step 7250: train loss 2.6792, val loss 2.6083\n",
      "step 7260: train loss 2.5576, val loss 2.6445\n",
      "step 7270: train loss 2.5628, val loss 2.6256\n",
      "step 7280: train loss 2.6328, val loss 2.5907\n",
      "step 7290: train loss 2.5630, val loss 2.6614\n",
      "step 7300: train loss 2.6727, val loss 2.6248\n",
      "step 7310: train loss 2.6177, val loss 2.6024\n",
      "step 7320: train loss 2.5595, val loss 2.6873\n",
      "step 7330: train loss 2.5142, val loss 2.6071\n",
      "step 7340: train loss 2.5005, val loss 2.6776\n",
      "step 7350: train loss 2.5295, val loss 2.6448\n",
      "step 7360: train loss 2.6874, val loss 2.5296\n",
      "step 7370: train loss 2.6496, val loss 2.5488\n",
      "step 7380: train loss 2.6724, val loss 2.6225\n",
      "step 7390: train loss 2.5721, val loss 2.5626\n",
      "step 7400: train loss 2.5672, val loss 2.5322\n",
      "step 7410: train loss 2.5535, val loss 2.5017\n",
      "step 7420: train loss 2.6603, val loss 2.5157\n",
      "step 7430: train loss 2.6233, val loss 2.5456\n",
      "step 7440: train loss 2.6178, val loss 2.6061\n",
      "step 7450: train loss 2.6336, val loss 2.6675\n",
      "step 7460: train loss 2.5926, val loss 2.5386\n",
      "step 7470: train loss 2.5117, val loss 2.5737\n",
      "step 7480: train loss 2.5410, val loss 2.5934\n",
      "step 7490: train loss 2.5856, val loss 2.5863\n",
      "step 7500: train loss 2.5954, val loss 2.5790\n",
      "step 7510: train loss 2.6245, val loss 2.6283\n",
      "step 7520: train loss 2.5289, val loss 2.5982\n",
      "step 7530: train loss 2.5906, val loss 2.6062\n",
      "step 7540: train loss 2.6705, val loss 2.6009\n",
      "step 7550: train loss 2.5973, val loss 2.5001\n",
      "step 7560: train loss 2.5964, val loss 2.4812\n",
      "step 7570: train loss 2.5559, val loss 2.5643\n",
      "step 7580: train loss 2.6184, val loss 2.5507\n",
      "step 7590: train loss 2.5888, val loss 2.6753\n",
      "step 7600: train loss 2.5351, val loss 2.5154\n",
      "step 7610: train loss 2.5690, val loss 2.4973\n",
      "step 7620: train loss 2.6205, val loss 2.5725\n",
      "step 7630: train loss 2.5311, val loss 2.5535\n",
      "step 7640: train loss 2.6127, val loss 2.5924\n",
      "step 7650: train loss 2.5053, val loss 2.6129\n",
      "step 7660: train loss 2.5905, val loss 2.5846\n",
      "step 7670: train loss 2.5638, val loss 2.5210\n",
      "step 7680: train loss 2.6291, val loss 2.5948\n",
      "step 7690: train loss 2.6250, val loss 2.5830\n",
      "step 7700: train loss 2.5064, val loss 2.5951\n",
      "step 7710: train loss 2.5786, val loss 2.5299\n",
      "step 7720: train loss 2.5399, val loss 2.4890\n",
      "step 7730: train loss 2.6452, val loss 2.5949\n",
      "step 7740: train loss 2.5852, val loss 2.4997\n",
      "step 7750: train loss 2.5441, val loss 2.5805\n",
      "step 7760: train loss 2.5709, val loss 2.4946\n",
      "step 7770: train loss 2.5274, val loss 2.5991\n",
      "step 7780: train loss 2.4948, val loss 2.5821\n",
      "step 7790: train loss 2.5414, val loss 2.6380\n",
      "step 7800: train loss 2.5504, val loss 2.5548\n",
      "step 7810: train loss 2.6090, val loss 2.5432\n",
      "step 7820: train loss 2.5341, val loss 2.5541\n",
      "step 7830: train loss 2.4828, val loss 2.5348\n",
      "step 7840: train loss 2.5870, val loss 2.5825\n",
      "step 7850: train loss 2.5579, val loss 2.6090\n",
      "step 7860: train loss 2.5642, val loss 2.6132\n",
      "step 7870: train loss 2.5166, val loss 2.5882\n",
      "step 7880: train loss 2.5865, val loss 2.5563\n",
      "step 7890: train loss 2.5928, val loss 2.5744\n",
      "step 7900: train loss 2.5505, val loss 2.5672\n",
      "step 7910: train loss 2.5553, val loss 2.4502\n",
      "step 7920: train loss 2.5885, val loss 2.5157\n",
      "step 7930: train loss 2.5248, val loss 2.5680\n",
      "step 7940: train loss 2.6084, val loss 2.6055\n",
      "step 7950: train loss 2.5579, val loss 2.6099\n",
      "step 7960: train loss 2.6004, val loss 2.5391\n",
      "step 7970: train loss 2.6429, val loss 2.5490\n",
      "step 7980: train loss 2.5608, val loss 2.5306\n",
      "step 7990: train loss 2.5127, val loss 2.5759\n",
      "step 8000: train loss 2.5225, val loss 2.6150\n",
      "step 8010: train loss 2.5841, val loss 2.5091\n",
      "step 8020: train loss 2.4940, val loss 2.5192\n",
      "step 8030: train loss 2.5190, val loss 2.5445\n",
      "step 8040: train loss 2.5315, val loss 2.5686\n",
      "step 8050: train loss 2.5813, val loss 2.5355\n",
      "step 8060: train loss 2.6213, val loss 2.6327\n",
      "step 8070: train loss 2.4901, val loss 2.4714\n",
      "step 8080: train loss 2.5100, val loss 2.5953\n",
      "step 8090: train loss 2.4807, val loss 2.5439\n",
      "step 8100: train loss 2.6097, val loss 2.5371\n",
      "step 8110: train loss 2.5780, val loss 2.6286\n",
      "step 8120: train loss 2.5201, val loss 2.5532\n",
      "step 8130: train loss 2.5635, val loss 2.5515\n",
      "step 8140: train loss 2.5419, val loss 2.5217\n",
      "step 8150: train loss 2.5322, val loss 2.5784\n",
      "step 8160: train loss 2.5457, val loss 2.5083\n",
      "step 8170: train loss 2.5229, val loss 2.6007\n",
      "step 8180: train loss 2.5636, val loss 2.6334\n",
      "step 8190: train loss 2.5625, val loss 2.5384\n",
      "step 8200: train loss 2.4733, val loss 2.5721\n",
      "step 8210: train loss 2.5534, val loss 2.5988\n",
      "step 8220: train loss 2.5269, val loss 2.4778\n",
      "step 8230: train loss 2.5396, val loss 2.6178\n",
      "step 8240: train loss 2.6509, val loss 2.5269\n",
      "step 8250: train loss 2.6089, val loss 2.5287\n",
      "step 8260: train loss 2.4298, val loss 2.5230\n",
      "step 8270: train loss 2.5155, val loss 2.5448\n",
      "step 8280: train loss 2.5231, val loss 2.5303\n",
      "step 8290: train loss 2.5849, val loss 2.5021\n",
      "step 8300: train loss 2.5100, val loss 2.5999\n",
      "step 8310: train loss 2.5041, val loss 2.4947\n",
      "step 8320: train loss 2.5100, val loss 2.5367\n",
      "step 8330: train loss 2.5467, val loss 2.5454\n",
      "step 8340: train loss 2.5668, val loss 2.5422\n",
      "step 8350: train loss 2.5828, val loss 2.5836\n",
      "step 8360: train loss 2.5579, val loss 2.6003\n",
      "step 8370: train loss 2.5632, val loss 2.6409\n",
      "step 8380: train loss 2.5924, val loss 2.5479\n",
      "step 8390: train loss 2.6327, val loss 2.5493\n",
      "step 8400: train loss 2.5417, val loss 2.5601\n",
      "step 8410: train loss 2.5045, val loss 2.5512\n",
      "step 8420: train loss 2.5406, val loss 2.4392\n",
      "step 8430: train loss 2.6095, val loss 2.6360\n",
      "step 8440: train loss 2.6047, val loss 2.5876\n",
      "step 8450: train loss 2.5926, val loss 2.5126\n",
      "step 8460: train loss 2.5418, val loss 2.5443\n",
      "step 8470: train loss 2.5421, val loss 2.5818\n",
      "step 8480: train loss 2.5004, val loss 2.6199\n",
      "step 8490: train loss 2.4667, val loss 2.5639\n",
      "step 8500: train loss 2.5381, val loss 2.5619\n",
      "step 8510: train loss 2.5230, val loss 2.5109\n",
      "step 8520: train loss 2.5834, val loss 2.4810\n",
      "step 8530: train loss 2.5068, val loss 2.5749\n",
      "step 8540: train loss 2.5977, val loss 2.5691\n",
      "step 8550: train loss 2.5284, val loss 2.5327\n",
      "step 8560: train loss 2.5405, val loss 2.4885\n",
      "step 8570: train loss 2.4772, val loss 2.5530\n",
      "step 8580: train loss 2.5730, val loss 2.5304\n",
      "step 8590: train loss 2.5685, val loss 2.5577\n",
      "step 8600: train loss 2.5427, val loss 2.5594\n",
      "step 8610: train loss 2.4963, val loss 2.5536\n",
      "step 8620: train loss 2.5363, val loss 2.5028\n",
      "step 8630: train loss 2.5175, val loss 2.5478\n",
      "step 8640: train loss 2.5171, val loss 2.5828\n",
      "step 8650: train loss 2.5197, val loss 2.5561\n",
      "step 8660: train loss 2.5288, val loss 2.5510\n",
      "step 8670: train loss 2.4494, val loss 2.4802\n",
      "step 8680: train loss 2.5411, val loss 2.5288\n",
      "step 8690: train loss 2.5381, val loss 2.4768\n",
      "step 8700: train loss 2.5567, val loss 2.5462\n",
      "step 8710: train loss 2.5091, val loss 2.4738\n",
      "step 8720: train loss 2.5585, val loss 2.6124\n",
      "step 8730: train loss 2.5163, val loss 2.6238\n",
      "step 8740: train loss 2.6428, val loss 2.5586\n",
      "step 8750: train loss 2.5466, val loss 2.5639\n",
      "step 8760: train loss 2.4598, val loss 2.5794\n",
      "step 8770: train loss 2.4588, val loss 2.6423\n",
      "step 8780: train loss 2.5349, val loss 2.6383\n",
      "step 8790: train loss 2.5239, val loss 2.5200\n",
      "step 8800: train loss 2.5058, val loss 2.5387\n",
      "step 8810: train loss 2.5465, val loss 2.4501\n",
      "step 8820: train loss 2.6031, val loss 2.5181\n",
      "step 8830: train loss 2.5673, val loss 2.5728\n",
      "step 8840: train loss 2.4754, val loss 2.5157\n",
      "step 8850: train loss 2.5451, val loss 2.6219\n",
      "step 8860: train loss 2.6547, val loss 2.5926\n",
      "step 8870: train loss 2.5872, val loss 2.5078\n",
      "step 8880: train loss 2.4520, val loss 2.5848\n",
      "step 8890: train loss 2.5342, val loss 2.5430\n",
      "step 8900: train loss 2.5605, val loss 2.5544\n",
      "step 8910: train loss 2.4271, val loss 2.5397\n",
      "step 8920: train loss 2.4919, val loss 2.4987\n",
      "step 8930: train loss 2.4680, val loss 2.5054\n",
      "step 8940: train loss 2.3895, val loss 2.5077\n",
      "step 8950: train loss 2.4299, val loss 2.5407\n",
      "step 8960: train loss 2.4806, val loss 2.5664\n",
      "step 8970: train loss 2.5308, val loss 2.4874\n",
      "step 8980: train loss 2.4837, val loss 2.4501\n",
      "step 8990: train loss 2.4876, val loss 2.5210\n",
      "step 9000: train loss 2.5821, val loss 2.5660\n",
      "step 9010: train loss 2.4255, val loss 2.5784\n",
      "step 9020: train loss 2.4900, val loss 2.5854\n",
      "step 9030: train loss 2.5911, val loss 2.5066\n",
      "step 9040: train loss 2.4541, val loss 2.5247\n",
      "step 9050: train loss 2.5189, val loss 2.5036\n",
      "step 9060: train loss 2.5029, val loss 2.4952\n",
      "step 9070: train loss 2.5929, val loss 2.5401\n",
      "step 9080: train loss 2.5485, val loss 2.5331\n",
      "step 9090: train loss 2.5395, val loss 2.4880\n",
      "step 9100: train loss 2.4684, val loss 2.4712\n",
      "step 9110: train loss 2.4900, val loss 2.4916\n",
      "step 9120: train loss 2.4516, val loss 2.4772\n",
      "step 9130: train loss 2.5910, val loss 2.5007\n",
      "step 9140: train loss 2.4751, val loss 2.5648\n",
      "step 9150: train loss 2.5010, val loss 2.5410\n",
      "step 9160: train loss 2.5028, val loss 2.4796\n",
      "step 9170: train loss 2.5687, val loss 2.5493\n",
      "step 9180: train loss 2.4643, val loss 2.5599\n",
      "step 9190: train loss 2.4799, val loss 2.4847\n",
      "step 9200: train loss 2.5361, val loss 2.5789\n",
      "step 9210: train loss 2.4999, val loss 2.5144\n",
      "step 9220: train loss 2.5455, val loss 2.4929\n",
      "step 9230: train loss 2.5333, val loss 2.4916\n",
      "step 9240: train loss 2.4963, val loss 2.5356\n",
      "step 9250: train loss 2.6158, val loss 2.4510\n",
      "step 9260: train loss 2.5303, val loss 2.5304\n",
      "step 9270: train loss 2.4413, val loss 2.5735\n",
      "step 9280: train loss 2.5858, val loss 2.4985\n",
      "step 9290: train loss 2.4799, val loss 2.4779\n",
      "step 9300: train loss 2.5516, val loss 2.5181\n",
      "step 9310: train loss 2.5895, val loss 2.5500\n",
      "step 9320: train loss 2.4384, val loss 2.4644\n",
      "step 9330: train loss 2.5440, val loss 2.4777\n",
      "step 9340: train loss 2.4366, val loss 2.4409\n",
      "step 9350: train loss 2.4884, val loss 2.5152\n",
      "step 9360: train loss 2.4833, val loss 2.5351\n",
      "step 9370: train loss 2.5549, val loss 2.4673\n",
      "step 9380: train loss 2.5423, val loss 2.5712\n",
      "step 9390: train loss 2.5316, val loss 2.6245\n",
      "step 9400: train loss 2.5324, val loss 2.4663\n",
      "step 9410: train loss 2.5308, val loss 2.4815\n",
      "step 9420: train loss 2.5037, val loss 2.5165\n",
      "step 9430: train loss 2.4561, val loss 2.5236\n",
      "step 9440: train loss 2.5245, val loss 2.5006\n",
      "step 9450: train loss 2.5332, val loss 2.4543\n",
      "step 9460: train loss 2.5538, val loss 2.4274\n",
      "step 9470: train loss 2.5232, val loss 2.5117\n",
      "step 9480: train loss 2.4730, val loss 2.4458\n",
      "step 9490: train loss 2.5100, val loss 2.4708\n",
      "step 9500: train loss 2.5483, val loss 2.5051\n",
      "step 9510: train loss 2.5722, val loss 2.4511\n",
      "step 9520: train loss 2.4725, val loss 2.4773\n",
      "step 9530: train loss 2.4687, val loss 2.5306\n",
      "step 9540: train loss 2.4982, val loss 2.4842\n",
      "step 9550: train loss 2.4520, val loss 2.5297\n",
      "step 9560: train loss 2.5442, val loss 2.4585\n",
      "step 9570: train loss 2.5028, val loss 2.5453\n",
      "step 9580: train loss 2.4085, val loss 2.5244\n",
      "step 9590: train loss 2.5186, val loss 2.5216\n",
      "step 9600: train loss 2.5632, val loss 2.5663\n",
      "step 9610: train loss 2.5029, val loss 2.4799\n",
      "step 9620: train loss 2.5368, val loss 2.4892\n",
      "step 9630: train loss 2.5612, val loss 2.5623\n",
      "step 9640: train loss 2.5372, val loss 2.5359\n",
      "step 9650: train loss 2.4789, val loss 2.5547\n",
      "step 9660: train loss 2.4894, val loss 2.4499\n",
      "step 9670: train loss 2.5510, val loss 2.4368\n",
      "step 9680: train loss 2.4893, val loss 2.4818\n",
      "step 9690: train loss 2.4601, val loss 2.4727\n",
      "step 9700: train loss 2.5222, val loss 2.5168\n",
      "step 9710: train loss 2.4724, val loss 2.4883\n",
      "step 9720: train loss 2.4816, val loss 2.5485\n",
      "step 9730: train loss 2.5793, val loss 2.5007\n",
      "step 9740: train loss 2.4965, val loss 2.5366\n",
      "step 9750: train loss 2.3897, val loss 2.6177\n",
      "step 9760: train loss 2.5526, val loss 2.5137\n",
      "step 9770: train loss 2.4702, val loss 2.4286\n",
      "step 9780: train loss 2.3856, val loss 2.5880\n",
      "step 9790: train loss 2.4774, val loss 2.4680\n",
      "step 9800: train loss 2.4741, val loss 2.4830\n",
      "step 9810: train loss 2.4968, val loss 2.4163\n",
      "step 9820: train loss 2.4968, val loss 2.4840\n",
      "step 9830: train loss 2.5661, val loss 2.4470\n",
      "step 9840: train loss 2.4860, val loss 2.4350\n",
      "step 9850: train loss 2.4708, val loss 2.4691\n",
      "step 9860: train loss 2.4201, val loss 2.5052\n",
      "step 9870: train loss 2.4481, val loss 2.5300\n",
      "step 9880: train loss 2.4726, val loss 2.5411\n",
      "step 9890: train loss 2.5087, val loss 2.5167\n",
      "step 9900: train loss 2.4305, val loss 2.5099\n",
      "step 9910: train loss 2.5520, val loss 2.5213\n",
      "step 9920: train loss 2.5065, val loss 2.5224\n",
      "step 9930: train loss 2.4621, val loss 2.4269\n",
      "step 9940: train loss 2.5522, val loss 2.4545\n",
      "step 9950: train loss 2.4915, val loss 2.4781\n",
      "step 9960: train loss 2.4400, val loss 2.5090\n",
      "step 9970: train loss 2.4742, val loss 2.5203\n",
      "step 9980: train loss 2.5495, val loss 2.5063\n",
      "step 9990: train loss 2.4603, val loss 2.5483\n",
      "step 10000: train loss 2.4787, val loss 2.4735\n",
      "step 10010: train loss 2.5358, val loss 2.5073\n",
      "step 10020: train loss 2.4185, val loss 2.4745\n",
      "step 10030: train loss 2.6283, val loss 2.4976\n",
      "step 10040: train loss 2.4349, val loss 2.3475\n",
      "step 10050: train loss 2.5201, val loss 2.4734\n",
      "step 10060: train loss 2.5438, val loss 2.4628\n",
      "step 10070: train loss 2.4513, val loss 2.4985\n",
      "step 10080: train loss 2.4947, val loss 2.4150\n",
      "step 10090: train loss 2.4890, val loss 2.5700\n",
      "step 10100: train loss 2.4502, val loss 2.4646\n",
      "step 10110: train loss 2.5464, val loss 2.5225\n",
      "step 10120: train loss 2.5435, val loss 2.5110\n",
      "step 10130: train loss 2.4985, val loss 2.5936\n",
      "step 10140: train loss 2.4499, val loss 2.4320\n",
      "step 10150: train loss 2.5310, val loss 2.5305\n",
      "step 10160: train loss 2.5423, val loss 2.5728\n",
      "step 10170: train loss 2.5509, val loss 2.4883\n",
      "step 10180: train loss 2.5457, val loss 2.5024\n",
      "step 10190: train loss 2.4948, val loss 2.4217\n",
      "step 10200: train loss 2.5202, val loss 2.5249\n",
      "step 10210: train loss 2.5183, val loss 2.4830\n",
      "step 10220: train loss 2.4382, val loss 2.4967\n",
      "step 10230: train loss 2.4138, val loss 2.4938\n",
      "step 10240: train loss 2.4952, val loss 2.4817\n",
      "step 10250: train loss 2.4509, val loss 2.4747\n",
      "step 10260: train loss 2.5293, val loss 2.4668\n",
      "step 10270: train loss 2.5223, val loss 2.4528\n",
      "step 10280: train loss 2.4886, val loss 2.4164\n",
      "step 10290: train loss 2.3993, val loss 2.5294\n",
      "step 10300: train loss 2.4544, val loss 2.5060\n",
      "step 10310: train loss 2.4932, val loss 2.4195\n",
      "step 10320: train loss 2.4889, val loss 2.4586\n",
      "step 10330: train loss 2.5285, val loss 2.5484\n",
      "step 10340: train loss 2.4929, val loss 2.4961\n",
      "step 10350: train loss 2.3860, val loss 2.5136\n",
      "step 10360: train loss 2.5439, val loss 2.4810\n",
      "step 10370: train loss 2.4682, val loss 2.3972\n",
      "step 10380: train loss 2.4140, val loss 2.3962\n",
      "step 10390: train loss 2.4703, val loss 2.4519\n",
      "step 10400: train loss 2.4639, val loss 2.4990\n",
      "step 10410: train loss 2.4808, val loss 2.4834\n",
      "step 10420: train loss 2.4059, val loss 2.5000\n",
      "step 10430: train loss 2.4371, val loss 2.4400\n",
      "step 10440: train loss 2.5458, val loss 2.4788\n",
      "step 10450: train loss 2.6126, val loss 2.4616\n",
      "step 10460: train loss 2.4492, val loss 2.4882\n",
      "step 10470: train loss 2.5299, val loss 2.4747\n",
      "step 10480: train loss 2.5222, val loss 2.4957\n",
      "step 10490: train loss 2.4196, val loss 2.5537\n",
      "step 10500: train loss 2.4226, val loss 2.4723\n",
      "step 10510: train loss 2.4161, val loss 2.4373\n",
      "step 10520: train loss 2.6076, val loss 2.4787\n",
      "step 10530: train loss 2.5001, val loss 2.4437\n",
      "step 10540: train loss 2.4853, val loss 2.5747\n",
      "step 10550: train loss 2.4539, val loss 2.4626\n",
      "step 10560: train loss 2.3797, val loss 2.4143\n",
      "step 10570: train loss 2.4747, val loss 2.5133\n",
      "step 10580: train loss 2.5507, val loss 2.5443\n",
      "step 10590: train loss 2.5389, val loss 2.4552\n",
      "step 10600: train loss 2.5106, val loss 2.4218\n",
      "step 10610: train loss 2.4905, val loss 2.4492\n",
      "step 10620: train loss 2.4665, val loss 2.4585\n",
      "step 10630: train loss 2.4869, val loss 2.4376\n",
      "step 10640: train loss 2.4873, val loss 2.4449\n",
      "step 10650: train loss 2.4929, val loss 2.4728\n",
      "step 10660: train loss 2.4696, val loss 2.4815\n",
      "step 10670: train loss 2.4953, val loss 2.4871\n",
      "step 10680: train loss 2.5015, val loss 2.5540\n",
      "step 10690: train loss 2.5182, val loss 2.4484\n",
      "step 10700: train loss 2.4560, val loss 2.4756\n",
      "step 10710: train loss 2.4800, val loss 2.5233\n",
      "step 10720: train loss 2.4986, val loss 2.4277\n",
      "step 10730: train loss 2.5275, val loss 2.5271\n",
      "step 10740: train loss 2.5018, val loss 2.5085\n",
      "step 10750: train loss 2.5430, val loss 2.4410\n",
      "step 10760: train loss 2.4477, val loss 2.5183\n",
      "step 10770: train loss 2.4804, val loss 2.4855\n",
      "step 10780: train loss 2.4745, val loss 2.4275\n",
      "step 10790: train loss 2.4391, val loss 2.4632\n",
      "step 10800: train loss 2.5350, val loss 2.4274\n",
      "step 10810: train loss 2.4226, val loss 2.4170\n",
      "step 10820: train loss 2.4494, val loss 2.5065\n",
      "step 10830: train loss 2.4468, val loss 2.5092\n",
      "step 10840: train loss 2.4661, val loss 2.5213\n",
      "step 10850: train loss 2.5241, val loss 2.3938\n",
      "step 10860: train loss 2.4873, val loss 2.4850\n",
      "step 10870: train loss 2.5082, val loss 2.4661\n",
      "step 10880: train loss 2.4350, val loss 2.5429\n",
      "step 10890: train loss 2.4529, val loss 2.4066\n",
      "step 10900: train loss 2.5347, val loss 2.4821\n",
      "step 10910: train loss 2.4686, val loss 2.4872\n",
      "step 10920: train loss 2.4146, val loss 2.4612\n",
      "step 10930: train loss 2.4700, val loss 2.4302\n",
      "step 10940: train loss 2.4683, val loss 2.4608\n",
      "step 10950: train loss 2.4363, val loss 2.4015\n",
      "step 10960: train loss 2.4992, val loss 2.4003\n",
      "step 10970: train loss 2.4739, val loss 2.4207\n",
      "step 10980: train loss 2.4764, val loss 2.5157\n",
      "step 10990: train loss 2.4838, val loss 2.4655\n",
      "step 11000: train loss 2.4071, val loss 2.4773\n",
      "step 11010: train loss 2.5029, val loss 2.4701\n",
      "step 11020: train loss 2.5504, val loss 2.4823\n",
      "step 11030: train loss 2.5689, val loss 2.4239\n",
      "step 11040: train loss 2.4717, val loss 2.3850\n",
      "step 11050: train loss 2.4113, val loss 2.4256\n",
      "step 11060: train loss 2.4460, val loss 2.4367\n",
      "step 11070: train loss 2.4116, val loss 2.4713\n",
      "step 11080: train loss 2.5488, val loss 2.3846\n",
      "step 11090: train loss 2.5026, val loss 2.4880\n",
      "step 11100: train loss 2.4665, val loss 2.3690\n",
      "step 11110: train loss 2.5586, val loss 2.4227\n",
      "step 11120: train loss 2.5016, val loss 2.3537\n",
      "step 11130: train loss 2.4549, val loss 2.4440\n",
      "step 11140: train loss 2.4686, val loss 2.4886\n",
      "step 11150: train loss 2.5308, val loss 2.4496\n",
      "step 11160: train loss 2.4253, val loss 2.5351\n",
      "step 11170: train loss 2.4406, val loss 2.4559\n",
      "step 11180: train loss 2.5142, val loss 2.4935\n",
      "step 11190: train loss 2.4327, val loss 2.5618\n",
      "step 11200: train loss 2.5307, val loss 2.4824\n",
      "step 11210: train loss 2.4217, val loss 2.4488\n",
      "step 11220: train loss 2.4512, val loss 2.3882\n",
      "step 11230: train loss 2.4596, val loss 2.5079\n",
      "step 11240: train loss 2.3961, val loss 2.4262\n",
      "step 11250: train loss 2.4547, val loss 2.5336\n",
      "step 11260: train loss 2.4278, val loss 2.3931\n",
      "step 11270: train loss 2.4228, val loss 2.4496\n",
      "step 11280: train loss 2.4905, val loss 2.4723\n",
      "step 11290: train loss 2.4272, val loss 2.4277\n",
      "step 11300: train loss 2.4026, val loss 2.4979\n",
      "step 11310: train loss 2.4553, val loss 2.3418\n",
      "step 11320: train loss 2.4473, val loss 2.4084\n",
      "step 11330: train loss 2.4148, val loss 2.4588\n",
      "step 11340: train loss 2.4742, val loss 2.5002\n",
      "step 11350: train loss 2.5022, val loss 2.5491\n",
      "step 11360: train loss 2.4974, val loss 2.4593\n",
      "step 11370: train loss 2.4243, val loss 2.5770\n",
      "step 11380: train loss 2.4185, val loss 2.5322\n",
      "step 11390: train loss 2.5741, val loss 2.4590\n",
      "step 11400: train loss 2.4361, val loss 2.4892\n",
      "step 11410: train loss 2.4785, val loss 2.3989\n",
      "step 11420: train loss 2.3867, val loss 2.4284\n",
      "step 11430: train loss 2.5014, val loss 2.5466\n",
      "step 11440: train loss 2.4793, val loss 2.4281\n",
      "step 11450: train loss 2.3798, val loss 2.3844\n",
      "step 11460: train loss 2.4983, val loss 2.4338\n",
      "step 11470: train loss 2.4791, val loss 2.3773\n",
      "step 11480: train loss 2.4913, val loss 2.5266\n",
      "step 11490: train loss 2.4761, val loss 2.4685\n",
      "step 11500: train loss 2.4517, val loss 2.5073\n",
      "step 11510: train loss 2.5172, val loss 2.3983\n",
      "step 11520: train loss 2.4376, val loss 2.4259\n",
      "step 11530: train loss 2.4144, val loss 2.4568\n",
      "step 11540: train loss 2.3986, val loss 2.4536\n",
      "step 11550: train loss 2.4000, val loss 2.5462\n",
      "step 11560: train loss 2.4729, val loss 2.4323\n",
      "step 11570: train loss 2.4615, val loss 2.4488\n",
      "step 11580: train loss 2.4377, val loss 2.4943\n",
      "step 11590: train loss 2.3721, val loss 2.4599\n",
      "step 11600: train loss 2.4972, val loss 2.4434\n",
      "step 11610: train loss 2.4482, val loss 2.5143\n",
      "step 11620: train loss 2.4000, val loss 2.4083\n",
      "step 11630: train loss 2.5233, val loss 2.4722\n",
      "step 11640: train loss 2.3882, val loss 2.3872\n",
      "step 11650: train loss 2.4030, val loss 2.4921\n",
      "step 11660: train loss 2.4409, val loss 2.4951\n",
      "step 11670: train loss 2.4996, val loss 2.4130\n",
      "step 11680: train loss 2.4422, val loss 2.4246\n",
      "step 11690: train loss 2.4270, val loss 2.5132\n",
      "step 11700: train loss 2.4205, val loss 2.3829\n",
      "step 11710: train loss 2.3724, val loss 2.4570\n",
      "step 11720: train loss 2.4567, val loss 2.4370\n",
      "step 11730: train loss 2.4679, val loss 2.4702\n",
      "step 11740: train loss 2.4291, val loss 2.4561\n",
      "step 11750: train loss 2.4541, val loss 2.5515\n",
      "step 11760: train loss 2.5107, val loss 2.4583\n",
      "step 11770: train loss 2.4467, val loss 2.4904\n",
      "step 11780: train loss 2.4647, val loss 2.5111\n",
      "step 11790: train loss 2.4160, val loss 2.3789\n",
      "step 11800: train loss 2.4975, val loss 2.4444\n",
      "step 11810: train loss 2.5017, val loss 2.4300\n",
      "step 11820: train loss 2.3814, val loss 2.4317\n",
      "step 11830: train loss 2.4213, val loss 2.4502\n",
      "step 11840: train loss 2.4716, val loss 2.4549\n",
      "step 11850: train loss 2.4900, val loss 2.5057\n",
      "step 11860: train loss 2.4340, val loss 2.4951\n",
      "step 11870: train loss 2.4344, val loss 2.4434\n",
      "step 11880: train loss 2.4499, val loss 2.4818\n",
      "step 11890: train loss 2.4040, val loss 2.4777\n",
      "step 11900: train loss 2.3565, val loss 2.4182\n",
      "step 11910: train loss 2.4377, val loss 2.4644\n",
      "step 11920: train loss 2.4682, val loss 2.4291\n",
      "step 11930: train loss 2.5192, val loss 2.5317\n",
      "step 11940: train loss 2.4461, val loss 2.4333\n",
      "step 11950: train loss 2.4774, val loss 2.4067\n",
      "step 11960: train loss 2.4044, val loss 2.3916\n",
      "step 11970: train loss 2.4474, val loss 2.4129\n",
      "step 11980: train loss 2.4477, val loss 2.4483\n",
      "step 11990: train loss 2.5035, val loss 2.5127\n",
      "step 12000: train loss 2.4187, val loss 2.4720\n",
      "step 12010: train loss 2.4906, val loss 2.4934\n",
      "step 12020: train loss 2.4623, val loss 2.4565\n",
      "step 12030: train loss 2.4549, val loss 2.4264\n",
      "step 12040: train loss 2.4036, val loss 2.4481\n",
      "step 12050: train loss 2.4363, val loss 2.4830\n",
      "step 12060: train loss 2.4566, val loss 2.3733\n",
      "step 12070: train loss 2.4147, val loss 2.4989\n",
      "step 12080: train loss 2.4363, val loss 2.4727\n",
      "step 12090: train loss 2.4440, val loss 2.4642\n",
      "step 12100: train loss 2.4475, val loss 2.4640\n",
      "step 12110: train loss 2.4038, val loss 2.3802\n",
      "step 12120: train loss 2.4540, val loss 2.4196\n",
      "step 12130: train loss 2.4560, val loss 2.5177\n",
      "step 12140: train loss 2.4089, val loss 2.4592\n",
      "step 12150: train loss 2.4212, val loss 2.4970\n",
      "step 12160: train loss 2.4053, val loss 2.4817\n",
      "step 12170: train loss 2.4340, val loss 2.4531\n",
      "step 12180: train loss 2.3396, val loss 2.4849\n",
      "step 12190: train loss 2.5087, val loss 2.4438\n",
      "step 12200: train loss 2.4659, val loss 2.4895\n",
      "step 12210: train loss 2.4775, val loss 2.5306\n",
      "step 12220: train loss 2.4006, val loss 2.4197\n",
      "step 12230: train loss 2.4952, val loss 2.5051\n",
      "step 12240: train loss 2.4129, val loss 2.4475\n",
      "step 12250: train loss 2.4561, val loss 2.4101\n",
      "step 12260: train loss 2.3654, val loss 2.3580\n",
      "step 12270: train loss 2.4364, val loss 2.4959\n",
      "step 12280: train loss 2.4637, val loss 2.4529\n",
      "step 12290: train loss 2.4694, val loss 2.4759\n",
      "step 12300: train loss 2.4852, val loss 2.4710\n",
      "step 12310: train loss 2.4505, val loss 2.4769\n",
      "step 12320: train loss 2.4463, val loss 2.4102\n",
      "step 12330: train loss 2.4948, val loss 2.4209\n",
      "step 12340: train loss 2.4628, val loss 2.5165\n",
      "step 12350: train loss 2.4659, val loss 2.4836\n",
      "step 12360: train loss 2.4299, val loss 2.4494\n",
      "step 12370: train loss 2.4649, val loss 2.4958\n",
      "step 12380: train loss 2.5042, val loss 2.4170\n",
      "step 12390: train loss 2.4050, val loss 2.3843\n",
      "step 12400: train loss 2.4161, val loss 2.4181\n",
      "step 12410: train loss 2.4406, val loss 2.5034\n",
      "step 12420: train loss 2.4579, val loss 2.4035\n",
      "step 12430: train loss 2.4995, val loss 2.3657\n",
      "step 12440: train loss 2.4423, val loss 2.4308\n",
      "step 12450: train loss 2.5089, val loss 2.4114\n",
      "step 12460: train loss 2.5058, val loss 2.4855\n",
      "step 12470: train loss 2.3679, val loss 2.5247\n",
      "step 12480: train loss 2.3912, val loss 2.4058\n",
      "step 12490: train loss 2.3999, val loss 2.4732\n",
      "step 12500: train loss 2.3755, val loss 2.5449\n",
      "step 12510: train loss 2.5358, val loss 2.5304\n",
      "step 12520: train loss 2.5171, val loss 2.4997\n",
      "step 12530: train loss 2.4670, val loss 2.4510\n",
      "step 12540: train loss 2.5187, val loss 2.3807\n",
      "step 12550: train loss 2.4715, val loss 2.3884\n",
      "step 12560: train loss 2.4587, val loss 2.4017\n",
      "step 12570: train loss 2.3935, val loss 2.4142\n",
      "step 12580: train loss 2.3916, val loss 2.4078\n",
      "step 12590: train loss 2.4478, val loss 2.4466\n",
      "step 12600: train loss 2.4161, val loss 2.4347\n",
      "step 12610: train loss 2.4284, val loss 2.4841\n",
      "step 12620: train loss 2.5016, val loss 2.4372\n",
      "step 12630: train loss 2.4526, val loss 2.3357\n",
      "step 12640: train loss 2.4908, val loss 2.4769\n",
      "step 12650: train loss 2.4637, val loss 2.4857\n",
      "step 12660: train loss 2.5247, val loss 2.4496\n",
      "step 12670: train loss 2.4610, val loss 2.5024\n",
      "step 12680: train loss 2.4187, val loss 2.3656\n",
      "step 12690: train loss 2.4457, val loss 2.4782\n",
      "step 12700: train loss 2.4498, val loss 2.4055\n",
      "step 12710: train loss 2.4122, val loss 2.3756\n",
      "step 12720: train loss 2.4468, val loss 2.3901\n",
      "step 12730: train loss 2.4719, val loss 2.3861\n",
      "step 12740: train loss 2.5342, val loss 2.5023\n",
      "step 12750: train loss 2.5182, val loss 2.3964\n",
      "step 12760: train loss 2.4258, val loss 2.4735\n",
      "step 12770: train loss 2.4735, val loss 2.4232\n",
      "step 12780: train loss 2.4737, val loss 2.4376\n",
      "step 12790: train loss 2.4054, val loss 2.3315\n",
      "step 12800: train loss 2.4622, val loss 2.3749\n",
      "step 12810: train loss 2.4745, val loss 2.4288\n",
      "step 12820: train loss 2.3782, val loss 2.4132\n",
      "step 12830: train loss 2.4595, val loss 2.3791\n",
      "step 12840: train loss 2.4255, val loss 2.5259\n",
      "step 12850: train loss 2.4567, val loss 2.4579\n",
      "step 12860: train loss 2.4539, val loss 2.4625\n",
      "step 12870: train loss 2.4494, val loss 2.5038\n",
      "step 12880: train loss 2.4597, val loss 2.4762\n",
      "step 12890: train loss 2.5144, val loss 2.4807\n",
      "step 12900: train loss 2.4065, val loss 2.4056\n",
      "step 12910: train loss 2.4694, val loss 2.4061\n",
      "step 12920: train loss 2.4481, val loss 2.4454\n",
      "step 12930: train loss 2.4322, val loss 2.4069\n",
      "step 12940: train loss 2.4448, val loss 2.4379\n",
      "step 12950: train loss 2.4062, val loss 2.4524\n",
      "step 12960: train loss 2.4153, val loss 2.4461\n",
      "step 12970: train loss 2.3699, val loss 2.4006\n",
      "step 12980: train loss 2.3487, val loss 2.4359\n",
      "step 12990: train loss 2.4123, val loss 2.4430\n",
      "step 13000: train loss 2.4523, val loss 2.4031\n",
      "step 13010: train loss 2.3466, val loss 2.5186\n",
      "step 13020: train loss 2.3950, val loss 2.3987\n",
      "step 13030: train loss 2.4309, val loss 2.5179\n",
      "step 13040: train loss 2.4749, val loss 2.3756\n",
      "step 13050: train loss 2.4645, val loss 2.4316\n",
      "step 13060: train loss 2.4380, val loss 2.4186\n",
      "step 13070: train loss 2.4623, val loss 2.3789\n",
      "step 13080: train loss 2.4163, val loss 2.4603\n",
      "step 13090: train loss 2.3873, val loss 2.4226\n",
      "step 13100: train loss 2.4626, val loss 2.4304\n",
      "step 13110: train loss 2.4339, val loss 2.4270\n",
      "step 13120: train loss 2.5147, val loss 2.3984\n",
      "step 13130: train loss 2.4048, val loss 2.4306\n",
      "step 13140: train loss 2.4146, val loss 2.4420\n",
      "step 13150: train loss 2.4994, val loss 2.4516\n",
      "step 13160: train loss 2.4500, val loss 2.4431\n",
      "step 13170: train loss 2.4271, val loss 2.3654\n",
      "step 13180: train loss 2.4279, val loss 2.4342\n",
      "step 13190: train loss 2.3991, val loss 2.4247\n",
      "step 13200: train loss 2.4775, val loss 2.4146\n",
      "step 13210: train loss 2.4288, val loss 2.4632\n",
      "step 13220: train loss 2.5147, val loss 2.4830\n",
      "step 13230: train loss 2.3831, val loss 2.4172\n",
      "step 13240: train loss 2.4779, val loss 2.4482\n",
      "step 13250: train loss 2.4377, val loss 2.4658\n",
      "step 13260: train loss 2.5063, val loss 2.4156\n",
      "step 13270: train loss 2.4888, val loss 2.4139\n",
      "step 13280: train loss 2.4800, val loss 2.3895\n",
      "step 13290: train loss 2.4387, val loss 2.4002\n",
      "step 13300: train loss 2.4494, val loss 2.4697\n",
      "step 13310: train loss 2.4475, val loss 2.3482\n",
      "step 13320: train loss 2.4206, val loss 2.4281\n",
      "step 13330: train loss 2.5174, val loss 2.4321\n",
      "step 13340: train loss 2.3762, val loss 2.4830\n",
      "step 13350: train loss 2.4263, val loss 2.4720\n",
      "step 13360: train loss 2.4563, val loss 2.3768\n",
      "step 13370: train loss 2.4225, val loss 2.4371\n",
      "step 13380: train loss 2.4431, val loss 2.3959\n",
      "step 13390: train loss 2.3788, val loss 2.5605\n",
      "step 13400: train loss 2.4159, val loss 2.4389\n",
      "step 13410: train loss 2.3779, val loss 2.4110\n",
      "step 13420: train loss 2.4849, val loss 2.4232\n",
      "step 13430: train loss 2.5145, val loss 2.3835\n",
      "step 13440: train loss 2.4150, val loss 2.4341\n",
      "step 13450: train loss 2.5017, val loss 2.4683\n",
      "step 13460: train loss 2.4191, val loss 2.4738\n",
      "step 13470: train loss 2.3817, val loss 2.4126\n",
      "step 13480: train loss 2.4522, val loss 2.4036\n",
      "step 13490: train loss 2.4393, val loss 2.3888\n",
      "step 13500: train loss 2.4492, val loss 2.3678\n",
      "step 13510: train loss 2.4905, val loss 2.3905\n",
      "step 13520: train loss 2.4347, val loss 2.4137\n",
      "step 13530: train loss 2.4681, val loss 2.4711\n",
      "step 13540: train loss 2.4287, val loss 2.4947\n",
      "step 13550: train loss 2.4058, val loss 2.4274\n",
      "step 13560: train loss 2.4342, val loss 2.4009\n",
      "step 13570: train loss 2.4658, val loss 2.4108\n",
      "step 13580: train loss 2.3680, val loss 2.4592\n",
      "step 13590: train loss 2.4278, val loss 2.3975\n",
      "step 13600: train loss 2.4007, val loss 2.4825\n",
      "step 13610: train loss 2.4412, val loss 2.4646\n",
      "step 13620: train loss 2.4225, val loss 2.4503\n",
      "step 13630: train loss 2.3623, val loss 2.4179\n",
      "step 13640: train loss 2.4014, val loss 2.4922\n",
      "step 13650: train loss 2.4399, val loss 2.3626\n",
      "step 13660: train loss 2.4330, val loss 2.3883\n",
      "step 13670: train loss 2.4321, val loss 2.3564\n",
      "step 13680: train loss 2.5169, val loss 2.4825\n",
      "step 13690: train loss 2.4499, val loss 2.3963\n",
      "step 13700: train loss 2.4442, val loss 2.4478\n",
      "step 13710: train loss 2.3951, val loss 2.5046\n",
      "step 13720: train loss 2.4429, val loss 2.4245\n",
      "step 13730: train loss 2.4694, val loss 2.4654\n",
      "step 13740: train loss 2.5476, val loss 2.3501\n",
      "step 13750: train loss 2.4665, val loss 2.3847\n",
      "step 13760: train loss 2.4152, val loss 2.4022\n",
      "step 13770: train loss 2.4559, val loss 2.3950\n",
      "step 13780: train loss 2.3782, val loss 2.4299\n",
      "step 13790: train loss 2.3602, val loss 2.4359\n",
      "step 13800: train loss 2.5143, val loss 2.3981\n",
      "step 13810: train loss 2.4486, val loss 2.4653\n",
      "step 13820: train loss 2.5029, val loss 2.4841\n",
      "step 13830: train loss 2.4453, val loss 2.4739\n",
      "step 13840: train loss 2.3420, val loss 2.3841\n",
      "step 13850: train loss 2.4265, val loss 2.4644\n",
      "step 13860: train loss 2.4791, val loss 2.4634\n",
      "step 13870: train loss 2.4254, val loss 2.3639\n",
      "step 13880: train loss 2.4445, val loss 2.4201\n",
      "step 13890: train loss 2.4200, val loss 2.4544\n",
      "step 13900: train loss 2.5512, val loss 2.4791\n",
      "step 13910: train loss 2.4265, val loss 2.3906\n",
      "step 13920: train loss 2.4445, val loss 2.4761\n",
      "step 13930: train loss 2.4626, val loss 2.3399\n",
      "step 13940: train loss 2.4561, val loss 2.4912\n",
      "step 13950: train loss 2.4765, val loss 2.4784\n",
      "step 13960: train loss 2.4787, val loss 2.4632\n",
      "step 13970: train loss 2.4607, val loss 2.3948\n",
      "step 13980: train loss 2.4251, val loss 2.4068\n",
      "step 13990: train loss 2.4664, val loss 2.3624\n",
      "step 14000: train loss 2.4802, val loss 2.3718\n",
      "step 14010: train loss 2.4325, val loss 2.4584\n",
      "step 14020: train loss 2.3813, val loss 2.4560\n",
      "step 14030: train loss 2.3802, val loss 2.3811\n",
      "step 14040: train loss 2.4513, val loss 2.4065\n",
      "step 14050: train loss 2.4512, val loss 2.4640\n",
      "step 14060: train loss 2.3856, val loss 2.5152\n",
      "step 14070: train loss 2.4030, val loss 2.4558\n",
      "step 14080: train loss 2.4255, val loss 2.3675\n",
      "step 14090: train loss 2.4364, val loss 2.5156\n",
      "step 14100: train loss 2.4656, val loss 2.3688\n",
      "step 14110: train loss 2.4837, val loss 2.4193\n",
      "step 14120: train loss 2.3195, val loss 2.4395\n",
      "step 14130: train loss 2.4421, val loss 2.4207\n",
      "step 14140: train loss 2.4003, val loss 2.3826\n",
      "step 14150: train loss 2.5038, val loss 2.4106\n",
      "step 14160: train loss 2.3935, val loss 2.3598\n",
      "step 14170: train loss 2.3959, val loss 2.5063\n",
      "step 14180: train loss 2.4619, val loss 2.4496\n",
      "step 14190: train loss 2.4799, val loss 2.4453\n",
      "step 14200: train loss 2.4395, val loss 2.4504\n",
      "step 14210: train loss 2.4917, val loss 2.3718\n",
      "step 14220: train loss 2.5066, val loss 2.3862\n",
      "step 14230: train loss 2.3962, val loss 2.4453\n",
      "step 14240: train loss 2.4426, val loss 2.4780\n",
      "step 14250: train loss 2.5411, val loss 2.4828\n",
      "step 14260: train loss 2.3716, val loss 2.4020\n",
      "step 14270: train loss 2.4505, val loss 2.4306\n",
      "step 14280: train loss 2.4457, val loss 2.3901\n",
      "step 14290: train loss 2.3957, val loss 2.3506\n",
      "step 14300: train loss 2.3454, val loss 2.3122\n",
      "step 14310: train loss 2.4508, val loss 2.4716\n",
      "step 14320: train loss 2.4626, val loss 2.4104\n",
      "step 14330: train loss 2.4559, val loss 2.3767\n",
      "step 14340: train loss 2.3732, val loss 2.4585\n",
      "step 14350: train loss 2.4405, val loss 2.3943\n",
      "step 14360: train loss 2.4472, val loss 2.4008\n",
      "step 14370: train loss 2.4116, val loss 2.4225\n",
      "step 14380: train loss 2.3973, val loss 2.3797\n",
      "step 14390: train loss 2.4162, val loss 2.4363\n",
      "step 14400: train loss 2.4738, val loss 2.4103\n",
      "step 14410: train loss 2.4173, val loss 2.3765\n",
      "step 14420: train loss 2.4675, val loss 2.3936\n",
      "step 14430: train loss 2.4677, val loss 2.4123\n",
      "step 14440: train loss 2.4616, val loss 2.3799\n",
      "step 14450: train loss 2.4794, val loss 2.5186\n",
      "step 14460: train loss 2.3668, val loss 2.3912\n",
      "step 14470: train loss 2.4531, val loss 2.3380\n",
      "step 14480: train loss 2.3939, val loss 2.5166\n",
      "step 14490: train loss 2.4262, val loss 2.3621\n",
      "step 14500: train loss 2.4988, val loss 2.4195\n",
      "step 14510: train loss 2.4738, val loss 2.4729\n",
      "step 14520: train loss 2.4407, val loss 2.3721\n",
      "step 14530: train loss 2.5195, val loss 2.4085\n",
      "step 14540: train loss 2.3813, val loss 2.5161\n",
      "step 14550: train loss 2.3927, val loss 2.4788\n",
      "step 14560: train loss 2.3187, val loss 2.4425\n",
      "step 14570: train loss 2.4548, val loss 2.3997\n",
      "step 14580: train loss 2.4768, val loss 2.3651\n",
      "step 14590: train loss 2.4343, val loss 2.4166\n",
      "step 14600: train loss 2.4042, val loss 2.3872\n",
      "step 14610: train loss 2.4395, val loss 2.3996\n",
      "step 14620: train loss 2.3769, val loss 2.3880\n",
      "step 14630: train loss 2.3796, val loss 2.4755\n",
      "step 14640: train loss 2.4600, val loss 2.4694\n",
      "step 14650: train loss 2.3614, val loss 2.4352\n",
      "step 14660: train loss 2.4871, val loss 2.4248\n",
      "step 14670: train loss 2.4374, val loss 2.3604\n",
      "step 14680: train loss 2.4206, val loss 2.4528\n",
      "step 14690: train loss 2.4118, val loss 2.4931\n",
      "step 14700: train loss 2.4925, val loss 2.4071\n",
      "step 14710: train loss 2.4244, val loss 2.3988\n",
      "step 14720: train loss 2.4428, val loss 2.3943\n",
      "step 14730: train loss 2.4832, val loss 2.4388\n",
      "step 14740: train loss 2.3879, val loss 2.4596\n",
      "step 14750: train loss 2.4175, val loss 2.4576\n",
      "step 14760: train loss 2.3995, val loss 2.5233\n",
      "step 14770: train loss 2.4306, val loss 2.4028\n",
      "step 14780: train loss 2.4040, val loss 2.4433\n",
      "step 14790: train loss 2.4682, val loss 2.4016\n",
      "step 14800: train loss 2.4211, val loss 2.3547\n",
      "step 14810: train loss 2.4616, val loss 2.4028\n",
      "step 14820: train loss 2.4268, val loss 2.4388\n",
      "step 14830: train loss 2.3731, val loss 2.4091\n",
      "step 14840: train loss 2.3766, val loss 2.3916\n",
      "step 14850: train loss 2.3412, val loss 2.4274\n",
      "step 14860: train loss 2.3837, val loss 2.4771\n",
      "step 14870: train loss 2.4710, val loss 2.4060\n",
      "step 14880: train loss 2.3806, val loss 2.4409\n",
      "step 14890: train loss 2.3633, val loss 2.5130\n",
      "step 14900: train loss 2.3808, val loss 2.3963\n",
      "step 14910: train loss 2.4424, val loss 2.4274\n",
      "step 14920: train loss 2.4222, val loss 2.3820\n",
      "step 14930: train loss 2.4299, val loss 2.4826\n",
      "step 14940: train loss 2.4582, val loss 2.4045\n",
      "step 14950: train loss 2.4190, val loss 2.5129\n",
      "step 14960: train loss 2.4209, val loss 2.3826\n",
      "step 14970: train loss 2.4297, val loss 2.4750\n",
      "step 14980: train loss 2.4205, val loss 2.4158\n",
      "step 14990: train loss 2.4129, val loss 2.3749\n",
      "step 15000: train loss 2.3842, val loss 2.3285\n",
      "step 15010: train loss 2.5218, val loss 2.4351\n",
      "step 15020: train loss 2.4208, val loss 2.4263\n",
      "step 15030: train loss 2.4359, val loss 2.4223\n",
      "step 15040: train loss 2.4376, val loss 2.3981\n",
      "step 15050: train loss 2.4653, val loss 2.4491\n",
      "step 15060: train loss 2.4245, val loss 2.4052\n",
      "step 15070: train loss 2.4873, val loss 2.4229\n",
      "step 15080: train loss 2.4522, val loss 2.3761\n",
      "step 15090: train loss 2.3603, val loss 2.4717\n",
      "step 15100: train loss 2.4487, val loss 2.4104\n",
      "step 15110: train loss 2.4185, val loss 2.3428\n",
      "step 15120: train loss 2.4831, val loss 2.3838\n",
      "step 15130: train loss 2.4194, val loss 2.3565\n",
      "step 15140: train loss 2.4629, val loss 2.3987\n",
      "step 15150: train loss 2.3529, val loss 2.4106\n",
      "step 15160: train loss 2.4048, val loss 2.3964\n",
      "step 15170: train loss 2.4248, val loss 2.3987\n",
      "step 15180: train loss 2.4353, val loss 2.4669\n",
      "step 15190: train loss 2.5307, val loss 2.4433\n",
      "step 15200: train loss 2.4167, val loss 2.4968\n",
      "step 15210: train loss 2.4070, val loss 2.3666\n",
      "step 15220: train loss 2.4485, val loss 2.3790\n",
      "step 15230: train loss 2.4192, val loss 2.4404\n",
      "step 15240: train loss 2.5197, val loss 2.3944\n",
      "step 15250: train loss 2.4802, val loss 2.4082\n",
      "step 15260: train loss 2.4315, val loss 2.3308\n",
      "step 15270: train loss 2.3833, val loss 2.3776\n",
      "step 15280: train loss 2.3460, val loss 2.3946\n",
      "step 15290: train loss 2.4210, val loss 2.4045\n",
      "step 15300: train loss 2.4256, val loss 2.3776\n",
      "step 15310: train loss 2.4632, val loss 2.5193\n",
      "step 15320: train loss 2.3803, val loss 2.3984\n",
      "step 15330: train loss 2.3979, val loss 2.3774\n",
      "step 15340: train loss 2.4228, val loss 2.3834\n",
      "step 15350: train loss 2.3426, val loss 2.4610\n",
      "step 15360: train loss 2.4689, val loss 2.3575\n",
      "step 15370: train loss 2.4614, val loss 2.3691\n",
      "step 15380: train loss 2.3870, val loss 2.4171\n",
      "step 15390: train loss 2.4300, val loss 2.4234\n",
      "step 15400: train loss 2.3985, val loss 2.4334\n",
      "step 15410: train loss 2.4057, val loss 2.4028\n",
      "step 15420: train loss 2.4650, val loss 2.4202\n",
      "step 15430: train loss 2.3800, val loss 2.4405\n",
      "step 15440: train loss 2.4485, val loss 2.3363\n",
      "step 15450: train loss 2.4392, val loss 2.4701\n",
      "step 15460: train loss 2.3927, val loss 2.4713\n",
      "step 15470: train loss 2.4230, val loss 2.3914\n",
      "step 15480: train loss 2.3204, val loss 2.4523\n",
      "step 15490: train loss 2.4282, val loss 2.4430\n",
      "step 15500: train loss 2.4685, val loss 2.3165\n",
      "step 15510: train loss 2.4440, val loss 2.3903\n",
      "step 15520: train loss 2.4015, val loss 2.3817\n",
      "step 15530: train loss 2.4859, val loss 2.3684\n",
      "step 15540: train loss 2.4277, val loss 2.4378\n",
      "step 15550: train loss 2.4401, val loss 2.4277\n",
      "step 15560: train loss 2.3898, val loss 2.3676\n",
      "step 15570: train loss 2.4410, val loss 2.3992\n",
      "step 15580: train loss 2.4427, val loss 2.5099\n",
      "step 15590: train loss 2.3612, val loss 2.4543\n",
      "step 15600: train loss 2.3435, val loss 2.4532\n",
      "step 15610: train loss 2.3793, val loss 2.5033\n",
      "step 15620: train loss 2.3825, val loss 2.4383\n",
      "step 15630: train loss 2.4519, val loss 2.4401\n",
      "step 15640: train loss 2.4194, val loss 2.3791\n",
      "step 15650: train loss 2.3413, val loss 2.3854\n",
      "step 15660: train loss 2.4454, val loss 2.3961\n",
      "step 15670: train loss 2.3646, val loss 2.4043\n",
      "step 15680: train loss 2.3375, val loss 2.4416\n",
      "step 15690: train loss 2.4186, val loss 2.3898\n",
      "step 15700: train loss 2.4316, val loss 2.3717\n",
      "step 15710: train loss 2.4270, val loss 2.3708\n",
      "step 15720: train loss 2.3832, val loss 2.4252\n",
      "step 15730: train loss 2.3781, val loss 2.4657\n",
      "step 15740: train loss 2.4098, val loss 2.3836\n",
      "step 15750: train loss 2.4656, val loss 2.4002\n",
      "step 15760: train loss 2.4598, val loss 2.4561\n",
      "step 15770: train loss 2.3656, val loss 2.3365\n",
      "step 15780: train loss 2.4943, val loss 2.3882\n",
      "step 15790: train loss 2.4579, val loss 2.4652\n",
      "step 15800: train loss 2.4817, val loss 2.2896\n",
      "step 15810: train loss 2.4721, val loss 2.4891\n",
      "step 15820: train loss 2.3764, val loss 2.3683\n",
      "step 15830: train loss 2.4161, val loss 2.4296\n",
      "step 15840: train loss 2.4576, val loss 2.3524\n",
      "step 15850: train loss 2.4683, val loss 2.4295\n",
      "step 15860: train loss 2.4234, val loss 2.3901\n",
      "step 15870: train loss 2.4222, val loss 2.4411\n",
      "step 15880: train loss 2.3940, val loss 2.4683\n",
      "step 15890: train loss 2.3598, val loss 2.4692\n",
      "step 15900: train loss 2.3731, val loss 2.4511\n",
      "step 15910: train loss 2.4326, val loss 2.4705\n",
      "step 15920: train loss 2.5237, val loss 2.3904\n",
      "step 15930: train loss 2.4253, val loss 2.4266\n",
      "step 15940: train loss 2.4191, val loss 2.4060\n",
      "step 15950: train loss 2.4185, val loss 2.4037\n",
      "step 15960: train loss 2.4224, val loss 2.4016\n",
      "step 15970: train loss 2.3817, val loss 2.3836\n",
      "step 15980: train loss 2.4487, val loss 2.3534\n",
      "step 15990: train loss 2.4310, val loss 2.4093\n",
      "step 16000: train loss 2.4147, val loss 2.4469\n",
      "step 16010: train loss 2.4111, val loss 2.4203\n",
      "step 16020: train loss 2.4580, val loss 2.4974\n",
      "step 16030: train loss 2.3608, val loss 2.4632\n",
      "step 16040: train loss 2.4014, val loss 2.4054\n",
      "step 16050: train loss 2.4539, val loss 2.3877\n",
      "step 16060: train loss 2.3412, val loss 2.4733\n",
      "step 16070: train loss 2.5100, val loss 2.3647\n",
      "step 16080: train loss 2.4332, val loss 2.3585\n",
      "step 16090: train loss 2.4510, val loss 2.4644\n",
      "step 16100: train loss 2.4106, val loss 2.4180\n",
      "step 16110: train loss 2.4010, val loss 2.3825\n",
      "step 16120: train loss 2.4639, val loss 2.4074\n",
      "step 16130: train loss 2.4732, val loss 2.4158\n",
      "step 16140: train loss 2.4420, val loss 2.3969\n",
      "step 16150: train loss 2.4335, val loss 2.3653\n",
      "step 16160: train loss 2.3730, val loss 2.4106\n",
      "step 16170: train loss 2.3844, val loss 2.3518\n",
      "step 16180: train loss 2.4212, val loss 2.3964\n",
      "step 16190: train loss 2.4278, val loss 2.4532\n",
      "step 16200: train loss 2.4110, val loss 2.4544\n",
      "step 16210: train loss 2.4092, val loss 2.3745\n",
      "step 16220: train loss 2.4720, val loss 2.4497\n",
      "step 16230: train loss 2.3795, val loss 2.4023\n",
      "step 16240: train loss 2.3979, val loss 2.4062\n",
      "step 16250: train loss 2.3816, val loss 2.4091\n",
      "step 16260: train loss 2.4500, val loss 2.3581\n",
      "step 16270: train loss 2.3988, val loss 2.3510\n",
      "step 16280: train loss 2.4416, val loss 2.4138\n",
      "step 16290: train loss 2.4344, val loss 2.4236\n",
      "step 16300: train loss 2.4086, val loss 2.3852\n",
      "step 16310: train loss 2.4262, val loss 2.4222\n",
      "step 16320: train loss 2.3770, val loss 2.4082\n",
      "step 16330: train loss 2.4698, val loss 2.3952\n",
      "step 16340: train loss 2.3211, val loss 2.3675\n",
      "step 16350: train loss 2.3866, val loss 2.4220\n",
      "step 16360: train loss 2.4138, val loss 2.4273\n",
      "step 16370: train loss 2.3992, val loss 2.4729\n",
      "step 16380: train loss 2.5174, val loss 2.3677\n",
      "step 16390: train loss 2.3534, val loss 2.3665\n",
      "step 16400: train loss 2.4543, val loss 2.3264\n",
      "step 16410: train loss 2.4229, val loss 2.4176\n",
      "step 16420: train loss 2.3918, val loss 2.3530\n",
      "step 16430: train loss 2.4147, val loss 2.4283\n",
      "step 16440: train loss 2.4632, val loss 2.3639\n",
      "step 16450: train loss 2.3188, val loss 2.4286\n",
      "step 16460: train loss 2.3925, val loss 2.4300\n",
      "step 16470: train loss 2.3572, val loss 2.4051\n",
      "step 16480: train loss 2.4903, val loss 2.4295\n",
      "step 16490: train loss 2.3823, val loss 2.3938\n",
      "step 16500: train loss 2.3963, val loss 2.4080\n",
      "step 16510: train loss 2.3926, val loss 2.3512\n",
      "step 16520: train loss 2.4076, val loss 2.3335\n",
      "step 16530: train loss 2.4136, val loss 2.3383\n",
      "step 16540: train loss 2.4102, val loss 2.4076\n",
      "step 16550: train loss 2.3460, val loss 2.4132\n",
      "step 16560: train loss 2.3689, val loss 2.3520\n",
      "step 16570: train loss 2.4551, val loss 2.4710\n",
      "step 16580: train loss 2.3401, val loss 2.4211\n",
      "step 16590: train loss 2.4482, val loss 2.4031\n",
      "step 16600: train loss 2.3970, val loss 2.3485\n",
      "step 16610: train loss 2.4135, val loss 2.4007\n",
      "step 16620: train loss 2.4322, val loss 2.4273\n",
      "step 16630: train loss 2.3912, val loss 2.3502\n",
      "step 16640: train loss 2.3981, val loss 2.3862\n",
      "step 16650: train loss 2.4096, val loss 2.4593\n",
      "step 16660: train loss 2.3279, val loss 2.4350\n",
      "step 16670: train loss 2.3238, val loss 2.3543\n",
      "step 16680: train loss 2.3699, val loss 2.3898\n",
      "step 16690: train loss 2.4729, val loss 2.3457\n",
      "step 16700: train loss 2.4639, val loss 2.4106\n",
      "step 16710: train loss 2.4547, val loss 2.3572\n",
      "step 16720: train loss 2.3811, val loss 2.4859\n",
      "step 16730: train loss 2.4000, val loss 2.3372\n",
      "step 16740: train loss 2.4443, val loss 2.4953\n",
      "step 16750: train loss 2.4207, val loss 2.3633\n",
      "step 16760: train loss 2.4142, val loss 2.3680\n",
      "step 16770: train loss 2.5123, val loss 2.4257\n",
      "step 16780: train loss 2.4097, val loss 2.5264\n",
      "step 16790: train loss 2.4249, val loss 2.3967\n",
      "step 16800: train loss 2.4710, val loss 2.3705\n",
      "step 16810: train loss 2.4455, val loss 2.4205\n",
      "step 16820: train loss 2.3976, val loss 2.5098\n",
      "step 16830: train loss 2.4037, val loss 2.4149\n",
      "step 16840: train loss 2.4350, val loss 2.4196\n",
      "step 16850: train loss 2.3674, val loss 2.4705\n",
      "step 16860: train loss 2.3547, val loss 2.4412\n",
      "step 16870: train loss 2.3697, val loss 2.4448\n",
      "step 16880: train loss 2.4026, val loss 2.3784\n",
      "step 16890: train loss 2.3684, val loss 2.3929\n",
      "step 16900: train loss 2.4629, val loss 2.3863\n",
      "step 16910: train loss 2.3805, val loss 2.4112\n",
      "step 16920: train loss 2.3962, val loss 2.4078\n",
      "step 16930: train loss 2.4331, val loss 2.4253\n",
      "step 16940: train loss 2.4370, val loss 2.4743\n",
      "step 16950: train loss 2.3133, val loss 2.4076\n",
      "step 16960: train loss 2.4041, val loss 2.3374\n",
      "step 16970: train loss 2.3897, val loss 2.2954\n",
      "step 16980: train loss 2.4981, val loss 2.4480\n",
      "step 16990: train loss 2.3869, val loss 2.4510\n",
      "step 17000: train loss 2.3765, val loss 2.4351\n",
      "step 17010: train loss 2.3671, val loss 2.3521\n",
      "step 17020: train loss 2.3497, val loss 2.4707\n",
      "step 17030: train loss 2.4524, val loss 2.4320\n",
      "step 17040: train loss 2.4197, val loss 2.4889\n",
      "step 17050: train loss 2.3825, val loss 2.4792\n",
      "step 17060: train loss 2.4718, val loss 2.2883\n",
      "step 17070: train loss 2.4067, val loss 2.4485\n",
      "step 17080: train loss 2.3697, val loss 2.4007\n",
      "step 17090: train loss 2.3312, val loss 2.3920\n",
      "step 17100: train loss 2.4591, val loss 2.3439\n",
      "step 17110: train loss 2.3782, val loss 2.3531\n",
      "step 17120: train loss 2.3899, val loss 2.4315\n",
      "step 17130: train loss 2.3511, val loss 2.4126\n",
      "step 17140: train loss 2.3923, val loss 2.3924\n",
      "step 17150: train loss 2.3753, val loss 2.4050\n",
      "step 17160: train loss 2.4107, val loss 2.4148\n",
      "step 17170: train loss 2.4615, val loss 2.3935\n",
      "step 17180: train loss 2.3699, val loss 2.4788\n",
      "step 17190: train loss 2.3590, val loss 2.4144\n",
      "step 17200: train loss 2.4042, val loss 2.4175\n",
      "step 17210: train loss 2.4689, val loss 2.3916\n",
      "step 17220: train loss 2.3017, val loss 2.4065\n",
      "step 17230: train loss 2.4642, val loss 2.3682\n",
      "step 17240: train loss 2.4057, val loss 2.3521\n",
      "step 17250: train loss 2.3978, val loss 2.4976\n",
      "step 17260: train loss 2.3702, val loss 2.3611\n",
      "step 17270: train loss 2.4851, val loss 2.4192\n",
      "step 17280: train loss 2.4023, val loss 2.3951\n",
      "step 17290: train loss 2.3529, val loss 2.4198\n",
      "step 17300: train loss 2.4231, val loss 2.3788\n",
      "step 17310: train loss 2.4447, val loss 2.4121\n",
      "step 17320: train loss 2.3662, val loss 2.4197\n",
      "step 17330: train loss 2.3702, val loss 2.3374\n",
      "step 17340: train loss 2.4442, val loss 2.3438\n",
      "step 17350: train loss 2.3720, val loss 2.4017\n",
      "step 17360: train loss 2.3889, val loss 2.4224\n",
      "step 17370: train loss 2.3750, val loss 2.3827\n",
      "step 17380: train loss 2.3657, val loss 2.2774\n",
      "step 17390: train loss 2.3970, val loss 2.4387\n",
      "step 17400: train loss 2.4755, val loss 2.3879\n",
      "step 17410: train loss 2.3905, val loss 2.4513\n",
      "step 17420: train loss 2.4475, val loss 2.4262\n",
      "step 17430: train loss 2.4620, val loss 2.4066\n",
      "step 17440: train loss 2.3960, val loss 2.4200\n",
      "step 17450: train loss 2.4520, val loss 2.4751\n",
      "step 17460: train loss 2.3542, val loss 2.4284\n",
      "step 17470: train loss 2.3782, val loss 2.4684\n",
      "step 17480: train loss 2.3973, val loss 2.3925\n",
      "step 17490: train loss 2.3852, val loss 2.4366\n",
      "step 17500: train loss 2.4164, val loss 2.3542\n",
      "step 17510: train loss 2.4064, val loss 2.3941\n",
      "step 17520: train loss 2.4000, val loss 2.3830\n",
      "step 17530: train loss 2.3669, val loss 2.3341\n",
      "step 17540: train loss 2.3884, val loss 2.4203\n",
      "step 17550: train loss 2.4200, val loss 2.4205\n",
      "step 17560: train loss 2.3868, val loss 2.3572\n",
      "step 17570: train loss 2.3695, val loss 2.4173\n",
      "step 17580: train loss 2.4715, val loss 2.4332\n",
      "step 17590: train loss 2.5136, val loss 2.4074\n",
      "step 17600: train loss 2.3752, val loss 2.3995\n",
      "step 17610: train loss 2.3835, val loss 2.3961\n",
      "step 17620: train loss 2.4117, val loss 2.4541\n",
      "step 17630: train loss 2.4147, val loss 2.4143\n",
      "step 17640: train loss 2.3672, val loss 2.3486\n",
      "step 17650: train loss 2.4195, val loss 2.4029\n",
      "step 17660: train loss 2.3905, val loss 2.4874\n",
      "step 17670: train loss 2.4352, val loss 2.3886\n",
      "step 17680: train loss 2.3614, val loss 2.3663\n",
      "step 17690: train loss 2.3949, val loss 2.4330\n",
      "step 17700: train loss 2.4348, val loss 2.3712\n",
      "step 17710: train loss 2.4100, val loss 2.2963\n",
      "step 17720: train loss 2.3681, val loss 2.4669\n",
      "step 17730: train loss 2.4049, val loss 2.4393\n",
      "step 17740: train loss 2.3700, val loss 2.3977\n",
      "step 17750: train loss 2.3927, val loss 2.3915\n",
      "step 17760: train loss 2.3207, val loss 2.3677\n",
      "step 17770: train loss 2.4515, val loss 2.4161\n",
      "step 17780: train loss 2.3116, val loss 2.4336\n",
      "step 17790: train loss 2.4302, val loss 2.3776\n",
      "step 17800: train loss 2.3675, val loss 2.3899\n",
      "step 17810: train loss 2.4675, val loss 2.4639\n",
      "step 17820: train loss 2.4509, val loss 2.4172\n",
      "step 17830: train loss 2.3671, val loss 2.3765\n",
      "step 17840: train loss 2.3604, val loss 2.3543\n",
      "step 17850: train loss 2.4381, val loss 2.4072\n",
      "step 17860: train loss 2.4698, val loss 2.4371\n",
      "step 17870: train loss 2.3713, val loss 2.3705\n",
      "step 17880: train loss 2.4189, val loss 2.4092\n",
      "step 17890: train loss 2.4089, val loss 2.3362\n",
      "step 17900: train loss 2.4552, val loss 2.4441\n",
      "step 17910: train loss 2.3976, val loss 2.3995\n",
      "step 17920: train loss 2.4383, val loss 2.4883\n",
      "step 17930: train loss 2.4768, val loss 2.3865\n",
      "step 17940: train loss 2.4023, val loss 2.3594\n",
      "step 17950: train loss 2.3926, val loss 2.4147\n",
      "step 17960: train loss 2.4049, val loss 2.3510\n",
      "step 17970: train loss 2.4298, val loss 2.4286\n",
      "step 17980: train loss 2.3352, val loss 2.3472\n",
      "step 17990: train loss 2.4174, val loss 2.4744\n",
      "step 18000: train loss 2.4184, val loss 2.4786\n",
      "step 18010: train loss 2.4342, val loss 2.4486\n",
      "step 18020: train loss 2.3844, val loss 2.4010\n",
      "step 18030: train loss 2.4533, val loss 2.4184\n",
      "step 18040: train loss 2.3987, val loss 2.4122\n",
      "step 18050: train loss 2.4155, val loss 2.4464\n",
      "step 18060: train loss 2.3564, val loss 2.4287\n",
      "step 18070: train loss 2.4695, val loss 2.3614\n",
      "step 18080: train loss 2.3787, val loss 2.4291\n",
      "step 18090: train loss 2.3624, val loss 2.4550\n",
      "step 18100: train loss 2.4557, val loss 2.3729\n",
      "step 18110: train loss 2.4028, val loss 2.3560\n",
      "step 18120: train loss 2.4568, val loss 2.4690\n",
      "step 18130: train loss 2.4044, val loss 2.4365\n",
      "step 18140: train loss 2.4094, val loss 2.3737\n",
      "step 18150: train loss 2.4145, val loss 2.4000\n",
      "step 18160: train loss 2.4510, val loss 2.4149\n",
      "step 18170: train loss 2.3435, val loss 2.4079\n",
      "step 18180: train loss 2.3762, val loss 2.4388\n",
      "step 18190: train loss 2.3004, val loss 2.4247\n",
      "step 18200: train loss 2.3877, val loss 2.3608\n",
      "step 18210: train loss 2.4241, val loss 2.4349\n",
      "step 18220: train loss 2.3705, val loss 2.4078\n",
      "step 18230: train loss 2.3809, val loss 2.5003\n",
      "step 18240: train loss 2.3552, val loss 2.5195\n",
      "step 18250: train loss 2.4261, val loss 2.3717\n",
      "step 18260: train loss 2.3718, val loss 2.4006\n",
      "step 18270: train loss 2.4190, val loss 2.4810\n",
      "step 18280: train loss 2.3989, val loss 2.4593\n",
      "step 18290: train loss 2.4013, val loss 2.3479\n",
      "step 18300: train loss 2.4076, val loss 2.3606\n",
      "step 18310: train loss 2.3270, val loss 2.3871\n",
      "step 18320: train loss 2.4663, val loss 2.4233\n",
      "step 18330: train loss 2.4503, val loss 2.3516\n",
      "step 18340: train loss 2.3043, val loss 2.3917\n",
      "step 18350: train loss 2.4464, val loss 2.5455\n",
      "step 18360: train loss 2.4409, val loss 2.3863\n",
      "step 18370: train loss 2.4041, val loss 2.4276\n",
      "step 18380: train loss 2.4374, val loss 2.3675\n",
      "step 18390: train loss 2.3776, val loss 2.3877\n",
      "step 18400: train loss 2.4061, val loss 2.3648\n",
      "step 18410: train loss 2.3729, val loss 2.4023\n",
      "step 18420: train loss 2.3696, val loss 2.4054\n",
      "step 18430: train loss 2.4097, val loss 2.4387\n",
      "step 18440: train loss 2.4311, val loss 2.3913\n",
      "step 18450: train loss 2.3578, val loss 2.4191\n",
      "step 18460: train loss 2.4091, val loss 2.3881\n",
      "step 18470: train loss 2.3974, val loss 2.4027\n",
      "step 18480: train loss 2.3541, val loss 2.3576\n",
      "step 18490: train loss 2.4813, val loss 2.4105\n",
      "step 18500: train loss 2.3498, val loss 2.4177\n",
      "step 18510: train loss 2.3649, val loss 2.3330\n",
      "step 18520: train loss 2.3970, val loss 2.3751\n",
      "step 18530: train loss 2.3632, val loss 2.4027\n",
      "step 18540: train loss 2.4067, val loss 2.4431\n",
      "step 18550: train loss 2.3527, val loss 2.3993\n",
      "step 18560: train loss 2.4517, val loss 2.4825\n",
      "step 18570: train loss 2.4953, val loss 2.4430\n",
      "step 18580: train loss 2.3101, val loss 2.3472\n",
      "step 18590: train loss 2.4218, val loss 2.3613\n",
      "step 18600: train loss 2.4658, val loss 2.3791\n",
      "step 18610: train loss 2.3941, val loss 2.3673\n",
      "step 18620: train loss 2.4067, val loss 2.3939\n",
      "step 18630: train loss 2.3280, val loss 2.3999\n",
      "step 18640: train loss 2.2893, val loss 2.4787\n",
      "step 18650: train loss 2.4758, val loss 2.3563\n",
      "step 18660: train loss 2.4269, val loss 2.4793\n",
      "step 18670: train loss 2.3720, val loss 2.4451\n",
      "step 18680: train loss 2.4118, val loss 2.3919\n",
      "step 18690: train loss 2.3884, val loss 2.4436\n",
      "step 18700: train loss 2.3936, val loss 2.4388\n",
      "step 18710: train loss 2.3790, val loss 2.3667\n",
      "step 18720: train loss 2.3930, val loss 2.3992\n",
      "step 18730: train loss 2.3309, val loss 2.4048\n",
      "step 18740: train loss 2.4400, val loss 2.4296\n",
      "step 18750: train loss 2.3721, val loss 2.3448\n",
      "step 18760: train loss 2.4176, val loss 2.4623\n",
      "step 18770: train loss 2.4758, val loss 2.4217\n",
      "step 18780: train loss 2.3988, val loss 2.4275\n",
      "step 18790: train loss 2.3834, val loss 2.4009\n",
      "step 18800: train loss 2.5443, val loss 2.4246\n",
      "step 18810: train loss 2.4037, val loss 2.3617\n",
      "step 18820: train loss 2.4004, val loss 2.4445\n",
      "step 18830: train loss 2.3826, val loss 2.3602\n",
      "step 18840: train loss 2.3950, val loss 2.5291\n",
      "step 18850: train loss 2.3429, val loss 2.3697\n",
      "step 18860: train loss 2.4795, val loss 2.3843\n",
      "step 18870: train loss 2.3845, val loss 2.2913\n",
      "step 18880: train loss 2.4508, val loss 2.4408\n",
      "step 18890: train loss 2.4231, val loss 2.3926\n",
      "step 18900: train loss 2.3667, val loss 2.3990\n",
      "step 18910: train loss 2.4320, val loss 2.3745\n",
      "step 18920: train loss 2.3653, val loss 2.4058\n",
      "step 18930: train loss 2.4269, val loss 2.3603\n",
      "step 18940: train loss 2.4275, val loss 2.3723\n",
      "step 18950: train loss 2.3865, val loss 2.3882\n",
      "step 18960: train loss 2.4031, val loss 2.4031\n",
      "step 18970: train loss 2.4664, val loss 2.3441\n",
      "step 18980: train loss 2.5222, val loss 2.4739\n",
      "step 18990: train loss 2.4266, val loss 2.3687\n",
      "step 19000: train loss 2.3514, val loss 2.2981\n",
      "step 19010: train loss 2.4423, val loss 2.3320\n",
      "step 19020: train loss 2.3833, val loss 2.4280\n",
      "step 19030: train loss 2.4314, val loss 2.4378\n",
      "step 19040: train loss 2.4500, val loss 2.4553\n",
      "step 19050: train loss 2.3884, val loss 2.4464\n",
      "step 19060: train loss 2.4317, val loss 2.4272\n",
      "step 19070: train loss 2.3465, val loss 2.3741\n",
      "step 19080: train loss 2.4333, val loss 2.3556\n",
      "step 19090: train loss 2.4295, val loss 2.4781\n",
      "step 19100: train loss 2.4260, val loss 2.4121\n",
      "step 19110: train loss 2.3844, val loss 2.3488\n",
      "step 19120: train loss 2.3741, val loss 2.4023\n",
      "step 19130: train loss 2.4632, val loss 2.3450\n",
      "step 19140: train loss 2.4047, val loss 2.4518\n",
      "step 19150: train loss 2.3957, val loss 2.4305\n",
      "step 19160: train loss 2.3935, val loss 2.3525\n",
      "step 19170: train loss 2.3808, val loss 2.4049\n",
      "step 19180: train loss 2.4441, val loss 2.4509\n",
      "step 19190: train loss 2.4103, val loss 2.3655\n",
      "step 19200: train loss 2.3707, val loss 2.4072\n",
      "step 19210: train loss 2.3500, val loss 2.3725\n",
      "step 19220: train loss 2.3918, val loss 2.3759\n",
      "step 19230: train loss 2.4280, val loss 2.4261\n",
      "step 19240: train loss 2.4269, val loss 2.3064\n",
      "step 19250: train loss 2.4053, val loss 2.3208\n",
      "step 19260: train loss 2.4406, val loss 2.3427\n",
      "step 19270: train loss 2.4716, val loss 2.3347\n",
      "step 19280: train loss 2.3906, val loss 2.3636\n",
      "step 19290: train loss 2.4342, val loss 2.3930\n",
      "step 19300: train loss 2.4010, val loss 2.4541\n",
      "step 19310: train loss 2.3319, val loss 2.4089\n",
      "step 19320: train loss 2.3702, val loss 2.4417\n",
      "step 19330: train loss 2.4401, val loss 2.3309\n",
      "step 19340: train loss 2.3543, val loss 2.4068\n",
      "step 19350: train loss 2.4641, val loss 2.4562\n",
      "step 19360: train loss 2.2987, val loss 2.4202\n",
      "step 19370: train loss 2.3987, val loss 2.4385\n",
      "step 19380: train loss 2.3797, val loss 2.3478\n",
      "step 19390: train loss 2.3681, val loss 2.3153\n",
      "step 19400: train loss 2.3661, val loss 2.4029\n",
      "step 19410: train loss 2.4745, val loss 2.4128\n",
      "step 19420: train loss 2.3529, val loss 2.3662\n",
      "step 19430: train loss 2.3519, val loss 2.4299\n",
      "step 19440: train loss 2.4026, val loss 2.4101\n",
      "step 19450: train loss 2.4390, val loss 2.3631\n",
      "step 19460: train loss 2.4697, val loss 2.4402\n",
      "step 19470: train loss 2.3418, val loss 2.4543\n",
      "step 19480: train loss 2.3550, val loss 2.3811\n",
      "step 19490: train loss 2.3328, val loss 2.4463\n",
      "step 19500: train loss 2.3571, val loss 2.4277\n",
      "step 19510: train loss 2.4425, val loss 2.3270\n",
      "step 19520: train loss 2.3801, val loss 2.3961\n",
      "step 19530: train loss 2.5049, val loss 2.4286\n",
      "step 19540: train loss 2.4970, val loss 2.4237\n",
      "step 19550: train loss 2.3987, val loss 2.3934\n",
      "step 19560: train loss 2.4202, val loss 2.3869\n",
      "step 19570: train loss 2.4432, val loss 2.4256\n",
      "step 19580: train loss 2.3304, val loss 2.3295\n",
      "step 19590: train loss 2.3823, val loss 2.3530\n",
      "step 19600: train loss 2.3343, val loss 2.4070\n",
      "step 19610: train loss 2.3760, val loss 2.3574\n",
      "step 19620: train loss 2.4213, val loss 2.3552\n",
      "step 19630: train loss 2.4214, val loss 2.3672\n",
      "step 19640: train loss 2.4547, val loss 2.3760\n",
      "step 19650: train loss 2.4503, val loss 2.4190\n",
      "step 19660: train loss 2.4315, val loss 2.3617\n",
      "step 19670: train loss 2.4699, val loss 2.3561\n",
      "step 19680: train loss 2.4429, val loss 2.3760\n",
      "step 19690: train loss 2.4726, val loss 2.3839\n",
      "step 19700: train loss 2.4617, val loss 2.4510\n",
      "step 19710: train loss 2.4084, val loss 2.3294\n",
      "step 19720: train loss 2.3683, val loss 2.4161\n",
      "step 19730: train loss 2.4599, val loss 2.4671\n",
      "step 19740: train loss 2.3999, val loss 2.5272\n",
      "step 19750: train loss 2.3333, val loss 2.3975\n",
      "step 19760: train loss 2.4302, val loss 2.3874\n",
      "step 19770: train loss 2.4061, val loss 2.4264\n",
      "step 19780: train loss 2.4168, val loss 2.3713\n",
      "step 19790: train loss 2.3724, val loss 2.4157\n",
      "step 19800: train loss 2.4061, val loss 2.4466\n",
      "step 19810: train loss 2.3508, val loss 2.3856\n",
      "step 19820: train loss 2.3890, val loss 2.3843\n",
      "step 19830: train loss 2.3301, val loss 2.3384\n",
      "step 19840: train loss 2.3904, val loss 2.3882\n",
      "step 19850: train loss 2.4175, val loss 2.4423\n",
      "step 19860: train loss 2.3461, val loss 2.3852\n",
      "step 19870: train loss 2.4495, val loss 2.3767\n",
      "step 19880: train loss 2.3732, val loss 2.3899\n",
      "step 19890: train loss 2.4858, val loss 2.3576\n",
      "step 19900: train loss 2.3668, val loss 2.3742\n",
      "step 19910: train loss 2.4425, val loss 2.3651\n",
      "step 19920: train loss 2.3884, val loss 2.4671\n",
      "step 19930: train loss 2.4251, val loss 2.3785\n",
      "step 19940: train loss 2.4337, val loss 2.4039\n",
      "step 19950: train loss 2.4070, val loss 2.4085\n",
      "step 19960: train loss 2.3925, val loss 2.4638\n",
      "step 19970: train loss 2.3417, val loss 2.3802\n",
      "step 19980: train loss 2.3566, val loss 2.3596\n",
      "step 19990: train loss 2.3991, val loss 2.3845\n",
      "step 20000: train loss 2.4176, val loss 2.3535\n",
      "step 20010: train loss 2.4090, val loss 2.3986\n",
      "step 20020: train loss 2.3402, val loss 2.4379\n",
      "step 20030: train loss 2.3420, val loss 2.3110\n",
      "step 20040: train loss 2.4205, val loss 2.4181\n",
      "step 20050: train loss 2.3050, val loss 2.4620\n",
      "step 20060: train loss 2.4576, val loss 2.4043\n",
      "step 20070: train loss 2.4098, val loss 2.3899\n",
      "step 20080: train loss 2.4094, val loss 2.3778\n",
      "step 20090: train loss 2.4078, val loss 2.4538\n",
      "step 20100: train loss 2.4064, val loss 2.4011\n",
      "step 20110: train loss 2.3941, val loss 2.4392\n",
      "step 20120: train loss 2.3561, val loss 2.4233\n",
      "step 20130: train loss 2.3676, val loss 2.3930\n",
      "step 20140: train loss 2.4341, val loss 2.3995\n",
      "step 20150: train loss 2.4524, val loss 2.4089\n",
      "step 20160: train loss 2.3379, val loss 2.4058\n",
      "step 20170: train loss 2.3709, val loss 2.3920\n",
      "step 20180: train loss 2.3872, val loss 2.3777\n",
      "step 20190: train loss 2.4238, val loss 2.2972\n",
      "step 20200: train loss 2.4175, val loss 2.3512\n",
      "step 20210: train loss 2.4901, val loss 2.3585\n",
      "step 20220: train loss 2.4340, val loss 2.3512\n",
      "step 20230: train loss 2.3846, val loss 2.3888\n",
      "step 20240: train loss 2.3630, val loss 2.4281\n",
      "step 20250: train loss 2.4644, val loss 2.3601\n",
      "step 20260: train loss 2.4248, val loss 2.3490\n",
      "step 20270: train loss 2.3520, val loss 2.3600\n",
      "step 20280: train loss 2.3832, val loss 2.3577\n",
      "step 20290: train loss 2.3398, val loss 2.3795\n",
      "step 20300: train loss 2.3536, val loss 2.3639\n",
      "step 20310: train loss 2.3787, val loss 2.4672\n",
      "step 20320: train loss 2.3599, val loss 2.3675\n",
      "step 20330: train loss 2.3358, val loss 2.3718\n",
      "step 20340: train loss 2.4448, val loss 2.3189\n",
      "step 20350: train loss 2.3975, val loss 2.3011\n",
      "step 20360: train loss 2.4084, val loss 2.4480\n",
      "step 20370: train loss 2.4072, val loss 2.4683\n",
      "step 20380: train loss 2.4530, val loss 2.4197\n",
      "step 20390: train loss 2.4050, val loss 2.3525\n",
      "step 20400: train loss 2.3764, val loss 2.4507\n",
      "step 20410: train loss 2.3887, val loss 2.3844\n",
      "step 20420: train loss 2.4209, val loss 2.3940\n",
      "step 20430: train loss 2.4473, val loss 2.3506\n",
      "step 20440: train loss 2.3864, val loss 2.3647\n",
      "step 20450: train loss 2.3455, val loss 2.3461\n",
      "step 20460: train loss 2.2995, val loss 2.3431\n",
      "step 20470: train loss 2.4110, val loss 2.4160\n",
      "step 20480: train loss 2.3362, val loss 2.3795\n",
      "step 20490: train loss 2.4132, val loss 2.3598\n",
      "step 20500: train loss 2.4170, val loss 2.3838\n",
      "step 20510: train loss 2.3928, val loss 2.4122\n",
      "step 20520: train loss 2.4477, val loss 2.4363\n",
      "step 20530: train loss 2.3507, val loss 2.4806\n",
      "step 20540: train loss 2.3613, val loss 2.3700\n",
      "step 20550: train loss 2.4543, val loss 2.3155\n",
      "step 20560: train loss 2.3713, val loss 2.3765\n",
      "step 20570: train loss 2.3570, val loss 2.4078\n",
      "step 20580: train loss 2.3796, val loss 2.3624\n",
      "step 20590: train loss 2.3757, val loss 2.3717\n",
      "step 20600: train loss 2.4005, val loss 2.3422\n",
      "step 20610: train loss 2.3090, val loss 2.4332\n",
      "step 20620: train loss 2.4501, val loss 2.3742\n",
      "step 20630: train loss 2.4242, val loss 2.3521\n",
      "step 20640: train loss 2.4439, val loss 2.3639\n",
      "step 20650: train loss 2.3837, val loss 2.3730\n",
      "step 20660: train loss 2.4539, val loss 2.3340\n",
      "step 20670: train loss 2.4067, val loss 2.4039\n",
      "step 20680: train loss 2.4288, val loss 2.4340\n",
      "step 20690: train loss 2.4098, val loss 2.4124\n",
      "step 20700: train loss 2.4379, val loss 2.3735\n",
      "step 20710: train loss 2.3493, val loss 2.4222\n",
      "step 20720: train loss 2.3766, val loss 2.3990\n",
      "step 20730: train loss 2.3681, val loss 2.4573\n",
      "step 20740: train loss 2.3724, val loss 2.3279\n",
      "step 20750: train loss 2.4095, val loss 2.3899\n",
      "step 20760: train loss 2.3807, val loss 2.3587\n",
      "step 20770: train loss 2.3503, val loss 2.3714\n",
      "step 20780: train loss 2.3869, val loss 2.3858\n",
      "step 20790: train loss 2.4660, val loss 2.4008\n",
      "step 20800: train loss 2.3383, val loss 2.3977\n",
      "step 20810: train loss 2.3158, val loss 2.3618\n",
      "step 20820: train loss 2.3673, val loss 2.4301\n",
      "step 20830: train loss 2.3965, val loss 2.3550\n",
      "step 20840: train loss 2.3597, val loss 2.5011\n",
      "step 20850: train loss 2.3714, val loss 2.4102\n",
      "step 20860: train loss 2.3927, val loss 2.4470\n",
      "step 20870: train loss 2.3802, val loss 2.4152\n",
      "step 20880: train loss 2.3202, val loss 2.4372\n",
      "step 20890: train loss 2.3678, val loss 2.3696\n",
      "step 20900: train loss 2.3898, val loss 2.3969\n",
      "step 20910: train loss 2.3374, val loss 2.3221\n",
      "step 20920: train loss 2.4197, val loss 2.3715\n",
      "step 20930: train loss 2.4007, val loss 2.4090\n",
      "step 20940: train loss 2.4317, val loss 2.3583\n",
      "step 20950: train loss 2.4809, val loss 2.3753\n",
      "step 20960: train loss 2.4596, val loss 2.3484\n",
      "step 20970: train loss 2.4580, val loss 2.3994\n",
      "step 20980: train loss 2.3499, val loss 2.4201\n",
      "step 20990: train loss 2.3808, val loss 2.4140\n",
      "step 21000: train loss 2.4104, val loss 2.3169\n",
      "step 21010: train loss 2.4292, val loss 2.3317\n",
      "step 21020: train loss 2.3883, val loss 2.3504\n",
      "step 21030: train loss 2.3584, val loss 2.3791\n",
      "step 21040: train loss 2.4348, val loss 2.4158\n",
      "step 21050: train loss 2.3802, val loss 2.3394\n",
      "step 21060: train loss 2.3894, val loss 2.3989\n",
      "step 21070: train loss 2.4262, val loss 2.3768\n",
      "step 21080: train loss 2.4281, val loss 2.3370\n",
      "step 21090: train loss 2.3161, val loss 2.3380\n",
      "step 21100: train loss 2.4147, val loss 2.4036\n",
      "step 21110: train loss 2.3547, val loss 2.3644\n",
      "step 21120: train loss 2.4286, val loss 2.3603\n",
      "step 21130: train loss 2.3870, val loss 2.3783\n",
      "step 21140: train loss 2.5051, val loss 2.3696\n",
      "step 21150: train loss 2.3484, val loss 2.3857\n",
      "step 21160: train loss 2.4209, val loss 2.3859\n",
      "step 21170: train loss 2.4296, val loss 2.4422\n",
      "step 21180: train loss 2.3765, val loss 2.4725\n",
      "step 21190: train loss 2.4163, val loss 2.4110\n",
      "step 21200: train loss 2.3968, val loss 2.3786\n",
      "step 21210: train loss 2.4156, val loss 2.3494\n",
      "step 21220: train loss 2.5247, val loss 2.3590\n",
      "step 21230: train loss 2.4528, val loss 2.3758\n",
      "step 21240: train loss 2.3792, val loss 2.3827\n",
      "step 21250: train loss 2.3921, val loss 2.4079\n",
      "step 21260: train loss 2.3271, val loss 2.3790\n",
      "step 21270: train loss 2.4320, val loss 2.4790\n",
      "step 21280: train loss 2.4259, val loss 2.4051\n",
      "step 21290: train loss 2.4228, val loss 2.3744\n",
      "step 21300: train loss 2.3887, val loss 2.3853\n",
      "step 21310: train loss 2.4080, val loss 2.3949\n",
      "step 21320: train loss 2.4545, val loss 2.3575\n",
      "step 21330: train loss 2.4656, val loss 2.3832\n",
      "step 21340: train loss 2.3919, val loss 2.4275\n",
      "step 21350: train loss 2.3781, val loss 2.3756\n",
      "step 21360: train loss 2.3997, val loss 2.3867\n",
      "step 21370: train loss 2.4342, val loss 2.4052\n",
      "step 21380: train loss 2.3629, val loss 2.3425\n",
      "step 21390: train loss 2.3863, val loss 2.4882\n",
      "step 21400: train loss 2.4616, val loss 2.3936\n",
      "step 21410: train loss 2.4474, val loss 2.3820\n",
      "step 21420: train loss 2.3812, val loss 2.3524\n",
      "step 21430: train loss 2.4293, val loss 2.3923\n",
      "step 21440: train loss 2.4216, val loss 2.4068\n",
      "step 21450: train loss 2.3498, val loss 2.4058\n",
      "step 21460: train loss 2.3403, val loss 2.4010\n",
      "step 21470: train loss 2.3379, val loss 2.4246\n",
      "step 21480: train loss 2.4136, val loss 2.4196\n",
      "step 21490: train loss 2.3350, val loss 2.4246\n",
      "step 21500: train loss 2.3983, val loss 2.3794\n",
      "step 21510: train loss 2.3751, val loss 2.4592\n",
      "step 21520: train loss 2.3462, val loss 2.3619\n",
      "step 21530: train loss 2.4067, val loss 2.4024\n",
      "step 21540: train loss 2.3838, val loss 2.4482\n",
      "step 21550: train loss 2.3937, val loss 2.3447\n",
      "step 21560: train loss 2.3543, val loss 2.3792\n",
      "step 21570: train loss 2.3139, val loss 2.3963\n",
      "step 21580: train loss 2.4867, val loss 2.3445\n",
      "step 21590: train loss 2.4618, val loss 2.4118\n",
      "step 21600: train loss 2.4625, val loss 2.3450\n",
      "step 21610: train loss 2.3509, val loss 2.3772\n",
      "step 21620: train loss 2.4343, val loss 2.3289\n",
      "step 21630: train loss 2.2916, val loss 2.3857\n",
      "step 21640: train loss 2.4322, val loss 2.3447\n",
      "step 21650: train loss 2.3713, val loss 2.3767\n",
      "step 21660: train loss 2.4335, val loss 2.3639\n",
      "step 21670: train loss 2.3511, val loss 2.4002\n",
      "step 21680: train loss 2.4259, val loss 2.3474\n",
      "step 21690: train loss 2.4295, val loss 2.3888\n",
      "step 21700: train loss 2.4021, val loss 2.3232\n",
      "step 21710: train loss 2.4295, val loss 2.3156\n",
      "step 21720: train loss 2.3693, val loss 2.4241\n",
      "step 21730: train loss 2.3052, val loss 2.4409\n",
      "step 21740: train loss 2.3506, val loss 2.4062\n",
      "step 21750: train loss 2.4647, val loss 2.4030\n",
      "step 21760: train loss 2.4038, val loss 2.3378\n",
      "step 21770: train loss 2.4255, val loss 2.4792\n",
      "step 21780: train loss 2.4447, val loss 2.4389\n",
      "step 21790: train loss 2.4103, val loss 2.3728\n",
      "step 21800: train loss 2.3758, val loss 2.3970\n",
      "step 21810: train loss 2.3848, val loss 2.4371\n",
      "step 21820: train loss 2.3271, val loss 2.4076\n",
      "step 21830: train loss 2.3396, val loss 2.3506\n",
      "step 21840: train loss 2.4448, val loss 2.3356\n",
      "step 21850: train loss 2.3419, val loss 2.3253\n",
      "step 21860: train loss 2.2923, val loss 2.3401\n",
      "step 21870: train loss 2.4801, val loss 2.3487\n",
      "step 21880: train loss 2.4141, val loss 2.3199\n",
      "step 21890: train loss 2.3922, val loss 2.3984\n",
      "step 21900: train loss 2.3615, val loss 2.4037\n",
      "step 21910: train loss 2.3370, val loss 2.3366\n",
      "step 21920: train loss 2.3949, val loss 2.3966\n",
      "step 21930: train loss 2.4273, val loss 2.3941\n",
      "step 21940: train loss 2.3141, val loss 2.3983\n",
      "step 21950: train loss 2.4278, val loss 2.3931\n",
      "step 21960: train loss 2.3387, val loss 2.4248\n",
      "step 21970: train loss 2.4013, val loss 2.3766\n",
      "step 21980: train loss 2.3654, val loss 2.3532\n",
      "step 21990: train loss 2.3697, val loss 2.4049\n",
      "step 22000: train loss 2.3631, val loss 2.4491\n",
      "step 22010: train loss 2.3954, val loss 2.4284\n",
      "step 22020: train loss 2.3734, val loss 2.4027\n",
      "step 22030: train loss 2.4149, val loss 2.4103\n",
      "step 22040: train loss 2.3563, val loss 2.4569\n",
      "step 22050: train loss 2.4085, val loss 2.3828\n",
      "step 22060: train loss 2.4000, val loss 2.5074\n",
      "step 22070: train loss 2.4305, val loss 2.3360\n",
      "step 22080: train loss 2.3901, val loss 2.3368\n",
      "step 22090: train loss 2.3867, val loss 2.4179\n",
      "step 22100: train loss 2.4583, val loss 2.4161\n",
      "step 22110: train loss 2.4645, val loss 2.4007\n",
      "step 22120: train loss 2.3908, val loss 2.3485\n",
      "step 22130: train loss 2.3628, val loss 2.3747\n",
      "step 22140: train loss 2.3489, val loss 2.4283\n",
      "step 22150: train loss 2.4546, val loss 2.3890\n",
      "step 22160: train loss 2.4100, val loss 2.4133\n",
      "step 22170: train loss 2.3788, val loss 2.4533\n",
      "step 22180: train loss 2.3462, val loss 2.3879\n",
      "step 22190: train loss 2.4130, val loss 2.3587\n",
      "step 22200: train loss 2.2954, val loss 2.4069\n",
      "step 22210: train loss 2.3570, val loss 2.3888\n",
      "step 22220: train loss 2.3712, val loss 2.4249\n",
      "step 22230: train loss 2.3973, val loss 2.3859\n",
      "step 22240: train loss 2.3887, val loss 2.3930\n",
      "step 22250: train loss 2.3864, val loss 2.3734\n",
      "step 22260: train loss 2.4019, val loss 2.3980\n",
      "step 22270: train loss 2.3703, val loss 2.3410\n",
      "step 22280: train loss 2.3303, val loss 2.4282\n",
      "step 22290: train loss 2.4502, val loss 2.5117\n",
      "step 22300: train loss 2.3957, val loss 2.3646\n",
      "step 22310: train loss 2.3404, val loss 2.4612\n",
      "step 22320: train loss 2.4582, val loss 2.4000\n",
      "step 22330: train loss 2.4173, val loss 2.3735\n",
      "step 22340: train loss 2.4576, val loss 2.3381\n",
      "step 22350: train loss 2.3558, val loss 2.3549\n",
      "step 22360: train loss 2.3942, val loss 2.4270\n",
      "step 22370: train loss 2.4418, val loss 2.3648\n",
      "step 22380: train loss 2.3831, val loss 2.4212\n",
      "step 22390: train loss 2.3725, val loss 2.3961\n",
      "step 22400: train loss 2.3903, val loss 2.4290\n",
      "step 22410: train loss 2.4303, val loss 2.3921\n",
      "step 22420: train loss 2.3620, val loss 2.3259\n",
      "step 22430: train loss 2.3676, val loss 2.3332\n",
      "step 22440: train loss 2.4091, val loss 2.4257\n",
      "step 22450: train loss 2.3418, val loss 2.4438\n",
      "step 22460: train loss 2.4374, val loss 2.3994\n",
      "step 22470: train loss 2.3876, val loss 2.3973\n",
      "step 22480: train loss 2.3706, val loss 2.3783\n",
      "step 22490: train loss 2.3584, val loss 2.3575\n",
      "step 22500: train loss 2.3696, val loss 2.3292\n",
      "step 22510: train loss 2.4122, val loss 2.4235\n",
      "step 22520: train loss 2.3170, val loss 2.3426\n",
      "step 22530: train loss 2.4119, val loss 2.4152\n",
      "step 22540: train loss 2.3285, val loss 2.3885\n",
      "step 22550: train loss 2.4407, val loss 2.3779\n",
      "step 22560: train loss 2.3995, val loss 2.3311\n",
      "step 22570: train loss 2.4215, val loss 2.3526\n",
      "step 22580: train loss 2.3598, val loss 2.3443\n",
      "step 22590: train loss 2.3586, val loss 2.3562\n",
      "step 22600: train loss 2.4764, val loss 2.3686\n",
      "step 22610: train loss 2.4124, val loss 2.3274\n",
      "step 22620: train loss 2.3671, val loss 2.3721\n",
      "step 22630: train loss 2.3844, val loss 2.4038\n",
      "step 22640: train loss 2.4425, val loss 2.3646\n",
      "step 22650: train loss 2.3985, val loss 2.4290\n",
      "step 22660: train loss 2.4088, val loss 2.3569\n",
      "step 22670: train loss 2.4614, val loss 2.3369\n",
      "step 22680: train loss 2.3654, val loss 2.4960\n",
      "step 22690: train loss 2.3866, val loss 2.3528\n",
      "step 22700: train loss 2.3634, val loss 2.3753\n",
      "step 22710: train loss 2.4070, val loss 2.3225\n",
      "step 22720: train loss 2.4548, val loss 2.4232\n",
      "step 22730: train loss 2.3850, val loss 2.4328\n",
      "step 22740: train loss 2.3039, val loss 2.2969\n",
      "step 22750: train loss 2.4076, val loss 2.3241\n",
      "step 22760: train loss 2.4261, val loss 2.4336\n",
      "step 22770: train loss 2.3366, val loss 2.4212\n",
      "step 22780: train loss 2.4006, val loss 2.4294\n",
      "step 22790: train loss 2.4238, val loss 2.3790\n",
      "step 22800: train loss 2.4084, val loss 2.4706\n",
      "step 22810: train loss 2.3918, val loss 2.3790\n",
      "step 22820: train loss 2.4748, val loss 2.3791\n",
      "step 22830: train loss 2.4019, val loss 2.3835\n",
      "step 22840: train loss 2.3543, val loss 2.3519\n",
      "step 22850: train loss 2.4167, val loss 2.4516\n",
      "step 22860: train loss 2.4067, val loss 2.3932\n",
      "step 22870: train loss 2.3410, val loss 2.4089\n",
      "step 22880: train loss 2.3559, val loss 2.3263\n",
      "step 22890: train loss 2.3787, val loss 2.3879\n",
      "step 22900: train loss 2.3918, val loss 2.3429\n",
      "step 22910: train loss 2.3535, val loss 2.3673\n",
      "step 22920: train loss 2.4199, val loss 2.3713\n",
      "step 22930: train loss 2.3944, val loss 2.4029\n",
      "step 22940: train loss 2.3712, val loss 2.4968\n",
      "step 22950: train loss 2.4186, val loss 2.4191\n",
      "step 22960: train loss 2.3811, val loss 2.3738\n",
      "step 22970: train loss 2.4442, val loss 2.4027\n",
      "step 22980: train loss 2.3457, val loss 2.3994\n",
      "step 22990: train loss 2.4536, val loss 2.3365\n",
      "step 23000: train loss 2.3688, val loss 2.4228\n",
      "step 23010: train loss 2.3796, val loss 2.3941\n",
      "step 23020: train loss 2.3815, val loss 2.3757\n",
      "step 23030: train loss 2.3751, val loss 2.4417\n",
      "step 23040: train loss 2.3597, val loss 2.4077\n",
      "step 23050: train loss 2.4373, val loss 2.5149\n",
      "step 23060: train loss 2.3831, val loss 2.3430\n",
      "step 23070: train loss 2.3785, val loss 2.3573\n",
      "step 23080: train loss 2.3695, val loss 2.3602\n",
      "step 23090: train loss 2.4559, val loss 2.3397\n",
      "step 23100: train loss 2.4269, val loss 2.3949\n",
      "step 23110: train loss 2.4387, val loss 2.3954\n",
      "step 23120: train loss 2.5138, val loss 2.4157\n",
      "step 23130: train loss 2.4471, val loss 2.4352\n",
      "step 23140: train loss 2.3630, val loss 2.3334\n",
      "step 23150: train loss 2.3444, val loss 2.3830\n",
      "step 23160: train loss 2.3802, val loss 2.4685\n",
      "step 23170: train loss 2.4227, val loss 2.3944\n",
      "step 23180: train loss 2.4362, val loss 2.3966\n",
      "step 23190: train loss 2.3571, val loss 2.2713\n",
      "step 23200: train loss 2.3988, val loss 2.3524\n",
      "step 23210: train loss 2.4217, val loss 2.3463\n",
      "step 23220: train loss 2.4335, val loss 2.3930\n",
      "step 23230: train loss 2.3595, val loss 2.4035\n",
      "step 23240: train loss 2.3969, val loss 2.3737\n",
      "step 23250: train loss 2.4013, val loss 2.2784\n",
      "step 23260: train loss 2.3325, val loss 2.3962\n",
      "step 23270: train loss 2.3801, val loss 2.3760\n",
      "step 23280: train loss 2.3171, val loss 2.3988\n",
      "step 23290: train loss 2.3512, val loss 2.3503\n",
      "step 23300: train loss 2.3968, val loss 2.3712\n",
      "step 23310: train loss 2.4214, val loss 2.4327\n",
      "step 23320: train loss 2.3703, val loss 2.3624\n",
      "step 23330: train loss 2.4766, val loss 2.4301\n",
      "step 23340: train loss 2.3615, val loss 2.3664\n",
      "step 23350: train loss 2.4213, val loss 2.3722\n",
      "step 23360: train loss 2.3807, val loss 2.3834\n",
      "step 23370: train loss 2.3450, val loss 2.3682\n",
      "step 23380: train loss 2.3458, val loss 2.3395\n",
      "step 23390: train loss 2.3597, val loss 2.3380\n",
      "step 23400: train loss 2.3892, val loss 2.3946\n",
      "step 23410: train loss 2.3769, val loss 2.3653\n",
      "step 23420: train loss 2.4117, val loss 2.3760\n",
      "step 23430: train loss 2.3847, val loss 2.3188\n",
      "step 23440: train loss 2.3534, val loss 2.3466\n",
      "step 23450: train loss 2.4772, val loss 2.3711\n",
      "step 23460: train loss 2.3628, val loss 2.4761\n",
      "step 23470: train loss 2.3782, val loss 2.4418\n",
      "step 23480: train loss 2.4664, val loss 2.3907\n",
      "step 23490: train loss 2.3405, val loss 2.3603\n",
      "step 23500: train loss 2.3659, val loss 2.4365\n",
      "step 23510: train loss 2.4063, val loss 2.3261\n",
      "step 23520: train loss 2.5019, val loss 2.3896\n",
      "step 23530: train loss 2.3319, val loss 2.3126\n",
      "step 23540: train loss 2.3641, val loss 2.4292\n",
      "step 23550: train loss 2.3060, val loss 2.3635\n",
      "step 23560: train loss 2.3751, val loss 2.4532\n",
      "step 23570: train loss 2.3808, val loss 2.4605\n",
      "step 23580: train loss 2.3914, val loss 2.3273\n",
      "step 23590: train loss 2.3890, val loss 2.3980\n",
      "step 23600: train loss 2.3812, val loss 2.3936\n",
      "step 23610: train loss 2.3995, val loss 2.2978\n",
      "step 23620: train loss 2.3352, val loss 2.4829\n",
      "step 23630: train loss 2.4337, val loss 2.4435\n",
      "step 23640: train loss 2.4919, val loss 2.5038\n",
      "step 23650: train loss 2.4294, val loss 2.3881\n",
      "step 23660: train loss 2.4000, val loss 2.4432\n",
      "step 23670: train loss 2.3323, val loss 2.4413\n",
      "step 23680: train loss 2.3476, val loss 2.3989\n",
      "step 23690: train loss 2.4231, val loss 2.3978\n",
      "step 23700: train loss 2.3387, val loss 2.3483\n",
      "step 23710: train loss 2.4018, val loss 2.3954\n",
      "step 23720: train loss 2.3845, val loss 2.3052\n",
      "step 23730: train loss 2.3908, val loss 2.4438\n",
      "step 23740: train loss 2.3367, val loss 2.3129\n",
      "step 23750: train loss 2.3459, val loss 2.3821\n",
      "step 23760: train loss 2.3627, val loss 2.4512\n",
      "step 23770: train loss 2.3898, val loss 2.4066\n",
      "step 23780: train loss 2.3845, val loss 2.4177\n",
      "step 23790: train loss 2.4155, val loss 2.3451\n",
      "step 23800: train loss 2.4145, val loss 2.4503\n",
      "step 23810: train loss 2.3711, val loss 2.4434\n",
      "step 23820: train loss 2.3696, val loss 2.4818\n",
      "step 23830: train loss 2.3676, val loss 2.4661\n",
      "step 23840: train loss 2.3982, val loss 2.4123\n",
      "step 23850: train loss 2.3732, val loss 2.4206\n",
      "step 23860: train loss 2.4271, val loss 2.3568\n",
      "step 23870: train loss 2.4108, val loss 2.4803\n",
      "step 23880: train loss 2.4184, val loss 2.3565\n",
      "step 23890: train loss 2.3612, val loss 2.3996\n",
      "step 23900: train loss 2.3524, val loss 2.4222\n",
      "step 23910: train loss 2.3899, val loss 2.3934\n",
      "step 23920: train loss 2.3537, val loss 2.3226\n",
      "step 23930: train loss 2.4235, val loss 2.4348\n",
      "step 23940: train loss 2.4328, val loss 2.4319\n",
      "step 23950: train loss 2.4727, val loss 2.3429\n",
      "step 23960: train loss 2.3917, val loss 2.4632\n",
      "step 23970: train loss 2.4266, val loss 2.3467\n",
      "step 23980: train loss 2.3784, val loss 2.4042\n",
      "step 23990: train loss 2.3842, val loss 2.2910\n",
      "step 24000: train loss 2.3890, val loss 2.4232\n",
      "step 24010: train loss 2.3728, val loss 2.4231\n",
      "step 24020: train loss 2.3860, val loss 2.4145\n",
      "step 24030: train loss 2.4165, val loss 2.2892\n",
      "step 24040: train loss 2.3722, val loss 2.4337\n",
      "step 24050: train loss 2.3441, val loss 2.3589\n",
      "step 24060: train loss 2.3477, val loss 2.3693\n",
      "step 24070: train loss 2.4545, val loss 2.4151\n",
      "step 24080: train loss 2.3822, val loss 2.3626\n",
      "step 24090: train loss 2.3989, val loss 2.4013\n",
      "step 24100: train loss 2.3942, val loss 2.3085\n",
      "step 24110: train loss 2.4192, val loss 2.3435\n",
      "step 24120: train loss 2.3225, val loss 2.3325\n",
      "step 24130: train loss 2.3997, val loss 2.3804\n",
      "step 24140: train loss 2.4128, val loss 2.3787\n",
      "step 24150: train loss 2.4023, val loss 2.3397\n",
      "step 24160: train loss 2.3502, val loss 2.3247\n",
      "step 24170: train loss 2.3425, val loss 2.4086\n",
      "step 24180: train loss 2.3762, val loss 2.3541\n",
      "step 24190: train loss 2.3794, val loss 2.3996\n",
      "step 24200: train loss 2.4312, val loss 2.4141\n",
      "step 24210: train loss 2.3103, val loss 2.3395\n",
      "step 24220: train loss 2.4763, val loss 2.3739\n",
      "step 24230: train loss 2.3583, val loss 2.3590\n",
      "step 24240: train loss 2.4104, val loss 2.3449\n",
      "step 24250: train loss 2.3843, val loss 2.3612\n",
      "step 24260: train loss 2.3299, val loss 2.3797\n",
      "step 24270: train loss 2.3627, val loss 2.3658\n",
      "step 24280: train loss 2.4166, val loss 2.3803\n",
      "step 24290: train loss 2.4009, val loss 2.3554\n",
      "step 24300: train loss 2.3821, val loss 2.3808\n",
      "step 24310: train loss 2.3530, val loss 2.4326\n",
      "step 24320: train loss 2.4085, val loss 2.3475\n",
      "step 24330: train loss 2.3810, val loss 2.4255\n",
      "step 24340: train loss 2.3304, val loss 2.4781\n",
      "step 24350: train loss 2.3627, val loss 2.3050\n",
      "step 24360: train loss 2.4731, val loss 2.2844\n",
      "step 24370: train loss 2.3303, val loss 2.3851\n",
      "step 24380: train loss 2.4081, val loss 2.3773\n",
      "step 24390: train loss 2.4276, val loss 2.3065\n",
      "step 24400: train loss 2.4344, val loss 2.4090\n",
      "step 24410: train loss 2.3816, val loss 2.3324\n",
      "step 24420: train loss 2.4566, val loss 2.3849\n",
      "step 24430: train loss 2.3487, val loss 2.4253\n",
      "step 24440: train loss 2.5073, val loss 2.5128\n",
      "step 24450: train loss 2.3731, val loss 2.4148\n",
      "step 24460: train loss 2.4319, val loss 2.4976\n",
      "step 24470: train loss 2.3906, val loss 2.3564\n",
      "step 24480: train loss 2.4442, val loss 2.3881\n",
      "step 24490: train loss 2.4611, val loss 2.4422\n",
      "step 24500: train loss 2.4235, val loss 2.4251\n",
      "step 24510: train loss 2.3647, val loss 2.3705\n",
      "step 24520: train loss 2.4217, val loss 2.2857\n",
      "step 24530: train loss 2.3350, val loss 2.3306\n",
      "step 24540: train loss 2.3321, val loss 2.3181\n",
      "step 24550: train loss 2.3863, val loss 2.4527\n",
      "step 24560: train loss 2.3890, val loss 2.4266\n",
      "step 24570: train loss 2.4141, val loss 2.3097\n",
      "step 24580: train loss 2.3968, val loss 2.3737\n",
      "step 24590: train loss 2.4232, val loss 2.3386\n",
      "step 24600: train loss 2.4042, val loss 2.3842\n",
      "step 24610: train loss 2.3821, val loss 2.3686\n",
      "step 24620: train loss 2.4142, val loss 2.3759\n",
      "step 24630: train loss 2.4124, val loss 2.4267\n",
      "step 24640: train loss 2.4299, val loss 2.3572\n",
      "step 24650: train loss 2.4120, val loss 2.3320\n",
      "step 24660: train loss 2.4552, val loss 2.3084\n",
      "step 24670: train loss 2.3710, val loss 2.3736\n",
      "step 24680: train loss 2.3649, val loss 2.4059\n",
      "step 24690: train loss 2.3555, val loss 2.3724\n",
      "step 24700: train loss 2.4011, val loss 2.4953\n",
      "step 24710: train loss 2.4116, val loss 2.3571\n",
      "step 24720: train loss 2.3635, val loss 2.3552\n",
      "step 24730: train loss 2.4119, val loss 2.3332\n",
      "step 24740: train loss 2.3401, val loss 2.3579\n",
      "step 24750: train loss 2.3700, val loss 2.4608\n",
      "step 24760: train loss 2.3843, val loss 2.3907\n",
      "step 24770: train loss 2.4051, val loss 2.3327\n",
      "step 24780: train loss 2.3616, val loss 2.4318\n",
      "step 24790: train loss 2.4727, val loss 2.3644\n",
      "step 24800: train loss 2.3791, val loss 2.3399\n",
      "step 24810: train loss 2.3061, val loss 2.3961\n",
      "step 24820: train loss 2.3980, val loss 2.3568\n",
      "step 24830: train loss 2.3937, val loss 2.3988\n",
      "step 24840: train loss 2.3929, val loss 2.3698\n",
      "step 24850: train loss 2.3574, val loss 2.4100\n",
      "step 24860: train loss 2.4572, val loss 2.4084\n",
      "step 24870: train loss 2.3869, val loss 2.3536\n",
      "step 24880: train loss 2.4481, val loss 2.3317\n",
      "step 24890: train loss 2.4273, val loss 2.3578\n",
      "step 24900: train loss 2.4094, val loss 2.3177\n",
      "step 24910: train loss 2.4716, val loss 2.4899\n",
      "step 24920: train loss 2.3026, val loss 2.4581\n",
      "step 24930: train loss 2.3975, val loss 2.3407\n",
      "step 24940: train loss 2.3607, val loss 2.4535\n",
      "step 24950: train loss 2.3869, val loss 2.4667\n",
      "step 24960: train loss 2.3872, val loss 2.4404\n",
      "step 24970: train loss 2.4044, val loss 2.2907\n",
      "step 24980: train loss 2.3472, val loss 2.4124\n",
      "step 24990: train loss 2.3338, val loss 2.4227\n",
      "step 25000: train loss 2.4599, val loss 2.3978\n",
      "step 25010: train loss 2.3395, val loss 2.3275\n",
      "step 25020: train loss 2.4703, val loss 2.3359\n",
      "step 25030: train loss 2.4255, val loss 2.4166\n",
      "step 25040: train loss 2.4437, val loss 2.4839\n",
      "step 25050: train loss 2.4207, val loss 2.3826\n",
      "step 25060: train loss 2.4195, val loss 2.3769\n",
      "step 25070: train loss 2.3868, val loss 2.4236\n",
      "step 25080: train loss 2.3566, val loss 2.3978\n",
      "step 25090: train loss 2.3451, val loss 2.4077\n",
      "step 25100: train loss 2.3975, val loss 2.3921\n",
      "step 25110: train loss 2.4259, val loss 2.4236\n",
      "step 25120: train loss 2.4807, val loss 2.3933\n",
      "step 25130: train loss 2.3879, val loss 2.3769\n",
      "step 25140: train loss 2.3951, val loss 2.4044\n",
      "step 25150: train loss 2.3721, val loss 2.3736\n",
      "step 25160: train loss 2.3832, val loss 2.3948\n",
      "step 25170: train loss 2.4052, val loss 2.3918\n",
      "step 25180: train loss 2.3690, val loss 2.3978\n",
      "step 25190: train loss 2.3788, val loss 2.4277\n",
      "step 25200: train loss 2.4289, val loss 2.3348\n",
      "step 25210: train loss 2.4249, val loss 2.3829\n",
      "step 25220: train loss 2.3655, val loss 2.3662\n",
      "step 25230: train loss 2.4397, val loss 2.3536\n",
      "step 25240: train loss 2.3978, val loss 2.3568\n",
      "step 25250: train loss 2.4118, val loss 2.4589\n",
      "step 25260: train loss 2.3792, val loss 2.3679\n",
      "step 25270: train loss 2.4012, val loss 2.4327\n",
      "step 25280: train loss 2.4163, val loss 2.3788\n",
      "step 25290: train loss 2.3694, val loss 2.4197\n",
      "step 25300: train loss 2.4014, val loss 2.4718\n",
      "step 25310: train loss 2.4042, val loss 2.4190\n",
      "step 25320: train loss 2.4111, val loss 2.3456\n",
      "step 25330: train loss 2.4175, val loss 2.4393\n",
      "step 25340: train loss 2.4082, val loss 2.4405\n",
      "step 25350: train loss 2.3749, val loss 2.4354\n",
      "step 25360: train loss 2.3436, val loss 2.3258\n",
      "step 25370: train loss 2.3644, val loss 2.3826\n",
      "step 25380: train loss 2.4444, val loss 2.4163\n",
      "step 25390: train loss 2.3513, val loss 2.4433\n",
      "step 25400: train loss 2.4345, val loss 2.3231\n",
      "step 25410: train loss 2.4005, val loss 2.3364\n",
      "step 25420: train loss 2.3824, val loss 2.3863\n",
      "step 25430: train loss 2.3697, val loss 2.3952\n",
      "step 25440: train loss 2.3162, val loss 2.4336\n",
      "step 25450: train loss 2.3731, val loss 2.4485\n",
      "step 25460: train loss 2.4071, val loss 2.4203\n",
      "step 25470: train loss 2.3121, val loss 2.4047\n",
      "step 25480: train loss 2.3639, val loss 2.4004\n",
      "step 25490: train loss 2.4551, val loss 2.3810\n",
      "step 25500: train loss 2.3841, val loss 2.4241\n",
      "step 25510: train loss 2.3635, val loss 2.4707\n",
      "step 25520: train loss 2.3656, val loss 2.3620\n",
      "step 25530: train loss 2.3959, val loss 2.3549\n",
      "step 25540: train loss 2.3788, val loss 2.4771\n",
      "step 25550: train loss 2.4247, val loss 2.4553\n",
      "step 25560: train loss 2.3221, val loss 2.4337\n",
      "step 25570: train loss 2.3595, val loss 2.4007\n",
      "step 25580: train loss 2.4322, val loss 2.4033\n",
      "step 25590: train loss 2.3967, val loss 2.3910\n",
      "step 25600: train loss 2.3669, val loss 2.3706\n",
      "step 25610: train loss 2.3486, val loss 2.4286\n",
      "step 25620: train loss 2.4459, val loss 2.4355\n",
      "step 25630: train loss 2.3908, val loss 2.3838\n",
      "step 25640: train loss 2.4469, val loss 2.4239\n",
      "step 25650: train loss 2.4515, val loss 2.4257\n",
      "step 25660: train loss 2.4668, val loss 2.3643\n",
      "step 25670: train loss 2.3979, val loss 2.3691\n",
      "step 25680: train loss 2.3927, val loss 2.3577\n",
      "step 25690: train loss 2.3558, val loss 2.3724\n",
      "step 25700: train loss 2.3744, val loss 2.3576\n",
      "step 25710: train loss 2.4569, val loss 2.3510\n",
      "step 25720: train loss 2.3588, val loss 2.3500\n",
      "step 25730: train loss 2.4320, val loss 2.4616\n",
      "step 25740: train loss 2.3907, val loss 2.3506\n",
      "step 25750: train loss 2.4503, val loss 2.3632\n",
      "step 25760: train loss 2.3615, val loss 2.3447\n",
      "step 25770: train loss 2.3858, val loss 2.3534\n",
      "step 25780: train loss 2.4183, val loss 2.3979\n",
      "step 25790: train loss 2.3205, val loss 2.3651\n",
      "step 25800: train loss 2.3732, val loss 2.4658\n",
      "step 25810: train loss 2.4045, val loss 2.3563\n",
      "step 25820: train loss 2.4001, val loss 2.3983\n",
      "step 25830: train loss 2.4293, val loss 2.3573\n",
      "step 25840: train loss 2.4327, val loss 2.4143\n",
      "step 25850: train loss 2.3694, val loss 2.4247\n",
      "step 25860: train loss 2.4173, val loss 2.3546\n",
      "step 25870: train loss 2.4560, val loss 2.3422\n",
      "step 25880: train loss 2.4423, val loss 2.3770\n",
      "step 25890: train loss 2.3721, val loss 2.3524\n",
      "step 25900: train loss 2.4115, val loss 2.3923\n",
      "step 25910: train loss 2.4124, val loss 2.4226\n",
      "step 25920: train loss 2.4033, val loss 2.3523\n",
      "step 25930: train loss 2.3780, val loss 2.4136\n",
      "step 25940: train loss 2.3931, val loss 2.3687\n",
      "step 25950: train loss 2.3249, val loss 2.4518\n",
      "step 25960: train loss 2.3790, val loss 2.3463\n",
      "step 25970: train loss 2.3796, val loss 2.3919\n",
      "step 25980: train loss 2.3272, val loss 2.3723\n",
      "step 25990: train loss 2.4708, val loss 2.4314\n",
      "step 26000: train loss 2.4038, val loss 2.3724\n",
      "step 26010: train loss 2.4075, val loss 2.3676\n",
      "step 26020: train loss 2.4300, val loss 2.4248\n",
      "step 26030: train loss 2.4334, val loss 2.3040\n",
      "step 26040: train loss 2.5252, val loss 2.4421\n",
      "step 26050: train loss 2.2885, val loss 2.3370\n",
      "step 26060: train loss 2.3893, val loss 2.3808\n",
      "step 26070: train loss 2.3897, val loss 2.3407\n",
      "step 26080: train loss 2.4077, val loss 2.3784\n",
      "step 26090: train loss 2.3833, val loss 2.3553\n",
      "step 26100: train loss 2.3869, val loss 2.3614\n",
      "step 26110: train loss 2.4048, val loss 2.3759\n",
      "step 26120: train loss 2.3831, val loss 2.4347\n",
      "step 26130: train loss 2.4412, val loss 2.3557\n",
      "step 26140: train loss 2.3732, val loss 2.4042\n",
      "step 26150: train loss 2.3593, val loss 2.3322\n",
      "step 26160: train loss 2.4089, val loss 2.2900\n",
      "step 26170: train loss 2.3462, val loss 2.3621\n",
      "step 26180: train loss 2.4416, val loss 2.4534\n",
      "step 26190: train loss 2.3309, val loss 2.4481\n",
      "step 26200: train loss 2.4472, val loss 2.4153\n",
      "step 26210: train loss 2.3913, val loss 2.4012\n",
      "step 26220: train loss 2.3362, val loss 2.3627\n",
      "step 26230: train loss 2.3768, val loss 2.4226\n",
      "step 26240: train loss 2.4639, val loss 2.4079\n",
      "step 26250: train loss 2.3548, val loss 2.3772\n",
      "step 26260: train loss 2.3685, val loss 2.3443\n",
      "step 26270: train loss 2.3581, val loss 2.3803\n",
      "step 26280: train loss 2.4270, val loss 2.4197\n",
      "step 26290: train loss 2.4044, val loss 2.4438\n",
      "step 26300: train loss 2.3906, val loss 2.4263\n",
      "step 26310: train loss 2.3716, val loss 2.3378\n",
      "step 26320: train loss 2.4013, val loss 2.3309\n",
      "step 26330: train loss 2.4140, val loss 2.3824\n",
      "step 26340: train loss 2.4556, val loss 2.3803\n",
      "step 26350: train loss 2.4858, val loss 2.3739\n",
      "step 26360: train loss 2.3671, val loss 2.4233\n",
      "step 26370: train loss 2.4649, val loss 2.4364\n",
      "step 26380: train loss 2.4205, val loss 2.3887\n",
      "step 26390: train loss 2.3695, val loss 2.3896\n",
      "step 26400: train loss 2.3777, val loss 2.3744\n",
      "step 26410: train loss 2.3509, val loss 2.3715\n",
      "step 26420: train loss 2.4134, val loss 2.4468\n",
      "step 26430: train loss 2.4135, val loss 2.3311\n",
      "step 26440: train loss 2.2918, val loss 2.3921\n",
      "step 26450: train loss 2.3600, val loss 2.3810\n",
      "step 26460: train loss 2.3977, val loss 2.4200\n",
      "step 26470: train loss 2.4119, val loss 2.3519\n",
      "step 26480: train loss 2.3883, val loss 2.4117\n",
      "step 26490: train loss 2.3555, val loss 2.4002\n",
      "step 26500: train loss 2.3576, val loss 2.3987\n",
      "step 26510: train loss 2.3950, val loss 2.3864\n",
      "step 26520: train loss 2.4681, val loss 2.3947\n",
      "step 26530: train loss 2.4109, val loss 2.4596\n",
      "step 26540: train loss 2.3806, val loss 2.4374\n",
      "step 26550: train loss 2.3431, val loss 2.5116\n",
      "step 26560: train loss 2.4646, val loss 2.3358\n",
      "step 26570: train loss 2.3719, val loss 2.3681\n",
      "step 26580: train loss 2.4006, val loss 2.3597\n",
      "step 26590: train loss 2.3339, val loss 2.4378\n",
      "step 26600: train loss 2.3461, val loss 2.3573\n",
      "step 26610: train loss 2.4393, val loss 2.3859\n",
      "step 26620: train loss 2.4133, val loss 2.4505\n",
      "step 26630: train loss 2.3941, val loss 2.4072\n",
      "step 26640: train loss 2.3827, val loss 2.3352\n",
      "step 26650: train loss 2.3489, val loss 2.3106\n",
      "step 26660: train loss 2.4281, val loss 2.3662\n",
      "step 26670: train loss 2.3487, val loss 2.3226\n",
      "step 26680: train loss 2.3435, val loss 2.3779\n",
      "step 26690: train loss 2.3495, val loss 2.3703\n",
      "step 26700: train loss 2.4387, val loss 2.3827\n",
      "step 26710: train loss 2.3073, val loss 2.4207\n",
      "step 26720: train loss 2.3609, val loss 2.2351\n",
      "step 26730: train loss 2.4029, val loss 2.3830\n",
      "step 26740: train loss 2.3982, val loss 2.4092\n",
      "step 26750: train loss 2.4921, val loss 2.4144\n",
      "step 26760: train loss 2.4055, val loss 2.4140\n",
      "step 26770: train loss 2.3446, val loss 2.4133\n",
      "step 26780: train loss 2.4591, val loss 2.3723\n",
      "step 26790: train loss 2.2974, val loss 2.4309\n",
      "step 26800: train loss 2.4039, val loss 2.3764\n",
      "step 26810: train loss 2.3570, val loss 2.3447\n",
      "step 26820: train loss 2.3914, val loss 2.4139\n",
      "step 26830: train loss 2.4050, val loss 2.3678\n",
      "step 26840: train loss 2.3671, val loss 2.3508\n",
      "step 26850: train loss 2.3145, val loss 2.3372\n",
      "step 26860: train loss 2.4345, val loss 2.3925\n",
      "step 26870: train loss 2.4777, val loss 2.3319\n",
      "step 26880: train loss 2.4381, val loss 2.3164\n",
      "step 26890: train loss 2.3408, val loss 2.3924\n",
      "step 26900: train loss 2.4444, val loss 2.3676\n",
      "step 26910: train loss 2.3287, val loss 2.3107\n",
      "step 26920: train loss 2.2808, val loss 2.3949\n",
      "step 26930: train loss 2.3776, val loss 2.3384\n",
      "step 26940: train loss 2.4268, val loss 2.3907\n",
      "step 26950: train loss 2.3937, val loss 2.3436\n",
      "step 26960: train loss 2.4216, val loss 2.4009\n",
      "step 26970: train loss 2.3430, val loss 2.3789\n",
      "step 26980: train loss 2.3590, val loss 2.3724\n",
      "step 26990: train loss 2.3926, val loss 2.4283\n",
      "step 27000: train loss 2.3874, val loss 2.2987\n",
      "step 27010: train loss 2.3957, val loss 2.3921\n",
      "step 27020: train loss 2.3206, val loss 2.4265\n",
      "step 27030: train loss 2.4354, val loss 2.4466\n",
      "step 27040: train loss 2.3303, val loss 2.3620\n",
      "step 27050: train loss 2.4395, val loss 2.3986\n",
      "step 27060: train loss 2.3954, val loss 2.4193\n",
      "step 27070: train loss 2.4395, val loss 2.3321\n",
      "step 27080: train loss 2.4365, val loss 2.3444\n",
      "step 27090: train loss 2.3837, val loss 2.3803\n",
      "step 27100: train loss 2.4386, val loss 2.4469\n",
      "step 27110: train loss 2.3519, val loss 2.4124\n",
      "step 27120: train loss 2.3482, val loss 2.3090\n",
      "step 27130: train loss 2.3445, val loss 2.3615\n",
      "step 27140: train loss 2.3581, val loss 2.4107\n",
      "step 27150: train loss 2.4281, val loss 2.3613\n",
      "step 27160: train loss 2.4092, val loss 2.3285\n",
      "step 27170: train loss 2.4226, val loss 2.4133\n",
      "step 27180: train loss 2.3141, val loss 2.3316\n",
      "step 27190: train loss 2.3815, val loss 2.4363\n",
      "step 27200: train loss 2.3858, val loss 2.3537\n",
      "step 27210: train loss 2.4070, val loss 2.3969\n",
      "step 27220: train loss 2.4089, val loss 2.3468\n",
      "step 27230: train loss 2.4216, val loss 2.3599\n",
      "step 27240: train loss 2.3768, val loss 2.3722\n",
      "step 27250: train loss 2.3428, val loss 2.3472\n",
      "step 27260: train loss 2.3197, val loss 2.4487\n",
      "step 27270: train loss 2.3220, val loss 2.3015\n",
      "step 27280: train loss 2.3847, val loss 2.3389\n",
      "step 27290: train loss 2.4566, val loss 2.3978\n",
      "step 27300: train loss 2.3920, val loss 2.4676\n",
      "step 27310: train loss 2.2885, val loss 2.3547\n",
      "step 27320: train loss 2.3819, val loss 2.3843\n",
      "step 27330: train loss 2.3833, val loss 2.3893\n",
      "step 27340: train loss 2.4118, val loss 2.4349\n",
      "step 27350: train loss 2.4563, val loss 2.3664\n",
      "step 27360: train loss 2.4045, val loss 2.4374\n",
      "step 27370: train loss 2.4089, val loss 2.3105\n",
      "step 27380: train loss 2.3953, val loss 2.3160\n",
      "step 27390: train loss 2.3960, val loss 2.3471\n",
      "step 27400: train loss 2.2999, val loss 2.3098\n",
      "step 27410: train loss 2.3744, val loss 2.3766\n",
      "step 27420: train loss 2.3977, val loss 2.3953\n",
      "step 27430: train loss 2.4359, val loss 2.4443\n",
      "step 27440: train loss 2.4440, val loss 2.3786\n",
      "step 27450: train loss 2.4071, val loss 2.3381\n",
      "step 27460: train loss 2.4437, val loss 2.4528\n",
      "step 27470: train loss 2.3002, val loss 2.3563\n",
      "step 27480: train loss 2.3022, val loss 2.3744\n",
      "step 27490: train loss 2.3511, val loss 2.3881\n",
      "step 27500: train loss 2.3235, val loss 2.3454\n",
      "step 27510: train loss 2.4012, val loss 2.3101\n",
      "step 27520: train loss 2.3703, val loss 2.2494\n",
      "step 27530: train loss 2.3476, val loss 2.2971\n",
      "step 27540: train loss 2.3441, val loss 2.3634\n",
      "step 27550: train loss 2.3624, val loss 2.3765\n",
      "step 27560: train loss 2.3957, val loss 2.3504\n",
      "step 27570: train loss 2.3562, val loss 2.4103\n",
      "step 27580: train loss 2.4108, val loss 2.3906\n",
      "step 27590: train loss 2.3749, val loss 2.3690\n",
      "step 27600: train loss 2.3791, val loss 2.3807\n",
      "step 27610: train loss 2.4333, val loss 2.4328\n",
      "step 27620: train loss 2.4726, val loss 2.4546\n",
      "step 27630: train loss 2.3729, val loss 2.3628\n",
      "step 27640: train loss 2.3307, val loss 2.4045\n",
      "step 27650: train loss 2.4366, val loss 2.3508\n",
      "step 27660: train loss 2.4251, val loss 2.4132\n",
      "step 27670: train loss 2.4509, val loss 2.3673\n",
      "step 27680: train loss 2.3311, val loss 2.3918\n",
      "step 27690: train loss 2.3266, val loss 2.4278\n",
      "step 27700: train loss 2.3071, val loss 2.4044\n",
      "step 27710: train loss 2.4889, val loss 2.3883\n",
      "step 27720: train loss 2.3515, val loss 2.3353\n",
      "step 27730: train loss 2.4157, val loss 2.3594\n",
      "step 27740: train loss 2.4461, val loss 2.2578\n",
      "step 27750: train loss 2.3715, val loss 2.3810\n",
      "step 27760: train loss 2.4589, val loss 2.3900\n",
      "step 27770: train loss 2.3708, val loss 2.3698\n",
      "step 27780: train loss 2.3690, val loss 2.3718\n",
      "step 27790: train loss 2.3659, val loss 2.4870\n",
      "step 27800: train loss 2.3827, val loss 2.3066\n",
      "step 27810: train loss 2.4422, val loss 2.3851\n",
      "step 27820: train loss 2.4435, val loss 2.4050\n",
      "step 27830: train loss 2.4657, val loss 2.3695\n",
      "step 27840: train loss 2.4345, val loss 2.3498\n",
      "step 27850: train loss 2.4542, val loss 2.3744\n",
      "step 27860: train loss 2.3743, val loss 2.3120\n",
      "step 27870: train loss 2.3297, val loss 2.3700\n",
      "step 27880: train loss 2.3670, val loss 2.3740\n",
      "step 27890: train loss 2.3420, val loss 2.3208\n",
      "step 27900: train loss 2.4138, val loss 2.3762\n",
      "step 27910: train loss 2.2978, val loss 2.3919\n",
      "step 27920: train loss 2.4153, val loss 2.3058\n",
      "step 27930: train loss 2.4108, val loss 2.3699\n",
      "step 27940: train loss 2.3779, val loss 2.3513\n",
      "step 27950: train loss 2.3716, val loss 2.3774\n",
      "step 27960: train loss 2.3546, val loss 2.4522\n",
      "step 27970: train loss 2.3763, val loss 2.3790\n",
      "step 27980: train loss 2.3361, val loss 2.3382\n",
      "step 27990: train loss 2.3712, val loss 2.3796\n",
      "step 28000: train loss 2.4210, val loss 2.4054\n",
      "step 28010: train loss 2.3722, val loss 2.4345\n",
      "step 28020: train loss 2.4095, val loss 2.2904\n",
      "step 28030: train loss 2.3724, val loss 2.3829\n",
      "step 28040: train loss 2.3396, val loss 2.4279\n",
      "step 28050: train loss 2.3688, val loss 2.3438\n",
      "step 28060: train loss 2.3634, val loss 2.3154\n",
      "step 28070: train loss 2.3438, val loss 2.4430\n",
      "step 28080: train loss 2.3716, val loss 2.4527\n",
      "step 28090: train loss 2.4221, val loss 2.3559\n",
      "step 28100: train loss 2.3438, val loss 2.3256\n",
      "step 28110: train loss 2.4037, val loss 2.3669\n",
      "step 28120: train loss 2.3810, val loss 2.3104\n",
      "step 28130: train loss 2.4217, val loss 2.4164\n",
      "step 28140: train loss 2.3725, val loss 2.4076\n",
      "step 28150: train loss 2.3922, val loss 2.4523\n",
      "step 28160: train loss 2.3614, val loss 2.3457\n",
      "step 28170: train loss 2.5293, val loss 2.4037\n",
      "step 28180: train loss 2.4531, val loss 2.3953\n",
      "step 28190: train loss 2.4000, val loss 2.3995\n",
      "step 28200: train loss 2.3992, val loss 2.3817\n",
      "step 28210: train loss 2.3242, val loss 2.3210\n",
      "step 28220: train loss 2.3603, val loss 2.3543\n",
      "step 28230: train loss 2.3820, val loss 2.3486\n",
      "step 28240: train loss 2.3466, val loss 2.2516\n",
      "step 28250: train loss 2.3991, val loss 2.4290\n",
      "step 28260: train loss 2.3899, val loss 2.3339\n",
      "step 28270: train loss 2.3823, val loss 2.3781\n",
      "step 28280: train loss 2.3795, val loss 2.4240\n",
      "step 28290: train loss 2.4706, val loss 2.3763\n",
      "step 28300: train loss 2.4010, val loss 2.3497\n",
      "step 28310: train loss 2.4105, val loss 2.3561\n",
      "step 28320: train loss 2.4098, val loss 2.3533\n",
      "step 28330: train loss 2.3913, val loss 2.4405\n",
      "step 28340: train loss 2.3206, val loss 2.3637\n",
      "step 28350: train loss 2.3841, val loss 2.3932\n",
      "step 28360: train loss 2.3892, val loss 2.3945\n",
      "step 28370: train loss 2.3734, val loss 2.3721\n",
      "step 28380: train loss 2.4216, val loss 2.4253\n",
      "step 28390: train loss 2.4395, val loss 2.3932\n",
      "step 28400: train loss 2.3900, val loss 2.3930\n",
      "step 28410: train loss 2.3520, val loss 2.4077\n",
      "step 28420: train loss 2.4003, val loss 2.4261\n",
      "step 28430: train loss 2.3724, val loss 2.3885\n",
      "step 28440: train loss 2.3240, val loss 2.4084\n",
      "step 28450: train loss 2.3887, val loss 2.3541\n",
      "step 28460: train loss 2.4042, val loss 2.3368\n",
      "step 28470: train loss 2.4218, val loss 2.3607\n",
      "step 28480: train loss 2.3158, val loss 2.3501\n",
      "step 28490: train loss 2.3369, val loss 2.3950\n",
      "step 28500: train loss 2.4461, val loss 2.3445\n",
      "step 28510: train loss 2.3661, val loss 2.4043\n",
      "step 28520: train loss 2.3868, val loss 2.3595\n",
      "step 28530: train loss 2.4112, val loss 2.3864\n",
      "step 28540: train loss 2.3569, val loss 2.3688\n",
      "step 28550: train loss 2.4051, val loss 2.3205\n",
      "step 28560: train loss 2.3855, val loss 2.3538\n",
      "step 28570: train loss 2.4329, val loss 2.3836\n",
      "step 28580: train loss 2.3332, val loss 2.3862\n",
      "step 28590: train loss 2.3336, val loss 2.3716\n",
      "step 28600: train loss 2.4807, val loss 2.3748\n",
      "step 28610: train loss 2.4154, val loss 2.3978\n",
      "step 28620: train loss 2.4501, val loss 2.4286\n",
      "step 28630: train loss 2.3876, val loss 2.4165\n",
      "step 28640: train loss 2.3731, val loss 2.4379\n",
      "step 28650: train loss 2.4410, val loss 2.3976\n",
      "step 28660: train loss 2.3296, val loss 2.3796\n",
      "step 28670: train loss 2.4735, val loss 2.3411\n",
      "step 28680: train loss 2.3457, val loss 2.3016\n",
      "step 28690: train loss 2.3769, val loss 2.4140\n",
      "step 28700: train loss 2.3611, val loss 2.3416\n",
      "step 28710: train loss 2.3691, val loss 2.3317\n",
      "step 28720: train loss 2.3477, val loss 2.4294\n",
      "step 28730: train loss 2.4267, val loss 2.3479\n",
      "step 28740: train loss 2.4329, val loss 2.2808\n",
      "step 28750: train loss 2.3099, val loss 2.3998\n",
      "step 28760: train loss 2.3806, val loss 2.4060\n",
      "step 28770: train loss 2.2786, val loss 2.4035\n",
      "step 28780: train loss 2.4530, val loss 2.3391\n",
      "step 28790: train loss 2.3785, val loss 2.3545\n",
      "step 28800: train loss 2.2842, val loss 2.3601\n",
      "step 28810: train loss 2.4097, val loss 2.3248\n",
      "step 28820: train loss 2.4118, val loss 2.4117\n",
      "step 28830: train loss 2.3970, val loss 2.4118\n",
      "step 28840: train loss 2.3423, val loss 2.4024\n",
      "step 28850: train loss 2.3737, val loss 2.4286\n",
      "step 28860: train loss 2.3907, val loss 2.4194\n",
      "step 28870: train loss 2.4343, val loss 2.3902\n",
      "step 28880: train loss 2.3958, val loss 2.3775\n",
      "step 28890: train loss 2.3946, val loss 2.3433\n",
      "step 28900: train loss 2.3663, val loss 2.3621\n",
      "step 28910: train loss 2.4169, val loss 2.3815\n",
      "step 28920: train loss 2.3897, val loss 2.2981\n",
      "step 28930: train loss 2.3166, val loss 2.4326\n",
      "step 28940: train loss 2.3630, val loss 2.3731\n",
      "step 28950: train loss 2.3961, val loss 2.4374\n",
      "step 28960: train loss 2.3760, val loss 2.3584\n",
      "step 28970: train loss 2.3418, val loss 2.4242\n",
      "step 28980: train loss 2.3937, val loss 2.3724\n",
      "step 28990: train loss 2.5069, val loss 2.3159\n",
      "step 29000: train loss 2.3229, val loss 2.3401\n",
      "step 29010: train loss 2.3850, val loss 2.3500\n",
      "step 29020: train loss 2.4119, val loss 2.3862\n",
      "step 29030: train loss 2.4279, val loss 2.4442\n",
      "step 29040: train loss 2.3814, val loss 2.4140\n",
      "step 29050: train loss 2.4343, val loss 2.4234\n",
      "step 29060: train loss 2.4288, val loss 2.5029\n",
      "step 29070: train loss 2.4644, val loss 2.4503\n",
      "step 29080: train loss 2.3271, val loss 2.3870\n",
      "step 29090: train loss 2.3889, val loss 2.3421\n",
      "step 29100: train loss 2.3477, val loss 2.3986\n",
      "step 29110: train loss 2.4049, val loss 2.4207\n",
      "step 29120: train loss 2.3981, val loss 2.3000\n",
      "step 29130: train loss 2.4423, val loss 2.3999\n",
      "step 29140: train loss 2.4250, val loss 2.3354\n",
      "step 29150: train loss 2.3235, val loss 2.3756\n",
      "step 29160: train loss 2.3310, val loss 2.2613\n",
      "step 29170: train loss 2.4179, val loss 2.3753\n",
      "step 29180: train loss 2.3660, val loss 2.3987\n",
      "step 29190: train loss 2.4450, val loss 2.4051\n",
      "step 29200: train loss 2.4058, val loss 2.3598\n",
      "step 29210: train loss 2.3742, val loss 2.3933\n",
      "step 29220: train loss 2.3603, val loss 2.4015\n",
      "step 29230: train loss 2.3291, val loss 2.4046\n",
      "step 29240: train loss 2.3398, val loss 2.3701\n",
      "step 29250: train loss 2.4254, val loss 2.3993\n",
      "step 29260: train loss 2.3665, val loss 2.4018\n",
      "step 29270: train loss 2.4595, val loss 2.3441\n",
      "step 29280: train loss 2.4457, val loss 2.3754\n",
      "step 29290: train loss 2.3588, val loss 2.4164\n",
      "step 29300: train loss 2.4088, val loss 2.4081\n",
      "step 29310: train loss 2.3694, val loss 2.4478\n",
      "step 29320: train loss 2.3523, val loss 2.3851\n",
      "step 29330: train loss 2.3378, val loss 2.3778\n",
      "step 29340: train loss 2.4304, val loss 2.4157\n",
      "step 29350: train loss 2.3536, val loss 2.3188\n",
      "step 29360: train loss 2.3726, val loss 2.4183\n",
      "step 29370: train loss 2.4270, val loss 2.2888\n",
      "step 29380: train loss 2.3671, val loss 2.3389\n",
      "step 29390: train loss 2.3871, val loss 2.3059\n",
      "step 29400: train loss 2.4387, val loss 2.3191\n",
      "step 29410: train loss 2.3919, val loss 2.3424\n",
      "step 29420: train loss 2.3450, val loss 2.3914\n",
      "step 29430: train loss 2.3945, val loss 2.3338\n",
      "step 29440: train loss 2.3692, val loss 2.3233\n",
      "step 29450: train loss 2.3437, val loss 2.3722\n",
      "step 29460: train loss 2.4700, val loss 2.4559\n",
      "step 29470: train loss 2.3897, val loss 2.3789\n",
      "step 29480: train loss 2.3347, val loss 2.2861\n",
      "step 29490: train loss 2.4179, val loss 2.3858\n",
      "step 29500: train loss 2.4017, val loss 2.3680\n",
      "step 29510: train loss 2.4359, val loss 2.3355\n",
      "step 29520: train loss 2.4123, val loss 2.3910\n",
      "step 29530: train loss 2.3742, val loss 2.4031\n",
      "step 29540: train loss 2.4357, val loss 2.3597\n",
      "step 29550: train loss 2.3287, val loss 2.3498\n",
      "step 29560: train loss 2.4364, val loss 2.3732\n",
      "step 29570: train loss 2.3743, val loss 2.4575\n",
      "step 29580: train loss 2.3913, val loss 2.3871\n",
      "step 29590: train loss 2.3557, val loss 2.3914\n",
      "step 29600: train loss 2.3724, val loss 2.4059\n",
      "step 29610: train loss 2.4420, val loss 2.3923\n",
      "step 29620: train loss 2.3915, val loss 2.4777\n",
      "step 29630: train loss 2.3975, val loss 2.3990\n",
      "step 29640: train loss 2.4113, val loss 2.3234\n",
      "step 29650: train loss 2.3097, val loss 2.3712\n",
      "step 29660: train loss 2.4024, val loss 2.3467\n",
      "step 29670: train loss 2.4248, val loss 2.3989\n",
      "step 29680: train loss 2.3989, val loss 2.3617\n",
      "step 29690: train loss 2.3409, val loss 2.3697\n",
      "step 29700: train loss 2.2881, val loss 2.3403\n",
      "step 29710: train loss 2.4129, val loss 2.3536\n",
      "step 29720: train loss 2.3604, val loss 2.3928\n",
      "step 29730: train loss 2.3530, val loss 2.3860\n",
      "step 29740: train loss 2.3865, val loss 2.4337\n",
      "step 29750: train loss 2.4218, val loss 2.4125\n",
      "step 29760: train loss 2.3936, val loss 2.3302\n",
      "step 29770: train loss 2.4651, val loss 2.3901\n",
      "step 29780: train loss 2.3435, val loss 2.3090\n",
      "step 29790: train loss 2.3717, val loss 2.3954\n",
      "step 29800: train loss 2.3795, val loss 2.3905\n",
      "step 29810: train loss 2.4189, val loss 2.3763\n",
      "step 29820: train loss 2.3194, val loss 2.3318\n",
      "step 29830: train loss 2.3890, val loss 2.3812\n",
      "step 29840: train loss 2.4078, val loss 2.3875\n",
      "step 29850: train loss 2.3682, val loss 2.4444\n",
      "step 29860: train loss 2.4201, val loss 2.3384\n",
      "step 29870: train loss 2.4174, val loss 2.3723\n",
      "step 29880: train loss 2.3461, val loss 2.3132\n",
      "step 29890: train loss 2.4334, val loss 2.3824\n",
      "step 29900: train loss 2.3473, val loss 2.3349\n",
      "step 29910: train loss 2.3885, val loss 2.4911\n",
      "step 29920: train loss 2.3352, val loss 2.4086\n",
      "step 29930: train loss 2.3700, val loss 2.4488\n",
      "step 29940: train loss 2.3944, val loss 2.3525\n",
      "step 29950: train loss 2.4219, val loss 2.3454\n",
      "step 29960: train loss 2.4083, val loss 2.4254\n",
      "step 29970: train loss 2.4326, val loss 2.4157\n",
      "step 29980: train loss 2.3085, val loss 2.3873\n",
      "step 29990: train loss 2.4414, val loss 2.3914\n",
      "step 30000: train loss 2.3356, val loss 2.3374\n",
      "step 30010: train loss 2.3705, val loss 2.3241\n",
      "step 30020: train loss 2.4148, val loss 2.4561\n",
      "step 30030: train loss 2.4530, val loss 2.3886\n",
      "step 30040: train loss 2.4295, val loss 2.3546\n",
      "step 30050: train loss 2.3715, val loss 2.3575\n",
      "step 30060: train loss 2.4102, val loss 2.3393\n",
      "step 30070: train loss 2.3501, val loss 2.3887\n",
      "step 30080: train loss 2.3987, val loss 2.3951\n",
      "step 30090: train loss 2.4395, val loss 2.4093\n",
      "step 30100: train loss 2.3444, val loss 2.2964\n",
      "step 30110: train loss 2.3688, val loss 2.3567\n",
      "step 30120: train loss 2.3551, val loss 2.3607\n",
      "step 30130: train loss 2.3558, val loss 2.3960\n",
      "step 30140: train loss 2.3886, val loss 2.3570\n",
      "step 30150: train loss 2.3775, val loss 2.3880\n",
      "step 30160: train loss 2.3416, val loss 2.3793\n",
      "step 30170: train loss 2.4077, val loss 2.3674\n",
      "step 30180: train loss 2.4130, val loss 2.3290\n",
      "step 30190: train loss 2.4183, val loss 2.3957\n",
      "step 30200: train loss 2.3938, val loss 2.4617\n",
      "step 30210: train loss 2.3957, val loss 2.4623\n",
      "step 30220: train loss 2.3329, val loss 2.3298\n",
      "step 30230: train loss 2.3395, val loss 2.3779\n",
      "step 30240: train loss 2.4043, val loss 2.3617\n",
      "step 30250: train loss 2.4333, val loss 2.4290\n",
      "step 30260: train loss 2.5349, val loss 2.4251\n",
      "step 30270: train loss 2.4335, val loss 2.4435\n",
      "step 30280: train loss 2.4018, val loss 2.4234\n",
      "step 30290: train loss 2.3972, val loss 2.3541\n",
      "step 30300: train loss 2.3829, val loss 2.3578\n",
      "step 30310: train loss 2.3658, val loss 2.4006\n",
      "step 30320: train loss 2.3807, val loss 2.4283\n",
      "step 30330: train loss 2.3986, val loss 2.3444\n",
      "step 30340: train loss 2.3926, val loss 2.4062\n",
      "step 30350: train loss 2.3319, val loss 2.4258\n",
      "step 30360: train loss 2.3409, val loss 2.3777\n",
      "step 30370: train loss 2.4383, val loss 2.3898\n",
      "step 30380: train loss 2.3790, val loss 2.2934\n",
      "step 30390: train loss 2.4370, val loss 2.3559\n",
      "step 30400: train loss 2.3766, val loss 2.4058\n",
      "step 30410: train loss 2.3577, val loss 2.4803\n",
      "step 30420: train loss 2.3882, val loss 2.4710\n",
      "step 30430: train loss 2.4473, val loss 2.3142\n",
      "step 30440: train loss 2.3591, val loss 2.3531\n",
      "step 30450: train loss 2.3644, val loss 2.3521\n",
      "step 30460: train loss 2.4063, val loss 2.2999\n",
      "step 30470: train loss 2.3380, val loss 2.3664\n",
      "step 30480: train loss 2.3221, val loss 2.4294\n",
      "step 30490: train loss 2.3743, val loss 2.3979\n",
      "step 30500: train loss 2.4438, val loss 2.3979\n",
      "step 30510: train loss 2.3078, val loss 2.2995\n",
      "step 30520: train loss 2.3834, val loss 2.3821\n",
      "step 30530: train loss 2.4453, val loss 2.3869\n",
      "step 30540: train loss 2.4037, val loss 2.3487\n",
      "step 30550: train loss 2.4176, val loss 2.3836\n",
      "step 30560: train loss 2.3888, val loss 2.4615\n",
      "step 30570: train loss 2.3094, val loss 2.3706\n",
      "step 30580: train loss 2.3323, val loss 2.4522\n",
      "step 30590: train loss 2.3752, val loss 2.3736\n",
      "step 30600: train loss 2.4163, val loss 2.3594\n",
      "step 30610: train loss 2.3520, val loss 2.4273\n",
      "step 30620: train loss 2.3776, val loss 2.4085\n",
      "step 30630: train loss 2.3985, val loss 2.3538\n",
      "step 30640: train loss 2.3359, val loss 2.3789\n",
      "step 30650: train loss 2.3889, val loss 2.4468\n",
      "step 30660: train loss 2.2574, val loss 2.3755\n",
      "step 30670: train loss 2.3112, val loss 2.3788\n",
      "step 30680: train loss 2.4017, val loss 2.4525\n",
      "step 30690: train loss 2.3773, val loss 2.3738\n",
      "step 30700: train loss 2.3915, val loss 2.3840\n",
      "step 30710: train loss 2.4158, val loss 2.3478\n",
      "step 30720: train loss 2.4037, val loss 2.2882\n",
      "step 30730: train loss 2.3790, val loss 2.3767\n",
      "step 30740: train loss 2.4149, val loss 2.4441\n",
      "step 30750: train loss 2.4126, val loss 2.4155\n",
      "step 30760: train loss 2.4172, val loss 2.4297\n",
      "step 30770: train loss 2.3538, val loss 2.3780\n",
      "step 30780: train loss 2.3956, val loss 2.2816\n",
      "step 30790: train loss 2.3482, val loss 2.4462\n",
      "step 30800: train loss 2.3941, val loss 2.2856\n",
      "step 30810: train loss 2.3622, val loss 2.3941\n",
      "step 30820: train loss 2.3443, val loss 2.3090\n",
      "step 30830: train loss 2.3869, val loss 2.4147\n",
      "step 30840: train loss 2.3907, val loss 2.4206\n",
      "step 30850: train loss 2.3628, val loss 2.3574\n",
      "step 30860: train loss 2.3047, val loss 2.3684\n",
      "step 30870: train loss 2.4047, val loss 2.3277\n",
      "step 30880: train loss 2.3784, val loss 2.3620\n",
      "step 30890: train loss 2.3806, val loss 2.4158\n",
      "step 30900: train loss 2.4094, val loss 2.4051\n",
      "step 30910: train loss 2.3544, val loss 2.3208\n",
      "step 30920: train loss 2.3572, val loss 2.4449\n",
      "step 30930: train loss 2.4162, val loss 2.3939\n",
      "step 30940: train loss 2.3219, val loss 2.5088\n",
      "step 30950: train loss 2.4191, val loss 2.3634\n",
      "step 30960: train loss 2.3547, val loss 2.4178\n",
      "step 30970: train loss 2.3603, val loss 2.4438\n",
      "step 30980: train loss 2.4058, val loss 2.3362\n",
      "step 30990: train loss 2.3365, val loss 2.3292\n",
      "step 31000: train loss 2.3799, val loss 2.4372\n",
      "step 31010: train loss 2.3913, val loss 2.3217\n",
      "step 31020: train loss 2.3832, val loss 2.4009\n",
      "step 31030: train loss 2.4318, val loss 2.4380\n",
      "step 31040: train loss 2.2743, val loss 2.3814\n",
      "step 31050: train loss 2.3704, val loss 2.4105\n",
      "step 31060: train loss 2.3519, val loss 2.3762\n",
      "step 31070: train loss 2.4121, val loss 2.3855\n",
      "step 31080: train loss 2.4226, val loss 2.3762\n",
      "step 31090: train loss 2.3085, val loss 2.3862\n",
      "step 31100: train loss 2.3394, val loss 2.3311\n",
      "step 31110: train loss 2.4457, val loss 2.3248\n",
      "step 31120: train loss 2.3416, val loss 2.3539\n",
      "step 31130: train loss 2.4396, val loss 2.4016\n",
      "step 31140: train loss 2.3670, val loss 2.3613\n",
      "step 31150: train loss 2.3922, val loss 2.3634\n",
      "step 31160: train loss 2.3579, val loss 2.3908\n",
      "step 31170: train loss 2.4187, val loss 2.3572\n",
      "step 31180: train loss 2.2902, val loss 2.4443\n",
      "step 31190: train loss 2.3474, val loss 2.3089\n",
      "step 31200: train loss 2.3714, val loss 2.4119\n",
      "step 31210: train loss 2.4061, val loss 2.4157\n",
      "step 31220: train loss 2.3648, val loss 2.4900\n",
      "step 31230: train loss 2.4414, val loss 2.3859\n",
      "step 31240: train loss 2.3818, val loss 2.3099\n",
      "step 31250: train loss 2.3975, val loss 2.3552\n",
      "step 31260: train loss 2.3106, val loss 2.3799\n",
      "step 31270: train loss 2.3630, val loss 2.3574\n",
      "step 31280: train loss 2.4366, val loss 2.3839\n",
      "step 31290: train loss 2.4585, val loss 2.3332\n",
      "step 31300: train loss 2.3923, val loss 2.4060\n",
      "step 31310: train loss 2.3237, val loss 2.3680\n",
      "step 31320: train loss 2.3632, val loss 2.4030\n",
      "step 31330: train loss 2.4097, val loss 2.3402\n",
      "step 31340: train loss 2.3307, val loss 2.3934\n",
      "step 31350: train loss 2.3116, val loss 2.3923\n",
      "step 31360: train loss 2.3770, val loss 2.3530\n",
      "step 31370: train loss 2.4131, val loss 2.4300\n",
      "step 31380: train loss 2.4012, val loss 2.4173\n",
      "step 31390: train loss 2.3776, val loss 2.3176\n",
      "step 31400: train loss 2.4095, val loss 2.4632\n",
      "step 31410: train loss 2.3975, val loss 2.3357\n",
      "step 31420: train loss 2.3690, val loss 2.3891\n",
      "step 31430: train loss 2.3499, val loss 2.4248\n",
      "step 31440: train loss 2.4358, val loss 2.3512\n",
      "step 31450: train loss 2.4189, val loss 2.4174\n",
      "step 31460: train loss 2.2981, val loss 2.4259\n",
      "step 31470: train loss 2.3999, val loss 2.3540\n",
      "step 31480: train loss 2.4086, val loss 2.3197\n",
      "step 31490: train loss 2.4038, val loss 2.3165\n",
      "step 31500: train loss 2.3765, val loss 2.3783\n",
      "step 31510: train loss 2.2736, val loss 2.4353\n",
      "step 31520: train loss 2.3821, val loss 2.3965\n",
      "step 31530: train loss 2.4458, val loss 2.3593\n",
      "step 31540: train loss 2.3453, val loss 2.3941\n",
      "step 31550: train loss 2.3777, val loss 2.3934\n",
      "step 31560: train loss 2.3942, val loss 2.3089\n",
      "step 31570: train loss 2.3530, val loss 2.3873\n",
      "step 31580: train loss 2.3569, val loss 2.3346\n",
      "step 31590: train loss 2.3869, val loss 2.3447\n",
      "step 31600: train loss 2.3793, val loss 2.3837\n",
      "step 31610: train loss 2.3337, val loss 2.2983\n",
      "step 31620: train loss 2.3611, val loss 2.3494\n",
      "step 31630: train loss 2.4318, val loss 2.3386\n",
      "step 31640: train loss 2.3805, val loss 2.3806\n",
      "step 31650: train loss 2.3498, val loss 2.3371\n",
      "step 31660: train loss 2.3228, val loss 2.3590\n",
      "step 31670: train loss 2.3931, val loss 2.3126\n",
      "step 31680: train loss 2.3894, val loss 2.3596\n",
      "step 31690: train loss 2.2805, val loss 2.3791\n",
      "step 31700: train loss 2.3746, val loss 2.4418\n",
      "step 31710: train loss 2.3712, val loss 2.2995\n",
      "step 31720: train loss 2.3395, val loss 2.4089\n",
      "step 31730: train loss 2.4165, val loss 2.3593\n",
      "step 31740: train loss 2.3983, val loss 2.4082\n",
      "step 31750: train loss 2.3965, val loss 2.5006\n",
      "step 31760: train loss 2.3615, val loss 2.3625\n",
      "step 31770: train loss 2.4176, val loss 2.3990\n",
      "step 31780: train loss 2.3366, val loss 2.3315\n",
      "step 31790: train loss 2.3504, val loss 2.3749\n",
      "step 31800: train loss 2.3665, val loss 2.4214\n",
      "step 31810: train loss 2.3980, val loss 2.3677\n",
      "step 31820: train loss 2.4208, val loss 2.3302\n",
      "step 31830: train loss 2.3770, val loss 2.3916\n",
      "step 31840: train loss 2.3679, val loss 2.4344\n",
      "step 31850: train loss 2.3771, val loss 2.3802\n",
      "step 31860: train loss 2.3792, val loss 2.2702\n",
      "step 31870: train loss 2.4075, val loss 2.3507\n",
      "step 31880: train loss 2.2493, val loss 2.3792\n",
      "step 31890: train loss 2.3985, val loss 2.3229\n",
      "step 31900: train loss 2.3526, val loss 2.3072\n",
      "step 31910: train loss 2.3811, val loss 2.4474\n",
      "step 31920: train loss 2.3476, val loss 2.3225\n",
      "step 31930: train loss 2.3850, val loss 2.4206\n",
      "step 31940: train loss 2.3703, val loss 2.4492\n",
      "step 31950: train loss 2.4673, val loss 2.3032\n",
      "step 31960: train loss 2.3758, val loss 2.3880\n",
      "step 31970: train loss 2.3983, val loss 2.3953\n",
      "step 31980: train loss 2.3094, val loss 2.4056\n",
      "step 31990: train loss 2.4544, val loss 2.5239\n",
      "step 32000: train loss 2.2381, val loss 2.3730\n",
      "step 32010: train loss 2.3070, val loss 2.3623\n",
      "step 32020: train loss 2.3350, val loss 2.4111\n",
      "step 32030: train loss 2.4157, val loss 2.3378\n",
      "step 32040: train loss 2.3429, val loss 2.4105\n",
      "step 32050: train loss 2.3632, val loss 2.3941\n",
      "step 32060: train loss 2.3932, val loss 2.3695\n",
      "step 32070: train loss 2.3894, val loss 2.3533\n",
      "step 32080: train loss 2.3858, val loss 2.4267\n",
      "step 32090: train loss 2.3167, val loss 2.3014\n",
      "step 32100: train loss 2.3603, val loss 2.4255\n",
      "step 32110: train loss 2.4081, val loss 2.3568\n",
      "step 32120: train loss 2.4013, val loss 2.3741\n",
      "step 32130: train loss 2.3623, val loss 2.3238\n",
      "step 32140: train loss 2.3945, val loss 2.3445\n",
      "step 32150: train loss 2.4334, val loss 2.2995\n",
      "step 32160: train loss 2.3067, val loss 2.3955\n",
      "step 32170: train loss 2.3886, val loss 2.4108\n",
      "step 32180: train loss 2.3432, val loss 2.3170\n",
      "step 32190: train loss 2.3809, val loss 2.3767\n",
      "step 32200: train loss 2.3515, val loss 2.3456\n",
      "step 32210: train loss 2.4231, val loss 2.3739\n",
      "step 32220: train loss 2.3668, val loss 2.3201\n",
      "step 32230: train loss 2.4190, val loss 2.4438\n",
      "step 32240: train loss 2.3408, val loss 2.3181\n",
      "step 32250: train loss 2.3596, val loss 2.3646\n",
      "step 32260: train loss 2.4262, val loss 2.2988\n",
      "step 32270: train loss 2.4256, val loss 2.3967\n",
      "step 32280: train loss 2.4167, val loss 2.3445\n",
      "step 32290: train loss 2.3573, val loss 2.3874\n",
      "step 32300: train loss 2.4109, val loss 2.4660\n",
      "step 32310: train loss 2.3943, val loss 2.3907\n",
      "step 32320: train loss 2.3387, val loss 2.3742\n",
      "step 32330: train loss 2.3970, val loss 2.3362\n",
      "step 32340: train loss 2.3593, val loss 2.3884\n",
      "step 32350: train loss 2.4700, val loss 2.3577\n",
      "step 32360: train loss 2.3949, val loss 2.3410\n",
      "step 32370: train loss 2.4180, val loss 2.3734\n",
      "step 32380: train loss 2.4327, val loss 2.3578\n",
      "step 32390: train loss 2.4309, val loss 2.3661\n",
      "step 32400: train loss 2.3244, val loss 2.3722\n",
      "step 32410: train loss 2.3666, val loss 2.4377\n",
      "step 32420: train loss 2.3741, val loss 2.4348\n",
      "step 32430: train loss 2.3922, val loss 2.3407\n",
      "step 32440: train loss 2.4549, val loss 2.3566\n",
      "step 32450: train loss 2.4377, val loss 2.2987\n",
      "step 32460: train loss 2.3842, val loss 2.3658\n",
      "step 32470: train loss 2.4794, val loss 2.4287\n",
      "step 32480: train loss 2.3745, val loss 2.2916\n",
      "step 32490: train loss 2.3916, val loss 2.3659\n",
      "step 32500: train loss 2.3785, val loss 2.3314\n",
      "step 32510: train loss 2.4101, val loss 2.3697\n",
      "step 32520: train loss 2.4119, val loss 2.3735\n",
      "step 32530: train loss 2.3451, val loss 2.3549\n",
      "step 32540: train loss 2.4030, val loss 2.3824\n",
      "step 32550: train loss 2.3645, val loss 2.3331\n",
      "step 32560: train loss 2.3958, val loss 2.3872\n",
      "step 32570: train loss 2.3677, val loss 2.4636\n",
      "step 32580: train loss 2.3720, val loss 2.3942\n",
      "step 32590: train loss 2.3926, val loss 2.2674\n",
      "step 32600: train loss 2.3776, val loss 2.3268\n",
      "step 32610: train loss 2.3692, val loss 2.3429\n",
      "step 32620: train loss 2.4765, val loss 2.4058\n",
      "step 32630: train loss 2.4392, val loss 2.3361\n",
      "step 32640: train loss 2.3916, val loss 2.3711\n",
      "step 32650: train loss 2.3213, val loss 2.3475\n",
      "step 32660: train loss 2.3841, val loss 2.3128\n",
      "step 32670: train loss 2.3520, val loss 2.3780\n",
      "step 32680: train loss 2.3575, val loss 2.3685\n",
      "step 32690: train loss 2.3511, val loss 2.2721\n",
      "step 32700: train loss 2.3428, val loss 2.3978\n",
      "step 32710: train loss 2.4309, val loss 2.3770\n",
      "step 32720: train loss 2.3873, val loss 2.4731\n",
      "step 32730: train loss 2.4088, val loss 2.4363\n",
      "step 32740: train loss 2.3750, val loss 2.3690\n",
      "step 32750: train loss 2.3170, val loss 2.3756\n",
      "step 32760: train loss 2.2874, val loss 2.3743\n",
      "step 32770: train loss 2.3794, val loss 2.3560\n",
      "step 32780: train loss 2.3754, val loss 2.3954\n",
      "step 32790: train loss 2.3308, val loss 2.3431\n",
      "step 32800: train loss 2.3788, val loss 2.3924\n",
      "step 32810: train loss 2.3867, val loss 2.4116\n",
      "step 32820: train loss 2.3425, val loss 2.3753\n",
      "step 32830: train loss 2.3586, val loss 2.3473\n",
      "step 32840: train loss 2.3165, val loss 2.3020\n",
      "step 32850: train loss 2.3902, val loss 2.3947\n",
      "step 32860: train loss 2.3709, val loss 2.2886\n",
      "step 32870: train loss 2.3733, val loss 2.3255\n",
      "step 32880: train loss 2.4114, val loss 2.4197\n",
      "step 32890: train loss 2.3762, val loss 2.2761\n",
      "step 32900: train loss 2.3883, val loss 2.3259\n",
      "step 32910: train loss 2.3834, val loss 2.3491\n",
      "step 32920: train loss 2.4381, val loss 2.3419\n",
      "step 32930: train loss 2.3784, val loss 2.3767\n",
      "step 32940: train loss 2.3202, val loss 2.4135\n",
      "step 32950: train loss 2.3378, val loss 2.3391\n",
      "step 32960: train loss 2.4429, val loss 2.3371\n",
      "step 32970: train loss 2.3800, val loss 2.3917\n",
      "step 32980: train loss 2.3457, val loss 2.3680\n",
      "step 32990: train loss 2.3479, val loss 2.3473\n",
      "step 33000: train loss 2.4163, val loss 2.3741\n",
      "step 33010: train loss 2.3509, val loss 2.3843\n",
      "step 33020: train loss 2.4943, val loss 2.4004\n",
      "step 33030: train loss 2.4086, val loss 2.3959\n",
      "step 33040: train loss 2.3567, val loss 2.4599\n",
      "step 33050: train loss 2.3522, val loss 2.3774\n",
      "step 33060: train loss 2.4340, val loss 2.3529\n",
      "step 33070: train loss 2.3622, val loss 2.4160\n",
      "step 33080: train loss 2.3237, val loss 2.3639\n",
      "step 33090: train loss 2.4287, val loss 2.4119\n",
      "step 33100: train loss 2.4083, val loss 2.4240\n",
      "step 33110: train loss 2.4260, val loss 2.3342\n",
      "step 33120: train loss 2.4085, val loss 2.3457\n",
      "step 33130: train loss 2.3733, val loss 2.3436\n",
      "step 33140: train loss 2.3891, val loss 2.3089\n",
      "step 33150: train loss 2.3562, val loss 2.3809\n",
      "step 33160: train loss 2.3267, val loss 2.3509\n",
      "step 33170: train loss 2.4097, val loss 2.3760\n",
      "step 33180: train loss 2.3357, val loss 2.3267\n",
      "step 33190: train loss 2.3985, val loss 2.3481\n",
      "step 33200: train loss 2.3841, val loss 2.3879\n",
      "step 33210: train loss 2.3269, val loss 2.3495\n",
      "step 33220: train loss 2.3292, val loss 2.2736\n",
      "step 33230: train loss 2.3841, val loss 2.3692\n",
      "step 33240: train loss 2.3848, val loss 2.3565\n",
      "step 33250: train loss 2.4637, val loss 2.3610\n",
      "step 33260: train loss 2.4515, val loss 2.3970\n",
      "step 33270: train loss 2.4370, val loss 2.3376\n",
      "step 33280: train loss 2.4193, val loss 2.3524\n",
      "step 33290: train loss 2.3392, val loss 2.3366\n",
      "step 33300: train loss 2.3518, val loss 2.3327\n",
      "step 33310: train loss 2.3090, val loss 2.4053\n",
      "step 33320: train loss 2.4193, val loss 2.3315\n",
      "step 33330: train loss 2.3486, val loss 2.3881\n",
      "step 33340: train loss 2.3912, val loss 2.4000\n",
      "step 33350: train loss 2.3303, val loss 2.3422\n",
      "step 33360: train loss 2.3526, val loss 2.3933\n",
      "step 33370: train loss 2.3446, val loss 2.3997\n",
      "step 33380: train loss 2.4501, val loss 2.3786\n",
      "step 33390: train loss 2.3667, val loss 2.4687\n",
      "step 33400: train loss 2.3906, val loss 2.3830\n",
      "step 33410: train loss 2.2464, val loss 2.3335\n",
      "step 33420: train loss 2.4192, val loss 2.3347\n",
      "step 33430: train loss 2.4585, val loss 2.3535\n",
      "step 33440: train loss 2.3221, val loss 2.4251\n",
      "step 33450: train loss 2.3640, val loss 2.3237\n",
      "step 33460: train loss 2.4974, val loss 2.4237\n",
      "step 33470: train loss 2.2952, val loss 2.3945\n",
      "step 33480: train loss 2.3851, val loss 2.4063\n",
      "step 33490: train loss 2.3636, val loss 2.4435\n",
      "step 33500: train loss 2.4019, val loss 2.3277\n",
      "step 33510: train loss 2.4765, val loss 2.3945\n",
      "step 33520: train loss 2.3702, val loss 2.3622\n",
      "step 33530: train loss 2.3971, val loss 2.3934\n",
      "step 33540: train loss 2.3844, val loss 2.4433\n",
      "step 33550: train loss 2.3342, val loss 2.3773\n",
      "step 33560: train loss 2.3703, val loss 2.4197\n",
      "step 33570: train loss 2.4051, val loss 2.3028\n",
      "step 33580: train loss 2.4434, val loss 2.3635\n",
      "step 33590: train loss 2.4613, val loss 2.4072\n",
      "step 33600: train loss 2.3994, val loss 2.3843\n",
      "step 33610: train loss 2.4196, val loss 2.3608\n",
      "step 33620: train loss 2.3432, val loss 2.4164\n",
      "step 33630: train loss 2.3288, val loss 2.3580\n",
      "step 33640: train loss 2.3700, val loss 2.4536\n",
      "step 33650: train loss 2.3910, val loss 2.3549\n",
      "step 33660: train loss 2.3488, val loss 2.4570\n",
      "step 33670: train loss 2.4567, val loss 2.4474\n",
      "step 33680: train loss 2.4105, val loss 2.3915\n",
      "step 33690: train loss 2.3520, val loss 2.3887\n",
      "step 33700: train loss 2.3721, val loss 2.2967\n",
      "step 33710: train loss 2.3713, val loss 2.3508\n",
      "step 33720: train loss 2.3521, val loss 2.3248\n",
      "step 33730: train loss 2.3860, val loss 2.4315\n",
      "step 33740: train loss 2.4003, val loss 2.3282\n",
      "step 33750: train loss 2.3627, val loss 2.3936\n",
      "step 33760: train loss 2.4500, val loss 2.3804\n",
      "step 33770: train loss 2.4265, val loss 2.4143\n",
      "step 33780: train loss 2.3526, val loss 2.3475\n",
      "step 33790: train loss 2.3208, val loss 2.3548\n",
      "step 33800: train loss 2.3155, val loss 2.4022\n",
      "step 33810: train loss 2.3848, val loss 2.3170\n",
      "step 33820: train loss 2.4422, val loss 2.3729\n",
      "step 33830: train loss 2.3490, val loss 2.4109\n",
      "step 33840: train loss 2.3428, val loss 2.3322\n",
      "step 33850: train loss 2.3448, val loss 2.4059\n",
      "step 33860: train loss 2.3573, val loss 2.3393\n",
      "step 33870: train loss 2.4481, val loss 2.3924\n",
      "step 33880: train loss 2.3886, val loss 2.3734\n",
      "step 33890: train loss 2.4651, val loss 2.3306\n",
      "step 33900: train loss 2.3333, val loss 2.4039\n",
      "step 33910: train loss 2.4252, val loss 2.3663\n",
      "step 33920: train loss 2.3757, val loss 2.3874\n",
      "step 33930: train loss 2.4492, val loss 2.4101\n",
      "step 33940: train loss 2.3985, val loss 2.3394\n",
      "step 33950: train loss 2.3743, val loss 2.3550\n",
      "step 33960: train loss 2.3820, val loss 2.3187\n",
      "step 33970: train loss 2.3611, val loss 2.2975\n",
      "step 33980: train loss 2.3950, val loss 2.3657\n",
      "step 33990: train loss 2.4189, val loss 2.4074\n",
      "step 34000: train loss 2.3941, val loss 2.3267\n",
      "step 34010: train loss 2.3457, val loss 2.4217\n",
      "step 34020: train loss 2.3930, val loss 2.2990\n",
      "step 34030: train loss 2.3848, val loss 2.4022\n",
      "step 34040: train loss 2.2930, val loss 2.3846\n",
      "step 34050: train loss 2.3297, val loss 2.3830\n",
      "step 34060: train loss 2.3323, val loss 2.4814\n",
      "step 34070: train loss 2.3790, val loss 2.3906\n",
      "step 34080: train loss 2.3683, val loss 2.4231\n",
      "step 34090: train loss 2.3939, val loss 2.3497\n",
      "step 34100: train loss 2.2950, val loss 2.3775\n",
      "step 34110: train loss 2.3269, val loss 2.3397\n",
      "step 34120: train loss 2.3859, val loss 2.4093\n",
      "step 34130: train loss 2.3163, val loss 2.4461\n",
      "step 34140: train loss 2.3101, val loss 2.3368\n",
      "step 34150: train loss 2.4346, val loss 2.3320\n",
      "step 34160: train loss 2.3542, val loss 2.3550\n",
      "step 34170: train loss 2.4619, val loss 2.3664\n",
      "step 34180: train loss 2.3432, val loss 2.3644\n",
      "step 34190: train loss 2.3840, val loss 2.3748\n",
      "step 34200: train loss 2.3673, val loss 2.3235\n",
      "step 34210: train loss 2.3397, val loss 2.3486\n",
      "step 34220: train loss 2.3843, val loss 2.3967\n",
      "step 34230: train loss 2.3228, val loss 2.3713\n",
      "step 34240: train loss 2.4385, val loss 2.2659\n",
      "step 34250: train loss 2.4098, val loss 2.4976\n",
      "step 34260: train loss 2.4589, val loss 2.3902\n",
      "step 34270: train loss 2.4137, val loss 2.3612\n",
      "step 34280: train loss 2.3717, val loss 2.3521\n",
      "step 34290: train loss 2.4234, val loss 2.3789\n",
      "step 34300: train loss 2.4027, val loss 2.3001\n",
      "step 34310: train loss 2.3687, val loss 2.3626\n",
      "step 34320: train loss 2.4075, val loss 2.3116\n",
      "step 34330: train loss 2.3036, val loss 2.3892\n",
      "step 34340: train loss 2.3207, val loss 2.2903\n",
      "step 34350: train loss 2.3464, val loss 2.3969\n",
      "step 34360: train loss 2.3679, val loss 2.3145\n",
      "step 34370: train loss 2.4040, val loss 2.3077\n",
      "step 34380: train loss 2.4694, val loss 2.3763\n",
      "step 34390: train loss 2.4481, val loss 2.3054\n",
      "step 34400: train loss 2.4622, val loss 2.3889\n",
      "step 34410: train loss 2.3675, val loss 2.4123\n",
      "step 34420: train loss 2.3464, val loss 2.3793\n",
      "step 34430: train loss 2.3531, val loss 2.4122\n",
      "step 34440: train loss 2.4346, val loss 2.4179\n",
      "step 34450: train loss 2.5104, val loss 2.4324\n",
      "step 34460: train loss 2.4016, val loss 2.4098\n",
      "step 34470: train loss 2.3957, val loss 2.3926\n",
      "step 34480: train loss 2.3269, val loss 2.3696\n",
      "step 34490: train loss 2.3076, val loss 2.3938\n",
      "step 34500: train loss 2.3991, val loss 2.3467\n",
      "step 34510: train loss 2.3916, val loss 2.3380\n",
      "step 34520: train loss 2.3070, val loss 2.3956\n",
      "step 34530: train loss 2.3660, val loss 2.4436\n",
      "step 34540: train loss 2.3664, val loss 2.3411\n",
      "step 34550: train loss 2.3519, val loss 2.3375\n",
      "step 34560: train loss 2.3550, val loss 2.3456\n",
      "step 34570: train loss 2.3845, val loss 2.3944\n",
      "step 34580: train loss 2.3993, val loss 2.3469\n",
      "step 34590: train loss 2.4229, val loss 2.3026\n",
      "step 34600: train loss 2.4203, val loss 2.3354\n",
      "step 34610: train loss 2.3906, val loss 2.4593\n",
      "step 34620: train loss 2.4766, val loss 2.4330\n",
      "step 34630: train loss 2.4400, val loss 2.4076\n",
      "step 34640: train loss 2.3900, val loss 2.4208\n",
      "step 34650: train loss 2.3999, val loss 2.3650\n",
      "step 34660: train loss 2.3956, val loss 2.3686\n",
      "step 34670: train loss 2.4160, val loss 2.3796\n",
      "step 34680: train loss 2.3038, val loss 2.4208\n",
      "step 34690: train loss 2.3353, val loss 2.3241\n",
      "step 34700: train loss 2.3241, val loss 2.3784\n",
      "step 34710: train loss 2.3835, val loss 2.3688\n",
      "step 34720: train loss 2.3307, val loss 2.4374\n",
      "step 34730: train loss 2.3898, val loss 2.3914\n",
      "step 34740: train loss 2.4866, val loss 2.3401\n",
      "step 34750: train loss 2.3362, val loss 2.3809\n",
      "step 34760: train loss 2.3399, val loss 2.3300\n",
      "step 34770: train loss 2.4116, val loss 2.3749\n",
      "step 34780: train loss 2.4004, val loss 2.4522\n",
      "step 34790: train loss 2.3721, val loss 2.3131\n",
      "step 34800: train loss 2.3818, val loss 2.3526\n",
      "step 34810: train loss 2.3806, val loss 2.3130\n",
      "step 34820: train loss 2.3848, val loss 2.3997\n",
      "step 34830: train loss 2.4512, val loss 2.3834\n",
      "step 34840: train loss 2.3842, val loss 2.3930\n",
      "step 34850: train loss 2.4368, val loss 2.4064\n",
      "step 34860: train loss 2.3482, val loss 2.4148\n",
      "step 34870: train loss 2.4072, val loss 2.3727\n",
      "step 34880: train loss 2.3898, val loss 2.3363\n",
      "step 34890: train loss 2.3824, val loss 2.3476\n",
      "step 34900: train loss 2.3733, val loss 2.3559\n",
      "step 34910: train loss 2.3930, val loss 2.3368\n",
      "step 34920: train loss 2.4090, val loss 2.3337\n",
      "step 34930: train loss 2.2832, val loss 2.3383\n",
      "step 34940: train loss 2.4107, val loss 2.3219\n",
      "step 34950: train loss 2.3976, val loss 2.4538\n",
      "step 34960: train loss 2.4050, val loss 2.3625\n",
      "step 34970: train loss 2.3022, val loss 2.3690\n",
      "step 34980: train loss 2.3002, val loss 2.3715\n",
      "step 34990: train loss 2.4105, val loss 2.3560\n",
      "step 35000: train loss 2.4021, val loss 2.4084\n",
      "step 35010: train loss 2.4065, val loss 2.3348\n",
      "step 35020: train loss 2.3431, val loss 2.4269\n",
      "step 35030: train loss 2.4292, val loss 2.3542\n",
      "step 35040: train loss 2.3791, val loss 2.3487\n",
      "step 35050: train loss 2.3545, val loss 2.4512\n",
      "step 35060: train loss 2.3546, val loss 2.3628\n",
      "step 35070: train loss 2.3340, val loss 2.3669\n",
      "step 35080: train loss 2.3838, val loss 2.3341\n",
      "step 35090: train loss 2.4508, val loss 2.4325\n",
      "step 35100: train loss 2.4375, val loss 2.3179\n",
      "step 35110: train loss 2.3533, val loss 2.3996\n",
      "step 35120: train loss 2.3631, val loss 2.3146\n",
      "step 35130: train loss 2.3999, val loss 2.3266\n",
      "step 35140: train loss 2.3933, val loss 2.4041\n",
      "step 35150: train loss 2.4407, val loss 2.3236\n",
      "step 35160: train loss 2.3946, val loss 2.3331\n",
      "step 35170: train loss 2.3504, val loss 2.3128\n",
      "step 35180: train loss 2.4553, val loss 2.3330\n",
      "step 35190: train loss 2.3280, val loss 2.3797\n",
      "step 35200: train loss 2.4051, val loss 2.3609\n",
      "step 35210: train loss 2.3495, val loss 2.3267\n",
      "step 35220: train loss 2.3535, val loss 2.4467\n",
      "step 35230: train loss 2.3760, val loss 2.3190\n",
      "step 35240: train loss 2.3931, val loss 2.3507\n",
      "step 35250: train loss 2.4724, val loss 2.3263\n",
      "step 35260: train loss 2.3747, val loss 2.3430\n",
      "step 35270: train loss 2.3632, val loss 2.3627\n",
      "step 35280: train loss 2.3839, val loss 2.4039\n",
      "step 35290: train loss 2.3561, val loss 2.3190\n",
      "step 35300: train loss 2.3783, val loss 2.3753\n",
      "step 35310: train loss 2.4102, val loss 2.3881\n",
      "step 35320: train loss 2.3548, val loss 2.3349\n",
      "step 35330: train loss 2.3146, val loss 2.4231\n",
      "step 35340: train loss 2.3738, val loss 2.3585\n",
      "step 35350: train loss 2.4365, val loss 2.4485\n",
      "step 35360: train loss 2.3646, val loss 2.3962\n",
      "step 35370: train loss 2.4323, val loss 2.3396\n",
      "step 35380: train loss 2.3899, val loss 2.3864\n",
      "step 35390: train loss 2.3178, val loss 2.3268\n",
      "step 35400: train loss 2.3400, val loss 2.3903\n",
      "step 35410: train loss 2.4202, val loss 2.3203\n",
      "step 35420: train loss 2.4385, val loss 2.4252\n",
      "step 35430: train loss 2.3390, val loss 2.4057\n",
      "step 35440: train loss 2.3737, val loss 2.3403\n",
      "step 35450: train loss 2.4163, val loss 2.3180\n",
      "step 35460: train loss 2.4170, val loss 2.4027\n",
      "step 35470: train loss 2.3847, val loss 2.3665\n",
      "step 35480: train loss 2.4027, val loss 2.3740\n",
      "step 35490: train loss 2.4007, val loss 2.3777\n",
      "step 35500: train loss 2.4168, val loss 2.3290\n",
      "step 35510: train loss 2.3184, val loss 2.3583\n",
      "step 35520: train loss 2.4355, val loss 2.3853\n",
      "step 35530: train loss 2.3755, val loss 2.3705\n",
      "step 35540: train loss 2.3801, val loss 2.3725\n",
      "step 35550: train loss 2.3541, val loss 2.3846\n",
      "step 35560: train loss 2.4414, val loss 2.2923\n",
      "step 35570: train loss 2.3721, val loss 2.2987\n",
      "step 35580: train loss 2.4066, val loss 2.3789\n",
      "step 35590: train loss 2.3560, val loss 2.3845\n",
      "step 35600: train loss 2.3940, val loss 2.2736\n",
      "step 35610: train loss 2.3964, val loss 2.3452\n",
      "step 35620: train loss 2.4694, val loss 2.4547\n",
      "step 35630: train loss 2.4178, val loss 2.3528\n",
      "step 35640: train loss 2.3505, val loss 2.3592\n",
      "step 35650: train loss 2.3948, val loss 2.3265\n",
      "step 35660: train loss 2.4136, val loss 2.3474\n",
      "step 35670: train loss 2.3338, val loss 2.3582\n",
      "step 35680: train loss 2.3521, val loss 2.3126\n",
      "step 35690: train loss 2.3786, val loss 2.2549\n",
      "step 35700: train loss 2.3443, val loss 2.3597\n",
      "step 35710: train loss 2.4564, val loss 2.3541\n",
      "step 35720: train loss 2.3254, val loss 2.4096\n",
      "step 35730: train loss 2.4340, val loss 2.3909\n",
      "step 35740: train loss 2.3550, val loss 2.3663\n",
      "step 35750: train loss 2.3358, val loss 2.3489\n",
      "step 35760: train loss 2.3709, val loss 2.4242\n",
      "step 35770: train loss 2.4481, val loss 2.3394\n",
      "step 35780: train loss 2.2976, val loss 2.3247\n",
      "step 35790: train loss 2.3144, val loss 2.3572\n",
      "step 35800: train loss 2.3255, val loss 2.3588\n",
      "step 35810: train loss 2.3766, val loss 2.3837\n",
      "step 35820: train loss 2.4289, val loss 2.3503\n",
      "step 35830: train loss 2.4251, val loss 2.3282\n",
      "step 35840: train loss 2.3640, val loss 2.3545\n",
      "step 35850: train loss 2.3384, val loss 2.3155\n",
      "step 35860: train loss 2.3761, val loss 2.3493\n",
      "step 35870: train loss 2.4475, val loss 2.4208\n",
      "step 35880: train loss 2.4565, val loss 2.4122\n",
      "step 35890: train loss 2.4181, val loss 2.4083\n",
      "step 35900: train loss 2.3025, val loss 2.3639\n",
      "step 35910: train loss 2.3862, val loss 2.4007\n",
      "step 35920: train loss 2.3215, val loss 2.3122\n",
      "step 35930: train loss 2.4062, val loss 2.2990\n",
      "step 35940: train loss 2.3782, val loss 2.3742\n",
      "step 35950: train loss 2.3892, val loss 2.3905\n",
      "step 35960: train loss 2.4225, val loss 2.3819\n",
      "step 35970: train loss 2.3406, val loss 2.3776\n",
      "step 35980: train loss 2.4419, val loss 2.3865\n",
      "step 35990: train loss 2.4000, val loss 2.3735\n",
      "step 36000: train loss 2.4743, val loss 2.3471\n",
      "step 36010: train loss 2.4600, val loss 2.4197\n",
      "step 36020: train loss 2.3530, val loss 2.4407\n",
      "step 36030: train loss 2.3885, val loss 2.4536\n",
      "step 36040: train loss 2.3749, val loss 2.3993\n",
      "step 36050: train loss 2.4578, val loss 2.3857\n",
      "step 36060: train loss 2.3850, val loss 2.3575\n",
      "step 36070: train loss 2.3716, val loss 2.3412\n",
      "step 36080: train loss 2.3698, val loss 2.3691\n",
      "step 36090: train loss 2.3459, val loss 2.3226\n",
      "step 36100: train loss 2.3601, val loss 2.4285\n",
      "step 36110: train loss 2.3274, val loss 2.3615\n",
      "step 36120: train loss 2.3924, val loss 2.3790\n",
      "step 36130: train loss 2.3591, val loss 2.4498\n",
      "step 36140: train loss 2.4109, val loss 2.4361\n",
      "step 36150: train loss 2.3930, val loss 2.4316\n",
      "step 36160: train loss 2.4035, val loss 2.3770\n",
      "step 36170: train loss 2.3688, val loss 2.3711\n",
      "step 36180: train loss 2.3893, val loss 2.3681\n",
      "step 36190: train loss 2.3343, val loss 2.3528\n",
      "step 36200: train loss 2.4494, val loss 2.4327\n",
      "step 36210: train loss 2.3016, val loss 2.3923\n",
      "step 36220: train loss 2.3898, val loss 2.3799\n",
      "step 36230: train loss 2.3397, val loss 2.4328\n",
      "step 36240: train loss 2.3906, val loss 2.3439\n",
      "step 36250: train loss 2.3176, val loss 2.3951\n",
      "step 36260: train loss 2.3394, val loss 2.4636\n",
      "step 36270: train loss 2.3664, val loss 2.3749\n",
      "step 36280: train loss 2.3625, val loss 2.3510\n",
      "step 36290: train loss 2.3590, val loss 2.4063\n",
      "step 36300: train loss 2.3665, val loss 2.3118\n",
      "step 36310: train loss 2.3439, val loss 2.3376\n",
      "step 36320: train loss 2.3272, val loss 2.3292\n",
      "step 36330: train loss 2.4152, val loss 2.3043\n",
      "step 36340: train loss 2.4121, val loss 2.3873\n",
      "step 36350: train loss 2.4053, val loss 2.3574\n",
      "step 36360: train loss 2.3906, val loss 2.3826\n",
      "step 36370: train loss 2.4159, val loss 2.3653\n",
      "step 36380: train loss 2.3327, val loss 2.3831\n",
      "step 36390: train loss 2.4150, val loss 2.3243\n",
      "step 36400: train loss 2.3678, val loss 2.3674\n",
      "step 36410: train loss 2.4331, val loss 2.3619\n",
      "step 36420: train loss 2.4450, val loss 2.3769\n",
      "step 36430: train loss 2.3644, val loss 2.2959\n",
      "step 36440: train loss 2.4553, val loss 2.3861\n",
      "step 36450: train loss 2.3999, val loss 2.3728\n",
      "step 36460: train loss 2.4124, val loss 2.3582\n",
      "step 36470: train loss 2.4232, val loss 2.3213\n",
      "step 36480: train loss 2.4666, val loss 2.3906\n",
      "step 36490: train loss 2.3702, val loss 2.3571\n",
      "step 36500: train loss 2.3106, val loss 2.3290\n",
      "step 36510: train loss 2.3386, val loss 2.3429\n",
      "step 36520: train loss 2.3647, val loss 2.3649\n",
      "step 36530: train loss 2.4159, val loss 2.4015\n",
      "step 36540: train loss 2.3590, val loss 2.3494\n",
      "step 36550: train loss 2.3829, val loss 2.3767\n",
      "step 36560: train loss 2.4618, val loss 2.3889\n",
      "step 36570: train loss 2.3842, val loss 2.3272\n",
      "step 36580: train loss 2.3331, val loss 2.3202\n",
      "step 36590: train loss 2.2718, val loss 2.3580\n",
      "step 36600: train loss 2.3910, val loss 2.3727\n",
      "step 36610: train loss 2.3339, val loss 2.3885\n",
      "step 36620: train loss 2.3868, val loss 2.3516\n",
      "step 36630: train loss 2.3126, val loss 2.3712\n",
      "step 36640: train loss 2.4285, val loss 2.3665\n",
      "step 36650: train loss 2.3389, val loss 2.4318\n",
      "step 36660: train loss 2.3785, val loss 2.3454\n",
      "step 36670: train loss 2.4390, val loss 2.3733\n",
      "step 36680: train loss 2.3090, val loss 2.3493\n",
      "step 36690: train loss 2.3915, val loss 2.4005\n",
      "step 36700: train loss 2.3506, val loss 2.4634\n",
      "step 36710: train loss 2.4107, val loss 2.3688\n",
      "step 36720: train loss 2.3999, val loss 2.3807\n",
      "step 36730: train loss 2.3915, val loss 2.3775\n",
      "step 36740: train loss 2.3665, val loss 2.3872\n",
      "step 36750: train loss 2.4811, val loss 2.3195\n",
      "step 36760: train loss 2.4054, val loss 2.3749\n",
      "step 36770: train loss 2.3634, val loss 2.4359\n",
      "step 36780: train loss 2.4636, val loss 2.3207\n",
      "step 36790: train loss 2.3289, val loss 2.3957\n",
      "step 36800: train loss 2.3924, val loss 2.3879\n",
      "step 36810: train loss 2.3896, val loss 2.3407\n",
      "step 36820: train loss 2.3556, val loss 2.4161\n",
      "step 36830: train loss 2.3808, val loss 2.3502\n",
      "step 36840: train loss 2.3934, val loss 2.3641\n",
      "step 36850: train loss 2.4522, val loss 2.3647\n",
      "step 36860: train loss 2.3901, val loss 2.3796\n",
      "step 36870: train loss 2.3749, val loss 2.3510\n",
      "step 36880: train loss 2.3447, val loss 2.4280\n",
      "step 36890: train loss 2.3995, val loss 2.3079\n",
      "step 36900: train loss 2.3766, val loss 2.2983\n",
      "step 36910: train loss 2.3355, val loss 2.2618\n",
      "step 36920: train loss 2.3081, val loss 2.3617\n",
      "step 36930: train loss 2.3658, val loss 2.3121\n",
      "step 36940: train loss 2.3577, val loss 2.3383\n",
      "step 36950: train loss 2.3181, val loss 2.3611\n",
      "step 36960: train loss 2.3953, val loss 2.4138\n",
      "step 36970: train loss 2.4362, val loss 2.3896\n",
      "step 36980: train loss 2.3924, val loss 2.3667\n",
      "step 36990: train loss 2.2977, val loss 2.4416\n",
      "step 37000: train loss 2.3389, val loss 2.3864\n",
      "step 37010: train loss 2.3726, val loss 2.3539\n",
      "step 37020: train loss 2.3010, val loss 2.4332\n",
      "step 37030: train loss 2.3066, val loss 2.3640\n",
      "step 37040: train loss 2.3822, val loss 2.3128\n",
      "step 37050: train loss 2.4399, val loss 2.3260\n",
      "step 37060: train loss 2.3550, val loss 2.4542\n",
      "step 37070: train loss 2.3847, val loss 2.4269\n",
      "step 37080: train loss 2.3291, val loss 2.3989\n",
      "step 37090: train loss 2.3547, val loss 2.3542\n",
      "step 37100: train loss 2.3427, val loss 2.3923\n",
      "step 37110: train loss 2.3942, val loss 2.3193\n",
      "step 37120: train loss 2.4957, val loss 2.3272\n",
      "step 37130: train loss 2.3254, val loss 2.3835\n",
      "step 37140: train loss 2.4174, val loss 2.3740\n",
      "step 37150: train loss 2.3606, val loss 2.3025\n",
      "step 37160: train loss 2.3064, val loss 2.4349\n",
      "step 37170: train loss 2.3026, val loss 2.4537\n",
      "step 37180: train loss 2.4460, val loss 2.3658\n",
      "step 37190: train loss 2.3380, val loss 2.2669\n",
      "step 37200: train loss 2.3922, val loss 2.3470\n",
      "step 37210: train loss 2.2997, val loss 2.4119\n",
      "step 37220: train loss 2.3154, val loss 2.4069\n",
      "step 37230: train loss 2.3701, val loss 2.3646\n",
      "step 37240: train loss 2.3590, val loss 2.3444\n",
      "step 37250: train loss 2.4115, val loss 2.3039\n",
      "step 37260: train loss 2.4562, val loss 2.3030\n",
      "step 37270: train loss 2.3769, val loss 2.2895\n",
      "step 37280: train loss 2.4284, val loss 2.3689\n",
      "step 37290: train loss 2.3407, val loss 2.3263\n",
      "step 37300: train loss 2.3392, val loss 2.3747\n",
      "step 37310: train loss 2.3594, val loss 2.3499\n",
      "step 37320: train loss 2.4280, val loss 2.3470\n",
      "step 37330: train loss 2.4601, val loss 2.3550\n",
      "step 37340: train loss 2.3576, val loss 2.4247\n",
      "step 37350: train loss 2.4026, val loss 2.3727\n",
      "step 37360: train loss 2.3995, val loss 2.3863\n",
      "step 37370: train loss 2.4025, val loss 2.4272\n",
      "step 37380: train loss 2.4267, val loss 2.3316\n",
      "step 37390: train loss 2.3663, val loss 2.3245\n",
      "step 37400: train loss 2.4477, val loss 2.4263\n",
      "step 37410: train loss 2.3478, val loss 2.3280\n",
      "step 37420: train loss 2.3978, val loss 2.2836\n",
      "step 37430: train loss 2.3923, val loss 2.4433\n",
      "step 37440: train loss 2.3532, val loss 2.3959\n",
      "step 37450: train loss 2.3644, val loss 2.3824\n",
      "step 37460: train loss 2.3385, val loss 2.3175\n",
      "step 37470: train loss 2.3729, val loss 2.4437\n",
      "step 37480: train loss 2.3676, val loss 2.4618\n",
      "step 37490: train loss 2.3379, val loss 2.3252\n",
      "step 37500: train loss 2.3081, val loss 2.4017\n",
      "step 37510: train loss 2.4789, val loss 2.3996\n",
      "step 37520: train loss 2.4105, val loss 2.3372\n",
      "step 37530: train loss 2.3674, val loss 2.3826\n",
      "step 37540: train loss 2.3517, val loss 2.4193\n",
      "step 37550: train loss 2.3609, val loss 2.4084\n",
      "step 37560: train loss 2.4385, val loss 2.3508\n",
      "step 37570: train loss 2.4200, val loss 2.3150\n",
      "step 37580: train loss 2.3284, val loss 2.3604\n",
      "step 37590: train loss 2.3980, val loss 2.3124\n",
      "step 37600: train loss 2.3418, val loss 2.3265\n",
      "step 37610: train loss 2.3793, val loss 2.4696\n",
      "step 37620: train loss 2.4484, val loss 2.3448\n",
      "step 37630: train loss 2.4070, val loss 2.4156\n",
      "step 37640: train loss 2.3908, val loss 2.3392\n",
      "step 37650: train loss 2.3580, val loss 2.3999\n",
      "step 37660: train loss 2.3529, val loss 2.4095\n",
      "step 37670: train loss 2.3879, val loss 2.3196\n",
      "step 37680: train loss 2.4041, val loss 2.3009\n",
      "step 37690: train loss 2.4255, val loss 2.3843\n",
      "step 37700: train loss 2.3890, val loss 2.4462\n",
      "step 37710: train loss 2.3549, val loss 2.3192\n",
      "step 37720: train loss 2.4053, val loss 2.4451\n",
      "step 37730: train loss 2.4015, val loss 2.4199\n",
      "step 37740: train loss 2.4154, val loss 2.3353\n",
      "step 37750: train loss 2.4312, val loss 2.3436\n",
      "step 37760: train loss 2.3664, val loss 2.3143\n",
      "step 37770: train loss 2.4521, val loss 2.3228\n",
      "step 37780: train loss 2.4072, val loss 2.2934\n",
      "step 37790: train loss 2.4039, val loss 2.3727\n",
      "step 37800: train loss 2.3893, val loss 2.3391\n",
      "step 37810: train loss 2.4325, val loss 2.4288\n",
      "step 37820: train loss 2.3867, val loss 2.4022\n",
      "step 37830: train loss 2.3998, val loss 2.3484\n",
      "step 37840: train loss 2.4030, val loss 2.3606\n",
      "step 37850: train loss 2.3532, val loss 2.3215\n",
      "step 37860: train loss 2.3227, val loss 2.3899\n",
      "step 37870: train loss 2.3573, val loss 2.4169\n",
      "step 37880: train loss 2.3864, val loss 2.3865\n",
      "step 37890: train loss 2.3762, val loss 2.4043\n",
      "step 37900: train loss 2.4043, val loss 2.3812\n",
      "step 37910: train loss 2.4180, val loss 2.4177\n",
      "step 37920: train loss 2.3682, val loss 2.3804\n",
      "step 37930: train loss 2.4955, val loss 2.3512\n",
      "step 37940: train loss 2.4248, val loss 2.3554\n",
      "step 37950: train loss 2.4242, val loss 2.3207\n",
      "step 37960: train loss 2.4361, val loss 2.4096\n",
      "step 37970: train loss 2.4024, val loss 2.3920\n",
      "step 37980: train loss 2.3848, val loss 2.3101\n",
      "step 37990: train loss 2.3986, val loss 2.3735\n",
      "step 38000: train loss 2.3884, val loss 2.2915\n",
      "step 38010: train loss 2.3942, val loss 2.3631\n",
      "step 38020: train loss 2.3947, val loss 2.3087\n",
      "step 38030: train loss 2.3495, val loss 2.3933\n",
      "step 38040: train loss 2.4265, val loss 2.3806\n",
      "step 38050: train loss 2.3235, val loss 2.3170\n",
      "step 38060: train loss 2.3788, val loss 2.3893\n",
      "step 38070: train loss 2.4494, val loss 2.3182\n",
      "step 38080: train loss 2.3455, val loss 2.3241\n",
      "step 38090: train loss 2.4899, val loss 2.4175\n",
      "step 38100: train loss 2.3372, val loss 2.3679\n",
      "step 38110: train loss 2.3161, val loss 2.4123\n",
      "step 38120: train loss 2.3709, val loss 2.3752\n",
      "step 38130: train loss 2.4196, val loss 2.4506\n",
      "step 38140: train loss 2.3419, val loss 2.4115\n",
      "step 38150: train loss 2.3502, val loss 2.3124\n",
      "step 38160: train loss 2.3665, val loss 2.3692\n",
      "step 38170: train loss 2.3420, val loss 2.3947\n",
      "step 38180: train loss 2.4333, val loss 2.4206\n",
      "step 38190: train loss 2.4160, val loss 2.3408\n",
      "step 38200: train loss 2.4164, val loss 2.3453\n",
      "step 38210: train loss 2.4158, val loss 2.3084\n",
      "step 38220: train loss 2.3258, val loss 2.3915\n",
      "step 38230: train loss 2.3642, val loss 2.2864\n",
      "step 38240: train loss 2.3433, val loss 2.3218\n",
      "step 38250: train loss 2.3761, val loss 2.3221\n",
      "step 38260: train loss 2.4021, val loss 2.2810\n",
      "step 38270: train loss 2.3769, val loss 2.4322\n",
      "step 38280: train loss 2.3924, val loss 2.3766\n",
      "step 38290: train loss 2.3501, val loss 2.3421\n",
      "step 38300: train loss 2.4303, val loss 2.2945\n",
      "step 38310: train loss 2.4589, val loss 2.3654\n",
      "step 38320: train loss 2.3619, val loss 2.3908\n",
      "step 38330: train loss 2.3153, val loss 2.3270\n",
      "step 38340: train loss 2.3593, val loss 2.4191\n",
      "step 38350: train loss 2.3920, val loss 2.3591\n",
      "step 38360: train loss 2.4226, val loss 2.3444\n",
      "step 38370: train loss 2.4160, val loss 2.3197\n",
      "step 38380: train loss 2.3267, val loss 2.4631\n",
      "step 38390: train loss 2.3710, val loss 2.4254\n",
      "step 38400: train loss 2.3375, val loss 2.3717\n",
      "step 38410: train loss 2.3763, val loss 2.3557\n",
      "step 38420: train loss 2.3476, val loss 2.2932\n",
      "step 38430: train loss 2.4201, val loss 2.3238\n",
      "step 38440: train loss 2.3488, val loss 2.3161\n",
      "step 38450: train loss 2.3557, val loss 2.3936\n",
      "step 38460: train loss 2.3107, val loss 2.3009\n",
      "step 38470: train loss 2.3881, val loss 2.4436\n",
      "step 38480: train loss 2.3823, val loss 2.3186\n",
      "step 38490: train loss 2.2866, val loss 2.4078\n",
      "step 38500: train loss 2.3778, val loss 2.3713\n",
      "step 38510: train loss 2.3968, val loss 2.3293\n",
      "step 38520: train loss 2.4257, val loss 2.3669\n",
      "step 38530: train loss 2.3285, val loss 2.3700\n",
      "step 38540: train loss 2.3785, val loss 2.3772\n",
      "step 38550: train loss 2.3501, val loss 2.3717\n",
      "step 38560: train loss 2.3735, val loss 2.3833\n",
      "step 38570: train loss 2.3098, val loss 2.3125\n",
      "step 38580: train loss 2.4390, val loss 2.3459\n",
      "step 38590: train loss 2.5268, val loss 2.3384\n",
      "step 38600: train loss 2.4129, val loss 2.4023\n",
      "step 38610: train loss 2.3404, val loss 2.4493\n",
      "step 38620: train loss 2.4007, val loss 2.4103\n",
      "step 38630: train loss 2.3808, val loss 2.3019\n",
      "step 38640: train loss 2.4120, val loss 2.4795\n",
      "step 38650: train loss 2.4890, val loss 2.3462\n",
      "step 38660: train loss 2.3650, val loss 2.3842\n",
      "step 38670: train loss 2.4440, val loss 2.4448\n",
      "step 38680: train loss 2.4039, val loss 2.3562\n",
      "step 38690: train loss 2.3899, val loss 2.3494\n",
      "step 38700: train loss 2.3038, val loss 2.3728\n",
      "step 38710: train loss 2.3604, val loss 2.3021\n",
      "step 38720: train loss 2.4719, val loss 2.3516\n",
      "step 38730: train loss 2.3587, val loss 2.3862\n",
      "step 38740: train loss 2.4976, val loss 2.4641\n",
      "step 38750: train loss 2.3734, val loss 2.3695\n",
      "step 38760: train loss 2.3307, val loss 2.3599\n",
      "step 38770: train loss 2.4138, val loss 2.3734\n",
      "step 38780: train loss 2.4111, val loss 2.2803\n",
      "step 38790: train loss 2.4044, val loss 2.3456\n",
      "step 38800: train loss 2.3671, val loss 2.3501\n",
      "step 38810: train loss 2.3666, val loss 2.3494\n",
      "step 38820: train loss 2.4410, val loss 2.2999\n",
      "step 38830: train loss 2.3758, val loss 2.3726\n",
      "step 38840: train loss 2.3813, val loss 2.3941\n",
      "step 38850: train loss 2.3943, val loss 2.3990\n",
      "step 38860: train loss 2.5029, val loss 2.3465\n",
      "step 38870: train loss 2.2829, val loss 2.3763\n",
      "step 38880: train loss 2.3407, val loss 2.3894\n",
      "step 38890: train loss 2.3683, val loss 2.3654\n",
      "step 38900: train loss 2.3334, val loss 2.3448\n",
      "step 38910: train loss 2.3574, val loss 2.4511\n",
      "step 38920: train loss 2.3631, val loss 2.4552\n",
      "step 38930: train loss 2.3269, val loss 2.3317\n",
      "step 38940: train loss 2.3876, val loss 2.3387\n",
      "step 38950: train loss 2.4274, val loss 2.4230\n",
      "step 38960: train loss 2.4282, val loss 2.4492\n",
      "step 38970: train loss 2.3925, val loss 2.4345\n",
      "step 38980: train loss 2.4187, val loss 2.3499\n",
      "step 38990: train loss 2.2933, val loss 2.3740\n",
      "step 39000: train loss 2.3230, val loss 2.3634\n",
      "step 39010: train loss 2.4655, val loss 2.3849\n",
      "step 39020: train loss 2.3954, val loss 2.4407\n",
      "step 39030: train loss 2.4092, val loss 2.3728\n",
      "step 39040: train loss 2.3702, val loss 2.4517\n",
      "step 39050: train loss 2.4507, val loss 2.3580\n",
      "step 39060: train loss 2.2917, val loss 2.3876\n",
      "step 39070: train loss 2.3366, val loss 2.3599\n",
      "step 39080: train loss 2.3733, val loss 2.3442\n",
      "step 39090: train loss 2.3808, val loss 2.3137\n",
      "step 39100: train loss 2.5069, val loss 2.3949\n",
      "step 39110: train loss 2.3350, val loss 2.3889\n",
      "step 39120: train loss 2.3568, val loss 2.3664\n",
      "step 39130: train loss 2.4127, val loss 2.3720\n",
      "step 39140: train loss 2.4883, val loss 2.3669\n",
      "step 39150: train loss 2.2553, val loss 2.3794\n",
      "step 39160: train loss 2.3943, val loss 2.3889\n",
      "step 39170: train loss 2.4766, val loss 2.4271\n",
      "step 39180: train loss 2.3488, val loss 2.3641\n",
      "step 39190: train loss 2.4011, val loss 2.3979\n",
      "step 39200: train loss 2.3817, val loss 2.4002\n",
      "step 39210: train loss 2.3435, val loss 2.3623\n",
      "step 39220: train loss 2.4099, val loss 2.3492\n",
      "step 39230: train loss 2.3503, val loss 2.4271\n",
      "step 39240: train loss 2.3581, val loss 2.4133\n",
      "step 39250: train loss 2.3399, val loss 2.3522\n",
      "step 39260: train loss 2.4243, val loss 2.4310\n",
      "step 39270: train loss 2.3999, val loss 2.3372\n",
      "step 39280: train loss 2.3817, val loss 2.4151\n",
      "step 39290: train loss 2.4730, val loss 2.4061\n",
      "step 39300: train loss 2.3620, val loss 2.3440\n",
      "step 39310: train loss 2.4508, val loss 2.3868\n",
      "step 39320: train loss 2.4550, val loss 2.2866\n",
      "step 39330: train loss 2.4286, val loss 2.3294\n",
      "step 39340: train loss 2.4426, val loss 2.3466\n",
      "step 39350: train loss 2.3749, val loss 2.3091\n",
      "step 39360: train loss 2.3675, val loss 2.3252\n",
      "step 39370: train loss 2.4420, val loss 2.3446\n",
      "step 39380: train loss 2.3558, val loss 2.3892\n",
      "step 39390: train loss 2.3524, val loss 2.3060\n",
      "step 39400: train loss 2.4087, val loss 2.3308\n",
      "step 39410: train loss 2.3180, val loss 2.3800\n",
      "step 39420: train loss 2.4310, val loss 2.3680\n",
      "step 39430: train loss 2.3719, val loss 2.4402\n",
      "step 39440: train loss 2.3828, val loss 2.3965\n",
      "step 39450: train loss 2.3724, val loss 2.4077\n",
      "step 39460: train loss 2.4626, val loss 2.4158\n",
      "step 39470: train loss 2.3099, val loss 2.4084\n",
      "step 39480: train loss 2.4232, val loss 2.3509\n",
      "step 39490: train loss 2.3407, val loss 2.3528\n",
      "step 39500: train loss 2.4420, val loss 2.3569\n",
      "step 39510: train loss 2.3385, val loss 2.3866\n",
      "step 39520: train loss 2.3461, val loss 2.4109\n",
      "step 39530: train loss 2.3129, val loss 2.3932\n",
      "step 39540: train loss 2.3537, val loss 2.3013\n",
      "step 39550: train loss 2.4275, val loss 2.3870\n",
      "step 39560: train loss 2.3637, val loss 2.3646\n",
      "step 39570: train loss 2.4219, val loss 2.3865\n",
      "step 39580: train loss 2.3316, val loss 2.3724\n",
      "step 39590: train loss 2.3897, val loss 2.2957\n",
      "step 39600: train loss 2.3994, val loss 2.3711\n",
      "step 39610: train loss 2.3493, val loss 2.3561\n",
      "step 39620: train loss 2.4165, val loss 2.3823\n",
      "step 39630: train loss 2.3785, val loss 2.3875\n",
      "step 39640: train loss 2.3233, val loss 2.3175\n",
      "step 39650: train loss 2.3693, val loss 2.3525\n",
      "step 39660: train loss 2.3701, val loss 2.4181\n",
      "step 39670: train loss 2.3585, val loss 2.4155\n",
      "step 39680: train loss 2.3082, val loss 2.4103\n",
      "step 39690: train loss 2.4370, val loss 2.3975\n",
      "step 39700: train loss 2.3083, val loss 2.3888\n",
      "step 39710: train loss 2.4463, val loss 2.3414\n",
      "step 39720: train loss 2.3549, val loss 2.3997\n",
      "step 39730: train loss 2.3405, val loss 2.3013\n",
      "step 39740: train loss 2.3559, val loss 2.4145\n",
      "step 39750: train loss 2.3629, val loss 2.5233\n",
      "step 39760: train loss 2.3944, val loss 2.3163\n",
      "step 39770: train loss 2.3490, val loss 2.3686\n",
      "step 39780: train loss 2.4474, val loss 2.4181\n",
      "step 39790: train loss 2.3167, val loss 2.3550\n",
      "step 39800: train loss 2.3334, val loss 2.3504\n",
      "step 39810: train loss 2.2878, val loss 2.4382\n",
      "step 39820: train loss 2.3076, val loss 2.3561\n",
      "step 39830: train loss 2.4534, val loss 2.3664\n",
      "step 39840: train loss 2.4151, val loss 2.4507\n",
      "step 39850: train loss 2.2805, val loss 2.4090\n",
      "step 39860: train loss 2.3260, val loss 2.3680\n",
      "step 39870: train loss 2.4610, val loss 2.4216\n",
      "step 39880: train loss 2.3594, val loss 2.3880\n",
      "step 39890: train loss 2.3652, val loss 2.4183\n",
      "step 39900: train loss 2.3443, val loss 2.3903\n",
      "step 39910: train loss 2.3228, val loss 2.3316\n",
      "step 39920: train loss 2.4121, val loss 2.2945\n",
      "step 39930: train loss 2.4215, val loss 2.2522\n",
      "step 39940: train loss 2.4085, val loss 2.3648\n",
      "step 39950: train loss 2.3769, val loss 2.3627\n",
      "step 39960: train loss 2.3552, val loss 2.4163\n",
      "step 39970: train loss 2.3855, val loss 2.3211\n",
      "step 39980: train loss 2.3124, val loss 2.3905\n",
      "step 39990: train loss 2.3784, val loss 2.3675\n",
      "step 40000: train loss 2.3574, val loss 2.3264\n",
      "step 40010: train loss 2.4250, val loss 2.4071\n",
      "step 40020: train loss 2.3326, val loss 2.3625\n",
      "step 40030: train loss 2.3042, val loss 2.3869\n",
      "step 40040: train loss 2.3649, val loss 2.3068\n",
      "step 40050: train loss 2.3502, val loss 2.4015\n",
      "step 40060: train loss 2.3776, val loss 2.3682\n",
      "step 40070: train loss 2.2951, val loss 2.4241\n",
      "step 40080: train loss 2.3686, val loss 2.3643\n",
      "step 40090: train loss 2.4092, val loss 2.4266\n",
      "step 40100: train loss 2.4014, val loss 2.3249\n",
      "step 40110: train loss 2.4195, val loss 2.3757\n",
      "step 40120: train loss 2.3326, val loss 2.4409\n",
      "step 40130: train loss 2.3882, val loss 2.3678\n",
      "step 40140: train loss 2.3671, val loss 2.3540\n",
      "step 40150: train loss 2.4022, val loss 2.3868\n",
      "step 40160: train loss 2.3861, val loss 2.3968\n",
      "step 40170: train loss 2.4202, val loss 2.4628\n",
      "step 40180: train loss 2.4373, val loss 2.3440\n",
      "step 40190: train loss 2.3869, val loss 2.3637\n",
      "step 40200: train loss 2.3580, val loss 2.3499\n",
      "step 40210: train loss 2.4000, val loss 2.3731\n",
      "step 40220: train loss 2.3678, val loss 2.3419\n",
      "step 40230: train loss 2.3724, val loss 2.4189\n",
      "step 40240: train loss 2.3985, val loss 2.3923\n",
      "step 40250: train loss 2.3549, val loss 2.3948\n",
      "step 40260: train loss 2.3709, val loss 2.3597\n",
      "step 40270: train loss 2.3555, val loss 2.3656\n",
      "step 40280: train loss 2.3476, val loss 2.4240\n",
      "step 40290: train loss 2.3400, val loss 2.3207\n",
      "step 40300: train loss 2.3842, val loss 2.3805\n",
      "step 40310: train loss 2.3705, val loss 2.3742\n",
      "step 40320: train loss 2.3957, val loss 2.3534\n",
      "step 40330: train loss 2.3590, val loss 2.3563\n",
      "step 40340: train loss 2.4280, val loss 2.3708\n",
      "step 40350: train loss 2.4083, val loss 2.4139\n",
      "step 40360: train loss 2.3844, val loss 2.4227\n",
      "step 40370: train loss 2.3917, val loss 2.3262\n",
      "step 40380: train loss 2.3893, val loss 2.4338\n",
      "step 40390: train loss 2.3272, val loss 2.3676\n",
      "step 40400: train loss 2.4324, val loss 2.3314\n",
      "step 40410: train loss 2.3370, val loss 2.3973\n",
      "step 40420: train loss 2.3748, val loss 2.4133\n",
      "step 40430: train loss 2.3451, val loss 2.3526\n",
      "step 40440: train loss 2.3601, val loss 2.3098\n",
      "step 40450: train loss 2.3249, val loss 2.4625\n",
      "step 40460: train loss 2.3786, val loss 2.3559\n",
      "step 40470: train loss 2.3522, val loss 2.4062\n",
      "step 40480: train loss 2.3487, val loss 2.3826\n",
      "step 40490: train loss 2.3720, val loss 2.2952\n",
      "step 40500: train loss 2.3406, val loss 2.4337\n",
      "step 40510: train loss 2.3330, val loss 2.3955\n",
      "step 40520: train loss 2.4446, val loss 2.3787\n",
      "step 40530: train loss 2.3491, val loss 2.3748\n",
      "step 40540: train loss 2.4282, val loss 2.4133\n",
      "step 40550: train loss 2.4008, val loss 2.3725\n",
      "step 40560: train loss 2.3719, val loss 2.4451\n",
      "step 40570: train loss 2.4430, val loss 2.3851\n",
      "step 40580: train loss 2.4222, val loss 2.3791\n",
      "step 40590: train loss 2.3850, val loss 2.4075\n",
      "step 40600: train loss 2.4132, val loss 2.2923\n",
      "step 40610: train loss 2.3909, val loss 2.3493\n",
      "step 40620: train loss 2.3357, val loss 2.4022\n",
      "step 40630: train loss 2.4000, val loss 2.3385\n",
      "step 40640: train loss 2.3394, val loss 2.3475\n",
      "step 40650: train loss 2.3840, val loss 2.3610\n",
      "step 40660: train loss 2.3320, val loss 2.3498\n",
      "step 40670: train loss 2.3796, val loss 2.3667\n",
      "step 40680: train loss 2.3886, val loss 2.3354\n",
      "step 40690: train loss 2.3464, val loss 2.3912\n",
      "step 40700: train loss 2.3399, val loss 2.3264\n",
      "step 40710: train loss 2.4183, val loss 2.3110\n",
      "step 40720: train loss 2.3994, val loss 2.4143\n",
      "step 40730: train loss 2.3758, val loss 2.5507\n",
      "step 40740: train loss 2.3916, val loss 2.3633\n",
      "step 40750: train loss 2.3301, val loss 2.3487\n",
      "step 40760: train loss 2.3931, val loss 2.3415\n",
      "step 40770: train loss 2.3018, val loss 2.3263\n",
      "step 40780: train loss 2.3991, val loss 2.4442\n",
      "step 40790: train loss 2.4276, val loss 2.3281\n",
      "step 40800: train loss 2.3435, val loss 2.3753\n",
      "step 40810: train loss 2.4026, val loss 2.4131\n",
      "step 40820: train loss 2.3764, val loss 2.4506\n",
      "step 40830: train loss 2.4179, val loss 2.3687\n",
      "step 40840: train loss 2.4179, val loss 2.3597\n",
      "step 40850: train loss 2.4252, val loss 2.2961\n",
      "step 40860: train loss 2.4334, val loss 2.3849\n",
      "step 40870: train loss 2.4424, val loss 2.4821\n",
      "step 40880: train loss 2.4611, val loss 2.3800\n",
      "step 40890: train loss 2.4493, val loss 2.3067\n",
      "step 40900: train loss 2.3529, val loss 2.3341\n",
      "step 40910: train loss 2.3273, val loss 2.4120\n",
      "step 40920: train loss 2.3787, val loss 2.3694\n",
      "step 40930: train loss 2.4151, val loss 2.3784\n",
      "step 40940: train loss 2.3980, val loss 2.4521\n",
      "step 40950: train loss 2.4212, val loss 2.3957\n",
      "step 40960: train loss 2.3845, val loss 2.2661\n",
      "step 40970: train loss 2.4553, val loss 2.3586\n",
      "step 40980: train loss 2.3594, val loss 2.3964\n",
      "step 40990: train loss 2.3625, val loss 2.3768\n",
      "step 41000: train loss 2.4333, val loss 2.2641\n",
      "step 41010: train loss 2.3874, val loss 2.3276\n",
      "step 41020: train loss 2.3159, val loss 2.4033\n",
      "step 41030: train loss 2.4145, val loss 2.3150\n",
      "step 41040: train loss 2.4653, val loss 2.3863\n",
      "step 41050: train loss 2.3978, val loss 2.3266\n",
      "step 41060: train loss 2.3851, val loss 2.3811\n",
      "step 41070: train loss 2.3687, val loss 2.3799\n",
      "step 41080: train loss 2.3961, val loss 2.4187\n",
      "step 41090: train loss 2.4167, val loss 2.4168\n",
      "step 41100: train loss 2.3346, val loss 2.3557\n",
      "step 41110: train loss 2.3863, val loss 2.3650\n",
      "step 41120: train loss 2.3775, val loss 2.3656\n",
      "step 41130: train loss 2.3531, val loss 2.4392\n",
      "step 41140: train loss 2.2924, val loss 2.3792\n",
      "step 41150: train loss 2.3960, val loss 2.3103\n",
      "step 41160: train loss 2.3861, val loss 2.3481\n",
      "step 41170: train loss 2.3455, val loss 2.4142\n",
      "step 41180: train loss 2.3654, val loss 2.3772\n",
      "step 41190: train loss 2.4041, val loss 2.3241\n",
      "step 41200: train loss 2.3959, val loss 2.3733\n",
      "step 41210: train loss 2.3266, val loss 2.3134\n",
      "step 41220: train loss 2.3889, val loss 2.3096\n",
      "step 41230: train loss 2.3727, val loss 2.3745\n",
      "step 41240: train loss 2.4188, val loss 2.3350\n",
      "step 41250: train loss 2.3465, val loss 2.4086\n",
      "step 41260: train loss 2.4019, val loss 2.3561\n",
      "step 41270: train loss 2.3607, val loss 2.3305\n",
      "step 41280: train loss 2.3448, val loss 2.2981\n",
      "step 41290: train loss 2.3751, val loss 2.3955\n",
      "step 41300: train loss 2.2501, val loss 2.3514\n",
      "step 41310: train loss 2.3933, val loss 2.3078\n",
      "step 41320: train loss 2.4107, val loss 2.3954\n",
      "step 41330: train loss 2.4216, val loss 2.4063\n",
      "step 41340: train loss 2.3127, val loss 2.3188\n",
      "step 41350: train loss 2.4069, val loss 2.3574\n",
      "step 41360: train loss 2.3725, val loss 2.4083\n",
      "step 41370: train loss 2.3734, val loss 2.3618\n",
      "step 41380: train loss 2.3782, val loss 2.3373\n",
      "step 41390: train loss 2.3448, val loss 2.3834\n",
      "step 41400: train loss 2.2680, val loss 2.3903\n",
      "step 41410: train loss 2.3357, val loss 2.4559\n",
      "step 41420: train loss 2.4277, val loss 2.3905\n",
      "step 41430: train loss 2.4009, val loss 2.3886\n",
      "step 41440: train loss 2.3815, val loss 2.3389\n",
      "step 41450: train loss 2.3114, val loss 2.3321\n",
      "step 41460: train loss 2.4596, val loss 2.3347\n",
      "step 41470: train loss 2.3262, val loss 2.4488\n",
      "step 41480: train loss 2.3755, val loss 2.3753\n",
      "step 41490: train loss 2.4329, val loss 2.4063\n",
      "step 41500: train loss 2.4024, val loss 2.3830\n",
      "step 41510: train loss 2.4046, val loss 2.3437\n",
      "step 41520: train loss 2.4269, val loss 2.2879\n",
      "step 41530: train loss 2.3773, val loss 2.3687\n",
      "step 41540: train loss 2.3687, val loss 2.3822\n",
      "step 41550: train loss 2.3611, val loss 2.3265\n",
      "step 41560: train loss 2.3204, val loss 2.2854\n",
      "step 41570: train loss 2.4175, val loss 2.3453\n",
      "step 41580: train loss 2.4167, val loss 2.3473\n",
      "step 41590: train loss 2.4365, val loss 2.3668\n",
      "step 41600: train loss 2.3593, val loss 2.3010\n",
      "step 41610: train loss 2.3674, val loss 2.4223\n",
      "step 41620: train loss 2.3101, val loss 2.3783\n",
      "step 41630: train loss 2.4010, val loss 2.3607\n",
      "step 41640: train loss 2.3910, val loss 2.3394\n",
      "step 41650: train loss 2.3402, val loss 2.4110\n",
      "step 41660: train loss 2.4044, val loss 2.3483\n",
      "step 41670: train loss 2.3533, val loss 2.4055\n",
      "step 41680: train loss 2.4415, val loss 2.3594\n",
      "step 41690: train loss 2.3300, val loss 2.3968\n",
      "step 41700: train loss 2.3340, val loss 2.3322\n",
      "step 41710: train loss 2.3858, val loss 2.4015\n",
      "step 41720: train loss 2.4407, val loss 2.3417\n",
      "step 41730: train loss 2.3435, val loss 2.4206\n",
      "step 41740: train loss 2.4774, val loss 2.4674\n",
      "step 41750: train loss 2.3363, val loss 2.3337\n",
      "step 41760: train loss 2.2965, val loss 2.3388\n",
      "step 41770: train loss 2.3490, val loss 2.3888\n",
      "step 41780: train loss 2.3447, val loss 2.4513\n",
      "step 41790: train loss 2.3280, val loss 2.4628\n",
      "step 41800: train loss 2.3470, val loss 2.3794\n",
      "step 41810: train loss 2.4471, val loss 2.3479\n",
      "step 41820: train loss 2.3202, val loss 2.3493\n",
      "step 41830: train loss 2.4137, val loss 2.3428\n",
      "step 41840: train loss 2.3861, val loss 2.3767\n",
      "step 41850: train loss 2.3828, val loss 2.4073\n",
      "step 41860: train loss 2.3739, val loss 2.3892\n",
      "step 41870: train loss 2.2952, val loss 2.3180\n",
      "step 41880: train loss 2.3946, val loss 2.3973\n",
      "step 41890: train loss 2.4047, val loss 2.4097\n",
      "step 41900: train loss 2.3938, val loss 2.4210\n",
      "step 41910: train loss 2.4416, val loss 2.2971\n",
      "step 41920: train loss 2.3398, val loss 2.3448\n",
      "step 41930: train loss 2.2851, val loss 2.3620\n",
      "step 41940: train loss 2.3577, val loss 2.3656\n",
      "step 41950: train loss 2.3935, val loss 2.3269\n",
      "step 41960: train loss 2.3765, val loss 2.3834\n",
      "step 41970: train loss 2.3552, val loss 2.3682\n",
      "step 41980: train loss 2.3679, val loss 2.3056\n",
      "step 41990: train loss 2.4333, val loss 2.3773\n",
      "step 42000: train loss 2.3464, val loss 2.2508\n",
      "step 42010: train loss 2.3694, val loss 2.3508\n",
      "step 42020: train loss 2.3154, val loss 2.3977\n",
      "step 42030: train loss 2.4260, val loss 2.3805\n",
      "step 42040: train loss 2.3713, val loss 2.3476\n",
      "step 42050: train loss 2.3451, val loss 2.2958\n",
      "step 42060: train loss 2.3453, val loss 2.4177\n",
      "step 42070: train loss 2.4191, val loss 2.3874\n",
      "step 42080: train loss 2.3706, val loss 2.3695\n",
      "step 42090: train loss 2.3050, val loss 2.4498\n",
      "step 42100: train loss 2.3800, val loss 2.3919\n",
      "step 42110: train loss 2.3937, val loss 2.3775\n",
      "step 42120: train loss 2.3302, val loss 2.4310\n",
      "step 42130: train loss 2.4458, val loss 2.3982\n",
      "step 42140: train loss 2.3772, val loss 2.4172\n",
      "step 42150: train loss 2.3049, val loss 2.3612\n",
      "step 42160: train loss 2.3740, val loss 2.3614\n",
      "step 42170: train loss 2.3071, val loss 2.4533\n",
      "step 42180: train loss 2.4528, val loss 2.3398\n",
      "step 42190: train loss 2.3682, val loss 2.4625\n",
      "step 42200: train loss 2.3437, val loss 2.3837\n",
      "step 42210: train loss 2.4057, val loss 2.3766\n",
      "step 42220: train loss 2.3963, val loss 2.3317\n",
      "step 42230: train loss 2.3677, val loss 2.3444\n",
      "step 42240: train loss 2.3607, val loss 2.3439\n",
      "step 42250: train loss 2.4042, val loss 2.3654\n",
      "step 42260: train loss 2.3809, val loss 2.3560\n",
      "step 42270: train loss 2.4119, val loss 2.3712\n",
      "step 42280: train loss 2.4054, val loss 2.4676\n",
      "step 42290: train loss 2.3308, val loss 2.3705\n",
      "step 42300: train loss 2.4096, val loss 2.3712\n",
      "step 42310: train loss 2.2967, val loss 2.3285\n",
      "step 42320: train loss 2.3433, val loss 2.4221\n",
      "step 42330: train loss 2.3638, val loss 2.3492\n",
      "step 42340: train loss 2.3827, val loss 2.4077\n",
      "step 42350: train loss 2.3540, val loss 2.4042\n",
      "step 42360: train loss 2.3737, val loss 2.4493\n",
      "step 42370: train loss 2.3707, val loss 2.3684\n",
      "step 42380: train loss 2.4343, val loss 2.2591\n",
      "step 42390: train loss 2.3298, val loss 2.3704\n",
      "step 42400: train loss 2.3264, val loss 2.3047\n",
      "step 42410: train loss 2.3785, val loss 2.4046\n",
      "step 42420: train loss 2.4014, val loss 2.4291\n",
      "step 42430: train loss 2.3721, val loss 2.3921\n",
      "step 42440: train loss 2.3266, val loss 2.3355\n",
      "step 42450: train loss 2.3352, val loss 2.3682\n",
      "step 42460: train loss 2.4141, val loss 2.3837\n",
      "step 42470: train loss 2.3910, val loss 2.2828\n",
      "step 42480: train loss 2.4516, val loss 2.3444\n",
      "step 42490: train loss 2.4350, val loss 2.3358\n",
      "step 42500: train loss 2.3710, val loss 2.3739\n",
      "step 42510: train loss 2.4338, val loss 2.4095\n",
      "step 42520: train loss 2.4250, val loss 2.3638\n",
      "step 42530: train loss 2.4016, val loss 2.4515\n",
      "step 42540: train loss 2.4246, val loss 2.3651\n",
      "step 42550: train loss 2.3903, val loss 2.4029\n",
      "step 42560: train loss 2.2918, val loss 2.2977\n",
      "step 42570: train loss 2.4350, val loss 2.3048\n",
      "step 42580: train loss 2.3662, val loss 2.3167\n",
      "step 42590: train loss 2.3653, val loss 2.3625\n",
      "step 42600: train loss 2.3362, val loss 2.4255\n",
      "step 42610: train loss 2.4098, val loss 2.4567\n",
      "step 42620: train loss 2.4029, val loss 2.3565\n",
      "step 42630: train loss 2.3203, val loss 2.3897\n",
      "step 42640: train loss 2.4369, val loss 2.2640\n",
      "step 42650: train loss 2.4045, val loss 2.3984\n",
      "step 42660: train loss 2.4359, val loss 2.3018\n",
      "step 42670: train loss 2.3419, val loss 2.3169\n",
      "step 42680: train loss 2.3802, val loss 2.2599\n",
      "step 42690: train loss 2.3961, val loss 2.3086\n",
      "step 42700: train loss 2.4574, val loss 2.3710\n",
      "step 42710: train loss 2.4706, val loss 2.3435\n",
      "step 42720: train loss 2.3631, val loss 2.3522\n",
      "step 42730: train loss 2.3978, val loss 2.2656\n",
      "step 42740: train loss 2.3567, val loss 2.3592\n",
      "step 42750: train loss 2.4348, val loss 2.3020\n",
      "step 42760: train loss 2.3640, val loss 2.2729\n",
      "step 42770: train loss 2.4205, val loss 2.3540\n",
      "step 42780: train loss 2.3851, val loss 2.3619\n",
      "step 42790: train loss 2.4092, val loss 2.3891\n",
      "step 42800: train loss 2.3250, val loss 2.3430\n",
      "step 42810: train loss 2.3270, val loss 2.2689\n",
      "step 42820: train loss 2.3681, val loss 2.4039\n",
      "step 42830: train loss 2.3329, val loss 2.3500\n",
      "step 42840: train loss 2.4645, val loss 2.4169\n",
      "step 42850: train loss 2.3738, val loss 2.3684\n",
      "step 42860: train loss 2.3944, val loss 2.3393\n",
      "step 42870: train loss 2.2989, val loss 2.4204\n",
      "step 42880: train loss 2.3640, val loss 2.3078\n",
      "step 42890: train loss 2.3911, val loss 2.3308\n",
      "step 42900: train loss 2.3895, val loss 2.3597\n",
      "step 42910: train loss 2.3820, val loss 2.4421\n",
      "step 42920: train loss 2.3965, val loss 2.3972\n",
      "step 42930: train loss 2.3578, val loss 2.3301\n",
      "step 42940: train loss 2.4522, val loss 2.4062\n",
      "step 42950: train loss 2.3734, val loss 2.3991\n",
      "step 42960: train loss 2.3947, val loss 2.4825\n",
      "step 42970: train loss 2.5013, val loss 2.4062\n",
      "step 42980: train loss 2.4194, val loss 2.4844\n",
      "step 42990: train loss 2.3696, val loss 2.3416\n",
      "step 43000: train loss 2.4137, val loss 2.3639\n",
      "step 43010: train loss 2.4194, val loss 2.3334\n",
      "step 43020: train loss 2.3714, val loss 2.4275\n",
      "step 43030: train loss 2.3276, val loss 2.3611\n",
      "step 43040: train loss 2.3188, val loss 2.3677\n",
      "step 43050: train loss 2.2724, val loss 2.3609\n",
      "step 43060: train loss 2.3484, val loss 2.3558\n",
      "step 43070: train loss 2.4079, val loss 2.3408\n",
      "step 43080: train loss 2.4058, val loss 2.4457\n",
      "step 43090: train loss 2.3471, val loss 2.4153\n",
      "step 43100: train loss 2.3838, val loss 2.3753\n",
      "step 43110: train loss 2.4026, val loss 2.3742\n",
      "step 43120: train loss 2.3963, val loss 2.3503\n",
      "step 43130: train loss 2.4066, val loss 2.4186\n",
      "step 43140: train loss 2.4022, val loss 2.3564\n",
      "step 43150: train loss 2.3787, val loss 2.3713\n",
      "step 43160: train loss 2.3039, val loss 2.3490\n",
      "step 43170: train loss 2.4098, val loss 2.3987\n",
      "step 43180: train loss 2.4172, val loss 2.3229\n",
      "step 43190: train loss 2.4125, val loss 2.3678\n",
      "step 43200: train loss 2.4202, val loss 2.3234\n",
      "step 43210: train loss 2.3147, val loss 2.3663\n",
      "step 43220: train loss 2.4029, val loss 2.4053\n",
      "step 43230: train loss 2.3231, val loss 2.4219\n",
      "step 43240: train loss 2.4387, val loss 2.2882\n",
      "step 43250: train loss 2.3900, val loss 2.4202\n",
      "step 43260: train loss 2.3439, val loss 2.3255\n",
      "step 43270: train loss 2.3543, val loss 2.2983\n",
      "step 43280: train loss 2.3910, val loss 2.4151\n",
      "step 43290: train loss 2.3624, val loss 2.4131\n",
      "step 43300: train loss 2.3864, val loss 2.3693\n",
      "step 43310: train loss 2.4157, val loss 2.3575\n",
      "step 43320: train loss 2.4218, val loss 2.3521\n",
      "step 43330: train loss 2.3748, val loss 2.3677\n",
      "step 43340: train loss 2.3212, val loss 2.3763\n",
      "step 43350: train loss 2.4201, val loss 2.3209\n",
      "step 43360: train loss 2.2988, val loss 2.3512\n",
      "step 43370: train loss 2.3423, val loss 2.2935\n",
      "step 43380: train loss 2.3600, val loss 2.3571\n",
      "step 43390: train loss 2.3647, val loss 2.4008\n",
      "step 43400: train loss 2.4024, val loss 2.3448\n",
      "step 43410: train loss 2.4270, val loss 2.2813\n",
      "step 43420: train loss 2.3486, val loss 2.3468\n",
      "step 43430: train loss 2.4372, val loss 2.3515\n",
      "step 43440: train loss 2.3652, val loss 2.3212\n",
      "step 43450: train loss 2.4684, val loss 2.3526\n",
      "step 43460: train loss 2.3407, val loss 2.4470\n",
      "step 43470: train loss 2.3922, val loss 2.4644\n",
      "step 43480: train loss 2.4596, val loss 2.3485\n",
      "step 43490: train loss 2.4978, val loss 2.3530\n",
      "step 43500: train loss 2.4497, val loss 2.3558\n",
      "step 43510: train loss 2.4526, val loss 2.4249\n",
      "step 43520: train loss 2.4326, val loss 2.4268\n",
      "step 43530: train loss 2.3736, val loss 2.2974\n",
      "step 43540: train loss 2.3545, val loss 2.3246\n",
      "step 43550: train loss 2.4069, val loss 2.4119\n",
      "step 43560: train loss 2.3879, val loss 2.4896\n",
      "step 43570: train loss 2.3585, val loss 2.2935\n",
      "step 43580: train loss 2.3510, val loss 2.3825\n",
      "step 43590: train loss 2.4329, val loss 2.2967\n",
      "step 43600: train loss 2.2938, val loss 2.3804\n",
      "step 43610: train loss 2.3617, val loss 2.4467\n",
      "step 43620: train loss 2.3872, val loss 2.3017\n",
      "step 43630: train loss 2.4002, val loss 2.4020\n",
      "step 43640: train loss 2.3194, val loss 2.3259\n",
      "step 43650: train loss 2.4910, val loss 2.3231\n",
      "step 43660: train loss 2.3940, val loss 2.3679\n",
      "step 43670: train loss 2.4009, val loss 2.3827\n",
      "step 43680: train loss 2.3201, val loss 2.2837\n",
      "step 43690: train loss 2.4511, val loss 2.3474\n",
      "step 43700: train loss 2.4012, val loss 2.3535\n",
      "step 43710: train loss 2.4572, val loss 2.3987\n",
      "step 43720: train loss 2.3806, val loss 2.3631\n",
      "step 43730: train loss 2.3770, val loss 2.2982\n",
      "step 43740: train loss 2.3599, val loss 2.3362\n",
      "step 43750: train loss 2.3550, val loss 2.3780\n",
      "step 43760: train loss 2.3529, val loss 2.3589\n",
      "step 43770: train loss 2.3904, val loss 2.2647\n",
      "step 43780: train loss 2.4112, val loss 2.3366\n",
      "step 43790: train loss 2.3905, val loss 2.3760\n",
      "step 43800: train loss 2.3973, val loss 2.3454\n",
      "step 43810: train loss 2.3982, val loss 2.4392\n",
      "step 43820: train loss 2.3538, val loss 2.3058\n",
      "step 43830: train loss 2.3916, val loss 2.3378\n",
      "step 43840: train loss 2.4470, val loss 2.3558\n",
      "step 43850: train loss 2.3739, val loss 2.4480\n",
      "step 43860: train loss 2.3556, val loss 2.4176\n",
      "step 43870: train loss 2.3593, val loss 2.3081\n",
      "step 43880: train loss 2.3939, val loss 2.3626\n",
      "step 43890: train loss 2.3085, val loss 2.3693\n",
      "step 43900: train loss 2.3239, val loss 2.3839\n",
      "step 43910: train loss 2.3107, val loss 2.4244\n",
      "step 43920: train loss 2.3706, val loss 2.3453\n",
      "step 43930: train loss 2.3725, val loss 2.3620\n",
      "step 43940: train loss 2.4199, val loss 2.4199\n",
      "step 43950: train loss 2.4258, val loss 2.3315\n",
      "step 43960: train loss 2.3916, val loss 2.4331\n",
      "step 43970: train loss 2.3568, val loss 2.4227\n",
      "step 43980: train loss 2.3403, val loss 2.3776\n",
      "step 43990: train loss 2.4172, val loss 2.4487\n",
      "step 44000: train loss 2.4336, val loss 2.3519\n",
      "step 44010: train loss 2.3865, val loss 2.3848\n",
      "step 44020: train loss 2.4372, val loss 2.3919\n",
      "step 44030: train loss 2.3810, val loss 2.3062\n",
      "step 44040: train loss 2.3312, val loss 2.3448\n",
      "step 44050: train loss 2.4643, val loss 2.3574\n",
      "step 44060: train loss 2.3902, val loss 2.3452\n",
      "step 44070: train loss 2.3769, val loss 2.3702\n",
      "step 44080: train loss 2.4289, val loss 2.3574\n",
      "step 44090: train loss 2.3487, val loss 2.3584\n",
      "step 44100: train loss 2.3835, val loss 2.3755\n",
      "step 44110: train loss 2.3705, val loss 2.3004\n",
      "step 44120: train loss 2.2928, val loss 2.3899\n",
      "step 44130: train loss 2.3512, val loss 2.4197\n",
      "step 44140: train loss 2.3952, val loss 2.3564\n",
      "step 44150: train loss 2.4366, val loss 2.3326\n",
      "step 44160: train loss 2.2896, val loss 2.3987\n",
      "step 44170: train loss 2.3800, val loss 2.3641\n",
      "step 44180: train loss 2.4656, val loss 2.4365\n",
      "step 44190: train loss 2.3816, val loss 2.3542\n",
      "step 44200: train loss 2.3613, val loss 2.3596\n",
      "step 44210: train loss 2.3157, val loss 2.3232\n",
      "step 44220: train loss 2.4183, val loss 2.3456\n",
      "step 44230: train loss 2.3442, val loss 2.2999\n",
      "step 44240: train loss 2.3402, val loss 2.4130\n",
      "step 44250: train loss 2.4891, val loss 2.4313\n",
      "step 44260: train loss 2.3815, val loss 2.4028\n",
      "step 44270: train loss 2.4192, val loss 2.3464\n",
      "step 44280: train loss 2.3224, val loss 2.3661\n",
      "step 44290: train loss 2.4463, val loss 2.3933\n",
      "step 44300: train loss 2.3229, val loss 2.3333\n",
      "step 44310: train loss 2.3794, val loss 2.4104\n",
      "step 44320: train loss 2.4080, val loss 2.3646\n",
      "step 44330: train loss 2.3639, val loss 2.3623\n",
      "step 44340: train loss 2.3144, val loss 2.4182\n",
      "step 44350: train loss 2.4178, val loss 2.3709\n",
      "step 44360: train loss 2.3159, val loss 2.4238\n",
      "step 44370: train loss 2.4193, val loss 2.3743\n",
      "step 44380: train loss 2.3539, val loss 2.3783\n",
      "step 44390: train loss 2.4118, val loss 2.3966\n",
      "step 44400: train loss 2.3935, val loss 2.3563\n",
      "step 44410: train loss 2.3541, val loss 2.4617\n",
      "step 44420: train loss 2.4153, val loss 2.4020\n",
      "step 44430: train loss 2.4179, val loss 2.4292\n",
      "step 44440: train loss 2.3574, val loss 2.3607\n",
      "step 44450: train loss 2.3645, val loss 2.5071\n",
      "step 44460: train loss 2.3268, val loss 2.3739\n",
      "step 44470: train loss 2.2680, val loss 2.3741\n",
      "step 44480: train loss 2.3631, val loss 2.3665\n",
      "step 44490: train loss 2.3518, val loss 2.3256\n",
      "step 44500: train loss 2.3955, val loss 2.3416\n",
      "step 44510: train loss 2.3649, val loss 2.4633\n",
      "step 44520: train loss 2.3470, val loss 2.3634\n",
      "step 44530: train loss 2.2731, val loss 2.3813\n",
      "step 44540: train loss 2.4508, val loss 2.2614\n",
      "step 44550: train loss 2.4099, val loss 2.3579\n",
      "step 44560: train loss 2.3396, val loss 2.3163\n",
      "step 44570: train loss 2.3714, val loss 2.4217\n",
      "step 44580: train loss 2.3799, val loss 2.3359\n",
      "step 44590: train loss 2.3967, val loss 2.4001\n",
      "step 44600: train loss 2.3304, val loss 2.3875\n",
      "step 44610: train loss 2.3091, val loss 2.3466\n",
      "step 44620: train loss 2.4188, val loss 2.3837\n",
      "step 44630: train loss 2.3997, val loss 2.4119\n",
      "step 44640: train loss 2.3593, val loss 2.3163\n",
      "step 44650: train loss 2.3656, val loss 2.3392\n",
      "step 44660: train loss 2.3901, val loss 2.4656\n",
      "step 44670: train loss 2.4276, val loss 2.3780\n",
      "step 44680: train loss 2.3905, val loss 2.4009\n",
      "step 44690: train loss 2.4613, val loss 2.3744\n",
      "step 44700: train loss 2.3459, val loss 2.4122\n",
      "step 44710: train loss 2.3614, val loss 2.3354\n",
      "step 44720: train loss 2.2825, val loss 2.3932\n",
      "step 44730: train loss 2.4041, val loss 2.2898\n",
      "step 44740: train loss 2.4179, val loss 2.4513\n",
      "step 44750: train loss 2.3685, val loss 2.3566\n",
      "step 44760: train loss 2.3943, val loss 2.3676\n",
      "step 44770: train loss 2.3505, val loss 2.4281\n",
      "step 44780: train loss 2.3712, val loss 2.3422\n",
      "step 44790: train loss 2.3644, val loss 2.4038\n",
      "step 44800: train loss 2.4043, val loss 2.4149\n",
      "step 44810: train loss 2.3674, val loss 2.4084\n",
      "step 44820: train loss 2.5199, val loss 2.3877\n",
      "step 44830: train loss 2.3101, val loss 2.3173\n",
      "step 44840: train loss 2.3982, val loss 2.2960\n",
      "step 44850: train loss 2.3788, val loss 2.4801\n",
      "step 44860: train loss 2.4009, val loss 2.3830\n",
      "step 44870: train loss 2.3888, val loss 2.3408\n",
      "step 44880: train loss 2.3196, val loss 2.3818\n",
      "step 44890: train loss 2.4054, val loss 2.3050\n",
      "step 44900: train loss 2.3873, val loss 2.3974\n",
      "step 44910: train loss 2.4262, val loss 2.3449\n",
      "step 44920: train loss 2.3297, val loss 2.4924\n",
      "step 44930: train loss 2.4148, val loss 2.4260\n",
      "step 44940: train loss 2.3686, val loss 2.3229\n",
      "step 44950: train loss 2.3999, val loss 2.3587\n",
      "step 44960: train loss 2.4592, val loss 2.3451\n",
      "step 44970: train loss 2.3182, val loss 2.3159\n",
      "step 44980: train loss 2.2564, val loss 2.4140\n",
      "step 44990: train loss 2.4296, val loss 2.3195\n",
      "step 45000: train loss 2.3723, val loss 2.3519\n",
      "step 45010: train loss 2.3918, val loss 2.2893\n",
      "step 45020: train loss 2.4318, val loss 2.4160\n",
      "step 45030: train loss 2.3224, val loss 2.4253\n",
      "step 45040: train loss 2.3559, val loss 2.4573\n",
      "step 45050: train loss 2.3942, val loss 2.3845\n",
      "step 45060: train loss 2.4742, val loss 2.4033\n",
      "step 45070: train loss 2.3603, val loss 2.3303\n",
      "step 45080: train loss 2.3771, val loss 2.3271\n",
      "step 45090: train loss 2.3721, val loss 2.3163\n",
      "step 45100: train loss 2.3469, val loss 2.4244\n",
      "step 45110: train loss 2.3669, val loss 2.3229\n",
      "step 45120: train loss 2.3356, val loss 2.3925\n",
      "step 45130: train loss 2.4206, val loss 2.3845\n",
      "step 45140: train loss 2.4135, val loss 2.3422\n",
      "step 45150: train loss 2.3648, val loss 2.2709\n",
      "step 45160: train loss 2.4141, val loss 2.3136\n",
      "step 45170: train loss 2.3895, val loss 2.3527\n",
      "step 45180: train loss 2.4242, val loss 2.4745\n",
      "step 45190: train loss 2.3406, val loss 2.3853\n",
      "step 45200: train loss 2.3679, val loss 2.2923\n",
      "step 45210: train loss 2.4539, val loss 2.3253\n",
      "step 45220: train loss 2.4077, val loss 2.4427\n",
      "step 45230: train loss 2.4823, val loss 2.3841\n",
      "step 45240: train loss 2.4350, val loss 2.3875\n",
      "step 45250: train loss 2.3793, val loss 2.3888\n",
      "step 45260: train loss 2.4176, val loss 2.3922\n",
      "step 45270: train loss 2.3672, val loss 2.3829\n",
      "step 45280: train loss 2.3636, val loss 2.3599\n",
      "step 45290: train loss 2.4372, val loss 2.3243\n",
      "step 45300: train loss 2.4450, val loss 2.3618\n",
      "step 45310: train loss 2.3822, val loss 2.3785\n",
      "step 45320: train loss 2.3850, val loss 2.3000\n",
      "step 45330: train loss 2.3970, val loss 2.4764\n",
      "step 45340: train loss 2.3361, val loss 2.2516\n",
      "step 45350: train loss 2.3891, val loss 2.3745\n",
      "step 45360: train loss 2.4754, val loss 2.3525\n",
      "step 45370: train loss 2.3637, val loss 2.4478\n",
      "step 45380: train loss 2.3619, val loss 2.3398\n",
      "step 45390: train loss 2.4354, val loss 2.3553\n",
      "step 45400: train loss 2.3771, val loss 2.4077\n",
      "step 45410: train loss 2.3290, val loss 2.3577\n",
      "step 45420: train loss 2.3786, val loss 2.3649\n",
      "step 45430: train loss 2.3876, val loss 2.4011\n",
      "step 45440: train loss 2.3812, val loss 2.3240\n",
      "step 45450: train loss 2.4329, val loss 2.4320\n",
      "step 45460: train loss 2.3845, val loss 2.3554\n",
      "step 45470: train loss 2.3639, val loss 2.3717\n",
      "step 45480: train loss 2.4623, val loss 2.3886\n",
      "step 45490: train loss 2.4279, val loss 2.2876\n",
      "step 45500: train loss 2.3378, val loss 2.3670\n",
      "step 45510: train loss 2.3698, val loss 2.4112\n",
      "step 45520: train loss 2.4078, val loss 2.3542\n",
      "step 45530: train loss 2.3723, val loss 2.3641\n",
      "step 45540: train loss 2.3892, val loss 2.3738\n",
      "step 45550: train loss 2.3403, val loss 2.4356\n",
      "step 45560: train loss 2.3948, val loss 2.4687\n",
      "step 45570: train loss 2.3402, val loss 2.3136\n",
      "step 45580: train loss 2.3365, val loss 2.4281\n",
      "step 45590: train loss 2.3735, val loss 2.3818\n",
      "step 45600: train loss 2.3824, val loss 2.4285\n",
      "step 45610: train loss 2.3348, val loss 2.4252\n",
      "step 45620: train loss 2.4124, val loss 2.3167\n",
      "step 45630: train loss 2.3630, val loss 2.3969\n",
      "step 45640: train loss 2.3199, val loss 2.3924\n",
      "step 45650: train loss 2.3190, val loss 2.3294\n",
      "step 45660: train loss 2.3379, val loss 2.3226\n",
      "step 45670: train loss 2.4819, val loss 2.3595\n",
      "step 45680: train loss 2.3235, val loss 2.4361\n",
      "step 45690: train loss 2.3023, val loss 2.3361\n",
      "step 45700: train loss 2.3679, val loss 2.3753\n",
      "step 45710: train loss 2.4349, val loss 2.3465\n",
      "step 45720: train loss 2.3915, val loss 2.3438\n",
      "step 45730: train loss 2.3635, val loss 2.3261\n",
      "step 45740: train loss 2.4259, val loss 2.4097\n",
      "step 45750: train loss 2.3745, val loss 2.3788\n",
      "step 45760: train loss 2.4151, val loss 2.3856\n",
      "step 45770: train loss 2.3880, val loss 2.2982\n",
      "step 45780: train loss 2.4179, val loss 2.3395\n",
      "step 45790: train loss 2.3812, val loss 2.3395\n",
      "step 45800: train loss 2.4427, val loss 2.3417\n",
      "step 45810: train loss 2.3959, val loss 2.3341\n",
      "step 45820: train loss 2.4455, val loss 2.3593\n",
      "step 45830: train loss 2.3434, val loss 2.3753\n",
      "step 45840: train loss 2.3209, val loss 2.3276\n",
      "step 45850: train loss 2.3730, val loss 2.3609\n",
      "step 45860: train loss 2.4558, val loss 2.3722\n",
      "step 45870: train loss 2.3782, val loss 2.2888\n",
      "step 45880: train loss 2.3859, val loss 2.4246\n",
      "step 45890: train loss 2.2978, val loss 2.3238\n",
      "step 45900: train loss 2.4555, val loss 2.4731\n",
      "step 45910: train loss 2.3798, val loss 2.3695\n",
      "step 45920: train loss 2.3965, val loss 2.4016\n",
      "step 45930: train loss 2.3577, val loss 2.2234\n",
      "step 45940: train loss 2.3580, val loss 2.3776\n",
      "step 45950: train loss 2.3352, val loss 2.3823\n",
      "step 45960: train loss 2.4196, val loss 2.3177\n",
      "step 45970: train loss 2.3941, val loss 2.4050\n",
      "step 45980: train loss 2.3577, val loss 2.4454\n",
      "step 45990: train loss 2.4148, val loss 2.4231\n",
      "step 46000: train loss 2.3348, val loss 2.3803\n",
      "step 46010: train loss 2.3556, val loss 2.4011\n",
      "step 46020: train loss 2.3867, val loss 2.3500\n",
      "step 46030: train loss 2.3241, val loss 2.3579\n",
      "step 46040: train loss 2.3619, val loss 2.4289\n",
      "step 46050: train loss 2.4213, val loss 2.4239\n",
      "step 46060: train loss 2.3612, val loss 2.3856\n",
      "step 46070: train loss 2.4137, val loss 2.3793\n",
      "step 46080: train loss 2.3952, val loss 2.3968\n",
      "step 46090: train loss 2.4138, val loss 2.3111\n",
      "step 46100: train loss 2.4243, val loss 2.3320\n",
      "step 46110: train loss 2.3211, val loss 2.3878\n",
      "step 46120: train loss 2.3787, val loss 2.3425\n",
      "step 46130: train loss 2.4088, val loss 2.3757\n",
      "step 46140: train loss 2.4560, val loss 2.3726\n",
      "step 46150: train loss 2.4457, val loss 2.4549\n",
      "step 46160: train loss 2.4421, val loss 2.4195\n",
      "step 46170: train loss 2.3691, val loss 2.3160\n",
      "step 46180: train loss 2.4620, val loss 2.3705\n",
      "step 46190: train loss 2.4116, val loss 2.4049\n",
      "step 46200: train loss 2.3333, val loss 2.3388\n",
      "step 46210: train loss 2.3395, val loss 2.2870\n",
      "step 46220: train loss 2.3148, val loss 2.3996\n",
      "step 46230: train loss 2.3874, val loss 2.3123\n",
      "step 46240: train loss 2.2951, val loss 2.3490\n",
      "step 46250: train loss 2.4190, val loss 2.4096\n",
      "step 46260: train loss 2.3449, val loss 2.3611\n",
      "step 46270: train loss 2.3844, val loss 2.3372\n",
      "step 46280: train loss 2.3476, val loss 2.3333\n",
      "step 46290: train loss 2.3067, val loss 2.3407\n",
      "step 46300: train loss 2.4297, val loss 2.4138\n",
      "step 46310: train loss 2.4278, val loss 2.3657\n",
      "step 46320: train loss 2.3889, val loss 2.4281\n",
      "step 46330: train loss 2.3727, val loss 2.4636\n",
      "step 46340: train loss 2.4528, val loss 2.3452\n",
      "step 46350: train loss 2.4221, val loss 2.4115\n",
      "step 46360: train loss 2.3904, val loss 2.3391\n",
      "step 46370: train loss 2.3854, val loss 2.3518\n",
      "step 46380: train loss 2.3734, val loss 2.4392\n",
      "step 46390: train loss 2.3450, val loss 2.3770\n",
      "step 46400: train loss 2.4103, val loss 2.3704\n",
      "step 46410: train loss 2.4193, val loss 2.3780\n",
      "step 46420: train loss 2.3326, val loss 2.3336\n",
      "step 46430: train loss 2.2545, val loss 2.4079\n",
      "step 46440: train loss 2.3983, val loss 2.4187\n",
      "step 46450: train loss 2.3611, val loss 2.3557\n",
      "step 46460: train loss 2.4527, val loss 2.3290\n",
      "step 46470: train loss 2.3857, val loss 2.3515\n",
      "step 46480: train loss 2.4323, val loss 2.3917\n",
      "step 46490: train loss 2.3788, val loss 2.3529\n",
      "step 46500: train loss 2.3575, val loss 2.3458\n",
      "step 46510: train loss 2.3856, val loss 2.3331\n",
      "step 46520: train loss 2.3814, val loss 2.3297\n",
      "step 46530: train loss 2.3735, val loss 2.3403\n",
      "step 46540: train loss 2.4250, val loss 2.4257\n",
      "step 46550: train loss 2.3824, val loss 2.2972\n",
      "step 46560: train loss 2.3353, val loss 2.2962\n",
      "step 46570: train loss 2.2785, val loss 2.4407\n",
      "step 46580: train loss 2.3928, val loss 2.3872\n",
      "step 46590: train loss 2.3344, val loss 2.3617\n",
      "step 46600: train loss 2.4242, val loss 2.3754\n",
      "step 46610: train loss 2.3645, val loss 2.3516\n",
      "step 46620: train loss 2.3508, val loss 2.3468\n",
      "step 46630: train loss 2.4349, val loss 2.3673\n",
      "step 46640: train loss 2.3277, val loss 2.3474\n",
      "step 46650: train loss 2.4388, val loss 2.3932\n",
      "step 46660: train loss 2.5019, val loss 2.3116\n",
      "step 46670: train loss 2.3361, val loss 2.4206\n",
      "step 46680: train loss 2.4042, val loss 2.2732\n",
      "step 46690: train loss 2.3205, val loss 2.3619\n",
      "step 46700: train loss 2.4012, val loss 2.3653\n",
      "step 46710: train loss 2.4117, val loss 2.3924\n",
      "step 46720: train loss 2.3428, val loss 2.3518\n",
      "step 46730: train loss 2.3355, val loss 2.3582\n",
      "step 46740: train loss 2.3418, val loss 2.3860\n",
      "step 46750: train loss 2.4527, val loss 2.3092\n",
      "step 46760: train loss 2.3913, val loss 2.3666\n",
      "step 46770: train loss 2.3403, val loss 2.4008\n",
      "step 46780: train loss 2.3486, val loss 2.3301\n",
      "step 46790: train loss 2.3513, val loss 2.3492\n",
      "step 46800: train loss 2.3953, val loss 2.3497\n",
      "step 46810: train loss 2.4273, val loss 2.3025\n",
      "step 46820: train loss 2.3749, val loss 2.3422\n",
      "step 46830: train loss 2.3502, val loss 2.3087\n",
      "step 46840: train loss 2.4054, val loss 2.3946\n",
      "step 46850: train loss 2.3529, val loss 2.4164\n",
      "step 46860: train loss 2.3586, val loss 2.4068\n",
      "step 46870: train loss 2.3834, val loss 2.2903\n",
      "step 46880: train loss 2.4758, val loss 2.3439\n",
      "step 46890: train loss 2.2781, val loss 2.3831\n",
      "step 46900: train loss 2.3901, val loss 2.3071\n",
      "step 46910: train loss 2.3547, val loss 2.4150\n",
      "step 46920: train loss 2.3899, val loss 2.3851\n",
      "step 46930: train loss 2.3515, val loss 2.4311\n",
      "step 46940: train loss 2.4059, val loss 2.3527\n",
      "step 46950: train loss 2.4326, val loss 2.3859\n",
      "step 46960: train loss 2.4473, val loss 2.3523\n",
      "step 46970: train loss 2.3330, val loss 2.3812\n",
      "step 46980: train loss 2.4554, val loss 2.3922\n",
      "step 46990: train loss 2.3639, val loss 2.3386\n",
      "step 47000: train loss 2.3108, val loss 2.3734\n",
      "step 47010: train loss 2.3954, val loss 2.3338\n",
      "step 47020: train loss 2.3544, val loss 2.4270\n",
      "step 47030: train loss 2.3858, val loss 2.2845\n",
      "step 47040: train loss 2.3936, val loss 2.3826\n",
      "step 47050: train loss 2.3751, val loss 2.3285\n",
      "step 47060: train loss 2.3726, val loss 2.3534\n",
      "step 47070: train loss 2.4361, val loss 2.3816\n",
      "step 47080: train loss 2.3840, val loss 2.3244\n",
      "step 47090: train loss 2.5098, val loss 2.4053\n",
      "step 47100: train loss 2.3361, val loss 2.3909\n",
      "step 47110: train loss 2.4212, val loss 2.3709\n",
      "step 47120: train loss 2.3297, val loss 2.3610\n",
      "step 47130: train loss 2.3644, val loss 2.3824\n",
      "step 47140: train loss 2.3920, val loss 2.4214\n",
      "step 47150: train loss 2.3668, val loss 2.3752\n",
      "step 47160: train loss 2.4535, val loss 2.3723\n",
      "step 47170: train loss 2.3214, val loss 2.3909\n",
      "step 47180: train loss 2.4159, val loss 2.3922\n",
      "step 47190: train loss 2.3407, val loss 2.3371\n",
      "step 47200: train loss 2.4962, val loss 2.3442\n",
      "step 47210: train loss 2.3437, val loss 2.3057\n",
      "step 47220: train loss 2.4062, val loss 2.3083\n",
      "step 47230: train loss 2.3671, val loss 2.3486\n",
      "step 47240: train loss 2.4449, val loss 2.3471\n",
      "step 47250: train loss 2.3907, val loss 2.4013\n",
      "step 47260: train loss 2.3936, val loss 2.3688\n",
      "step 47270: train loss 2.4653, val loss 2.3689\n",
      "step 47280: train loss 2.3720, val loss 2.2841\n",
      "step 47290: train loss 2.3194, val loss 2.3347\n",
      "step 47300: train loss 2.3385, val loss 2.3583\n",
      "step 47310: train loss 2.3589, val loss 2.3453\n",
      "step 47320: train loss 2.3759, val loss 2.3740\n",
      "step 47330: train loss 2.3769, val loss 2.3608\n",
      "step 47340: train loss 2.4082, val loss 2.4058\n",
      "step 47350: train loss 2.3637, val loss 2.3983\n",
      "step 47360: train loss 2.3973, val loss 2.3653\n",
      "step 47370: train loss 2.4087, val loss 2.4110\n",
      "step 47380: train loss 2.3672, val loss 2.4016\n",
      "step 47390: train loss 2.4056, val loss 2.3274\n",
      "step 47400: train loss 2.3712, val loss 2.3055\n",
      "step 47410: train loss 2.4615, val loss 2.3284\n",
      "step 47420: train loss 2.3324, val loss 2.3802\n",
      "step 47430: train loss 2.4112, val loss 2.4310\n",
      "step 47440: train loss 2.3569, val loss 2.3462\n",
      "step 47450: train loss 2.3423, val loss 2.3231\n",
      "step 47460: train loss 2.4172, val loss 2.2654\n",
      "step 47470: train loss 2.3641, val loss 2.4844\n",
      "step 47480: train loss 2.3698, val loss 2.3057\n",
      "step 47490: train loss 2.3404, val loss 2.3908\n",
      "step 47500: train loss 2.4474, val loss 2.3402\n",
      "step 47510: train loss 2.3322, val loss 2.3388\n",
      "step 47520: train loss 2.3331, val loss 2.4059\n",
      "step 47530: train loss 2.3802, val loss 2.3730\n",
      "step 47540: train loss 2.4377, val loss 2.4335\n",
      "step 47550: train loss 2.4029, val loss 2.4307\n",
      "step 47560: train loss 2.4517, val loss 2.4721\n",
      "step 47570: train loss 2.3752, val loss 2.3460\n",
      "step 47580: train loss 2.4249, val loss 2.3960\n",
      "step 47590: train loss 2.3496, val loss 2.3773\n",
      "step 47600: train loss 2.4046, val loss 2.3784\n",
      "step 47610: train loss 2.3379, val loss 2.3801\n",
      "step 47620: train loss 2.2854, val loss 2.3046\n",
      "step 47630: train loss 2.4284, val loss 2.3409\n",
      "step 47640: train loss 2.3573, val loss 2.3322\n",
      "step 47650: train loss 2.3123, val loss 2.2873\n",
      "step 47660: train loss 2.3377, val loss 2.3317\n",
      "step 47670: train loss 2.3805, val loss 2.3278\n",
      "step 47680: train loss 2.3868, val loss 2.3862\n",
      "step 47690: train loss 2.4018, val loss 2.2995\n",
      "step 47700: train loss 2.3648, val loss 2.3906\n",
      "step 47710: train loss 2.3582, val loss 2.3645\n",
      "step 47720: train loss 2.4062, val loss 2.3420\n",
      "step 47730: train loss 2.3112, val loss 2.4265\n",
      "step 47740: train loss 2.3223, val loss 2.4245\n",
      "step 47750: train loss 2.3880, val loss 2.3813\n",
      "step 47760: train loss 2.4135, val loss 2.3408\n",
      "step 47770: train loss 2.3674, val loss 2.3549\n",
      "step 47780: train loss 2.3359, val loss 2.4106\n",
      "step 47790: train loss 2.4591, val loss 2.2965\n",
      "step 47800: train loss 2.3578, val loss 2.3293\n",
      "step 47810: train loss 2.3955, val loss 2.4764\n",
      "step 47820: train loss 2.3790, val loss 2.4068\n",
      "step 47830: train loss 2.4350, val loss 2.3421\n",
      "step 47840: train loss 2.3926, val loss 2.2974\n",
      "step 47850: train loss 2.3040, val loss 2.3847\n",
      "step 47860: train loss 2.4180, val loss 2.3703\n",
      "step 47870: train loss 2.3244, val loss 2.3536\n",
      "step 47880: train loss 2.3765, val loss 2.4362\n",
      "step 47890: train loss 2.3972, val loss 2.3509\n",
      "step 47900: train loss 2.4742, val loss 2.3923\n",
      "step 47910: train loss 2.3798, val loss 2.3573\n",
      "step 47920: train loss 2.4295, val loss 2.4047\n",
      "step 47930: train loss 2.3203, val loss 2.3722\n",
      "step 47940: train loss 2.3674, val loss 2.3964\n",
      "step 47950: train loss 2.3738, val loss 2.4026\n",
      "step 47960: train loss 2.4259, val loss 2.3767\n",
      "step 47970: train loss 2.3998, val loss 2.3424\n",
      "step 47980: train loss 2.3573, val loss 2.4201\n",
      "step 47990: train loss 2.3953, val loss 2.3531\n",
      "step 48000: train loss 2.3306, val loss 2.3585\n",
      "step 48010: train loss 2.3781, val loss 2.3789\n",
      "step 48020: train loss 2.3870, val loss 2.3746\n",
      "step 48030: train loss 2.3756, val loss 2.4626\n",
      "step 48040: train loss 2.3622, val loss 2.4471\n",
      "step 48050: train loss 2.3141, val loss 2.4225\n",
      "step 48060: train loss 2.3959, val loss 2.3415\n",
      "step 48070: train loss 2.3497, val loss 2.4488\n",
      "step 48080: train loss 2.3222, val loss 2.3746\n",
      "step 48090: train loss 2.4307, val loss 2.4618\n",
      "step 48100: train loss 2.3322, val loss 2.3772\n",
      "step 48110: train loss 2.2774, val loss 2.3387\n",
      "step 48120: train loss 2.3024, val loss 2.3954\n",
      "step 48130: train loss 2.2927, val loss 2.3716\n",
      "step 48140: train loss 2.4174, val loss 2.4454\n",
      "step 48150: train loss 2.3852, val loss 2.3316\n",
      "step 48160: train loss 2.3192, val loss 2.2728\n",
      "step 48170: train loss 2.3956, val loss 2.4171\n",
      "step 48180: train loss 2.4602, val loss 2.4030\n",
      "step 48190: train loss 2.3177, val loss 2.3668\n",
      "step 48200: train loss 2.3930, val loss 2.3727\n",
      "step 48210: train loss 2.3890, val loss 2.4099\n",
      "step 48220: train loss 2.3773, val loss 2.3488\n",
      "step 48230: train loss 2.3854, val loss 2.3056\n",
      "step 48240: train loss 2.4020, val loss 2.3612\n",
      "step 48250: train loss 2.3915, val loss 2.2985\n",
      "step 48260: train loss 2.3427, val loss 2.3497\n",
      "step 48270: train loss 2.4021, val loss 2.3966\n",
      "step 48280: train loss 2.4507, val loss 2.4627\n",
      "step 48290: train loss 2.3879, val loss 2.3612\n",
      "step 48300: train loss 2.4482, val loss 2.3608\n",
      "step 48310: train loss 2.3848, val loss 2.4330\n",
      "step 48320: train loss 2.4518, val loss 2.3458\n",
      "step 48330: train loss 2.4102, val loss 2.3630\n",
      "step 48340: train loss 2.3407, val loss 2.3888\n",
      "step 48350: train loss 2.3851, val loss 2.4445\n",
      "step 48360: train loss 2.3999, val loss 2.3049\n",
      "step 48370: train loss 2.3822, val loss 2.3154\n",
      "step 48380: train loss 2.3946, val loss 2.3130\n",
      "step 48390: train loss 2.4207, val loss 2.3817\n",
      "step 48400: train loss 2.4480, val loss 2.3043\n",
      "step 48410: train loss 2.3845, val loss 2.3597\n",
      "step 48420: train loss 2.3781, val loss 2.3880\n",
      "step 48430: train loss 2.3838, val loss 2.3383\n",
      "step 48440: train loss 2.3687, val loss 2.3929\n",
      "step 48450: train loss 2.3264, val loss 2.3439\n",
      "step 48460: train loss 2.3400, val loss 2.3681\n",
      "step 48470: train loss 2.3615, val loss 2.2782\n",
      "step 48480: train loss 2.3261, val loss 2.3655\n",
      "step 48490: train loss 2.4113, val loss 2.4376\n",
      "step 48500: train loss 2.4314, val loss 2.2929\n",
      "step 48510: train loss 2.3165, val loss 2.3512\n",
      "step 48520: train loss 2.4236, val loss 2.3390\n",
      "step 48530: train loss 2.3666, val loss 2.3440\n",
      "step 48540: train loss 2.3766, val loss 2.3463\n",
      "step 48550: train loss 2.4012, val loss 2.4149\n",
      "step 48560: train loss 2.4280, val loss 2.4205\n",
      "step 48570: train loss 2.4327, val loss 2.4895\n",
      "step 48580: train loss 2.4120, val loss 2.3616\n",
      "step 48590: train loss 2.3550, val loss 2.3802\n",
      "step 48600: train loss 2.3521, val loss 2.4326\n",
      "step 48610: train loss 2.3284, val loss 2.4071\n",
      "step 48620: train loss 2.3765, val loss 2.4557\n",
      "step 48630: train loss 2.3685, val loss 2.3608\n",
      "step 48640: train loss 2.3974, val loss 2.3538\n",
      "step 48650: train loss 2.4183, val loss 2.3665\n",
      "step 48660: train loss 2.3885, val loss 2.3601\n",
      "step 48670: train loss 2.2766, val loss 2.4465\n",
      "step 48680: train loss 2.3336, val loss 2.3345\n",
      "step 48690: train loss 2.4072, val loss 2.3645\n",
      "step 48700: train loss 2.3362, val loss 2.4241\n",
      "step 48710: train loss 2.4050, val loss 2.3763\n",
      "step 48720: train loss 2.3629, val loss 2.3962\n",
      "step 48730: train loss 2.3953, val loss 2.4240\n",
      "step 48740: train loss 2.3673, val loss 2.3704\n",
      "step 48750: train loss 2.4221, val loss 2.3671\n",
      "step 48760: train loss 2.3987, val loss 2.3844\n",
      "step 48770: train loss 2.3521, val loss 2.3499\n",
      "step 48780: train loss 2.3775, val loss 2.3517\n",
      "step 48790: train loss 2.4269, val loss 2.3105\n",
      "step 48800: train loss 2.3352, val loss 2.3416\n",
      "step 48810: train loss 2.3689, val loss 2.4173\n",
      "step 48820: train loss 2.3618, val loss 2.3251\n",
      "step 48830: train loss 2.3374, val loss 2.3539\n",
      "step 48840: train loss 2.4037, val loss 2.3170\n",
      "step 48850: train loss 2.3639, val loss 2.3006\n",
      "step 48860: train loss 2.3328, val loss 2.4429\n",
      "step 48870: train loss 2.3616, val loss 2.3068\n",
      "step 48880: train loss 2.3841, val loss 2.3692\n",
      "step 48890: train loss 2.3483, val loss 2.4588\n",
      "step 48900: train loss 2.3725, val loss 2.3692\n",
      "step 48910: train loss 2.3756, val loss 2.3595\n",
      "step 48920: train loss 2.4013, val loss 2.3675\n",
      "step 48930: train loss 2.3817, val loss 2.3949\n",
      "step 48940: train loss 2.3612, val loss 2.3090\n",
      "step 48950: train loss 2.3675, val loss 2.3766\n",
      "step 48960: train loss 2.3973, val loss 2.3339\n",
      "step 48970: train loss 2.3511, val loss 2.3708\n",
      "step 48980: train loss 2.3122, val loss 2.2680\n",
      "step 48990: train loss 2.3255, val loss 2.3461\n",
      "step 49000: train loss 2.4261, val loss 2.3232\n",
      "step 49010: train loss 2.3565, val loss 2.3796\n",
      "step 49020: train loss 2.3601, val loss 2.3996\n",
      "step 49030: train loss 2.3718, val loss 2.2974\n",
      "step 49040: train loss 2.3996, val loss 2.3400\n",
      "step 49050: train loss 2.3787, val loss 2.2700\n",
      "step 49060: train loss 2.4325, val loss 2.3351\n",
      "step 49070: train loss 2.3992, val loss 2.3331\n",
      "step 49080: train loss 2.4869, val loss 2.3324\n",
      "step 49090: train loss 2.4513, val loss 2.2977\n",
      "step 49100: train loss 2.3700, val loss 2.2447\n",
      "step 49110: train loss 2.3851, val loss 2.3981\n",
      "step 49120: train loss 2.3259, val loss 2.4245\n",
      "step 49130: train loss 2.3090, val loss 2.3579\n",
      "step 49140: train loss 2.3666, val loss 2.3404\n",
      "step 49150: train loss 2.4296, val loss 2.3612\n",
      "step 49160: train loss 2.3853, val loss 2.3212\n",
      "step 49170: train loss 2.3428, val loss 2.3578\n",
      "step 49180: train loss 2.4610, val loss 2.3822\n",
      "step 49190: train loss 2.3939, val loss 2.2950\n",
      "step 49200: train loss 2.3631, val loss 2.3967\n",
      "step 49210: train loss 2.3512, val loss 2.3631\n",
      "step 49220: train loss 2.4097, val loss 2.3855\n",
      "step 49230: train loss 2.4291, val loss 2.3357\n",
      "step 49240: train loss 2.3890, val loss 2.3796\n",
      "step 49250: train loss 2.4141, val loss 2.4686\n",
      "step 49260: train loss 2.3901, val loss 2.3342\n",
      "step 49270: train loss 2.3522, val loss 2.3971\n",
      "step 49280: train loss 2.4013, val loss 2.3444\n",
      "step 49290: train loss 2.4358, val loss 2.4162\n",
      "step 49300: train loss 2.3939, val loss 2.3584\n",
      "step 49310: train loss 2.3498, val loss 2.4113\n",
      "step 49320: train loss 2.3731, val loss 2.4079\n",
      "step 49330: train loss 2.3992, val loss 2.4161\n",
      "step 49340: train loss 2.3999, val loss 2.3693\n",
      "step 49350: train loss 2.4244, val loss 2.3675\n",
      "step 49360: train loss 2.4054, val loss 2.2929\n",
      "step 49370: train loss 2.3751, val loss 2.3846\n",
      "step 49380: train loss 2.3254, val loss 2.3828\n",
      "step 49390: train loss 2.3622, val loss 2.4705\n",
      "step 49400: train loss 2.4370, val loss 2.3258\n",
      "step 49410: train loss 2.3870, val loss 2.3193\n",
      "step 49420: train loss 2.5102, val loss 2.4228\n",
      "step 49430: train loss 2.4589, val loss 2.3976\n",
      "step 49440: train loss 2.3443, val loss 2.3306\n",
      "step 49450: train loss 2.3498, val loss 2.3681\n",
      "step 49460: train loss 2.3485, val loss 2.2766\n",
      "step 49470: train loss 2.3499, val loss 2.2774\n",
      "step 49480: train loss 2.4158, val loss 2.3760\n",
      "step 49490: train loss 2.4191, val loss 2.4705\n",
      "step 49500: train loss 2.4133, val loss 2.3300\n",
      "step 49510: train loss 2.4159, val loss 2.3773\n",
      "step 49520: train loss 2.4250, val loss 2.4082\n",
      "step 49530: train loss 2.3877, val loss 2.3574\n",
      "step 49540: train loss 2.3591, val loss 2.3713\n",
      "step 49550: train loss 2.3681, val loss 2.3483\n",
      "step 49560: train loss 2.3872, val loss 2.3859\n",
      "step 49570: train loss 2.4261, val loss 2.3697\n",
      "step 49580: train loss 2.3658, val loss 2.3357\n",
      "step 49590: train loss 2.4040, val loss 2.3190\n",
      "step 49600: train loss 2.3853, val loss 2.3318\n",
      "step 49610: train loss 2.3738, val loss 2.2807\n",
      "step 49620: train loss 2.3611, val loss 2.3042\n",
      "step 49630: train loss 2.3997, val loss 2.4129\n",
      "step 49640: train loss 2.3528, val loss 2.3664\n",
      "step 49650: train loss 2.3122, val loss 2.3946\n",
      "step 49660: train loss 2.4322, val loss 2.3284\n",
      "step 49670: train loss 2.4174, val loss 2.3117\n",
      "step 49680: train loss 2.3011, val loss 2.2992\n",
      "step 49690: train loss 2.4309, val loss 2.4136\n",
      "step 49700: train loss 2.3236, val loss 2.3524\n",
      "step 49710: train loss 2.3568, val loss 2.2919\n",
      "step 49720: train loss 2.3888, val loss 2.4088\n",
      "step 49730: train loss 2.3041, val loss 2.3792\n",
      "step 49740: train loss 2.2973, val loss 2.4588\n",
      "step 49750: train loss 2.3119, val loss 2.2633\n",
      "step 49760: train loss 2.4335, val loss 2.4257\n",
      "step 49770: train loss 2.3516, val loss 2.3845\n",
      "step 49780: train loss 2.3877, val loss 2.3991\n",
      "step 49790: train loss 2.3791, val loss 2.3549\n",
      "step 49800: train loss 2.4210, val loss 2.3255\n",
      "step 49810: train loss 2.4399, val loss 2.4461\n",
      "step 49820: train loss 2.3906, val loss 2.3494\n",
      "step 49830: train loss 2.4023, val loss 2.3482\n",
      "step 49840: train loss 2.3061, val loss 2.4827\n",
      "step 49850: train loss 2.3614, val loss 2.3030\n",
      "step 49860: train loss 2.3980, val loss 2.3346\n",
      "step 49870: train loss 2.4218, val loss 2.3718\n",
      "step 49880: train loss 2.4360, val loss 2.3720\n",
      "step 49890: train loss 2.3556, val loss 2.3542\n",
      "step 49900: train loss 2.3792, val loss 2.3939\n",
      "step 49910: train loss 2.3777, val loss 2.3713\n",
      "step 49920: train loss 2.3814, val loss 2.4145\n",
      "step 49930: train loss 2.3700, val loss 2.4087\n",
      "step 49940: train loss 2.3810, val loss 2.3619\n",
      "step 49950: train loss 2.3873, val loss 2.3573\n",
      "step 49960: train loss 2.4119, val loss 2.3451\n",
      "step 49970: train loss 2.3797, val loss 2.4004\n",
      "step 49980: train loss 2.3510, val loss 2.4307\n",
      "step 49990: train loss 2.4060, val loss 2.3441\n",
      "step 50000: train loss 2.4519, val loss 2.3543\n",
      "step 50010: train loss 2.2829, val loss 2.4457\n",
      "step 50020: train loss 2.4057, val loss 2.4526\n",
      "step 50030: train loss 2.4740, val loss 2.3454\n",
      "step 50040: train loss 2.3917, val loss 2.2808\n",
      "step 50050: train loss 2.3734, val loss 2.3562\n",
      "step 50060: train loss 2.4218, val loss 2.4293\n",
      "step 50070: train loss 2.3767, val loss 2.4295\n",
      "step 50080: train loss 2.3885, val loss 2.4042\n",
      "step 50090: train loss 2.3989, val loss 2.3377\n",
      "step 50100: train loss 2.3834, val loss 2.3491\n",
      "step 50110: train loss 2.4157, val loss 2.3141\n",
      "step 50120: train loss 2.3716, val loss 2.3868\n",
      "step 50130: train loss 2.3849, val loss 2.3473\n",
      "step 50140: train loss 2.5336, val loss 2.3863\n",
      "step 50150: train loss 2.3556, val loss 2.2605\n",
      "step 50160: train loss 2.3475, val loss 2.3767\n",
      "step 50170: train loss 2.3663, val loss 2.3754\n",
      "step 50180: train loss 2.4147, val loss 2.3608\n",
      "step 50190: train loss 2.3923, val loss 2.3279\n",
      "step 50200: train loss 2.3994, val loss 2.3318\n",
      "step 50210: train loss 2.3620, val loss 2.4222\n",
      "step 50220: train loss 2.3511, val loss 2.4067\n",
      "step 50230: train loss 2.3022, val loss 2.2635\n",
      "step 50240: train loss 2.3030, val loss 2.3729\n",
      "step 50250: train loss 2.3071, val loss 2.3790\n",
      "step 50260: train loss 2.3996, val loss 2.3331\n",
      "step 50270: train loss 2.3575, val loss 2.4010\n",
      "step 50280: train loss 2.3170, val loss 2.3814\n",
      "step 50290: train loss 2.3658, val loss 2.4072\n",
      "step 50300: train loss 2.4341, val loss 2.3214\n",
      "step 50310: train loss 2.3970, val loss 2.4092\n",
      "step 50320: train loss 2.3185, val loss 2.3946\n",
      "step 50330: train loss 2.3962, val loss 2.2850\n",
      "step 50340: train loss 2.4440, val loss 2.3314\n",
      "step 50350: train loss 2.3983, val loss 2.4422\n",
      "step 50360: train loss 2.3911, val loss 2.3840\n",
      "step 50370: train loss 2.2894, val loss 2.3496\n",
      "step 50380: train loss 2.4487, val loss 2.3769\n",
      "step 50390: train loss 2.3163, val loss 2.3290\n",
      "step 50400: train loss 2.3668, val loss 2.3523\n",
      "step 50410: train loss 2.2966, val loss 2.3551\n",
      "step 50420: train loss 2.3723, val loss 2.4578\n",
      "step 50430: train loss 2.4444, val loss 2.3118\n",
      "step 50440: train loss 2.3516, val loss 2.3582\n",
      "step 50450: train loss 2.3121, val loss 2.3685\n",
      "step 50460: train loss 2.3221, val loss 2.4190\n",
      "step 50470: train loss 2.3853, val loss 2.3905\n",
      "step 50480: train loss 2.3874, val loss 2.3520\n",
      "step 50490: train loss 2.4097, val loss 2.4615\n",
      "step 50500: train loss 2.4430, val loss 2.3875\n",
      "step 50510: train loss 2.3863, val loss 2.3408\n",
      "step 50520: train loss 2.4225, val loss 2.3268\n",
      "step 50530: train loss 2.4165, val loss 2.4043\n",
      "step 50540: train loss 2.3457, val loss 2.3660\n",
      "step 50550: train loss 2.3592, val loss 2.3665\n",
      "step 50560: train loss 2.3550, val loss 2.3836\n",
      "step 50570: train loss 2.3866, val loss 2.3656\n",
      "step 50580: train loss 2.4099, val loss 2.3374\n",
      "step 50590: train loss 2.3863, val loss 2.3176\n",
      "step 50600: train loss 2.4062, val loss 2.3521\n",
      "step 50610: train loss 2.4381, val loss 2.3583\n",
      "step 50620: train loss 2.3571, val loss 2.3597\n",
      "step 50630: train loss 2.4018, val loss 2.3204\n",
      "step 50640: train loss 2.4414, val loss 2.4131\n",
      "step 50650: train loss 2.4060, val loss 2.3182\n",
      "step 50660: train loss 2.5007, val loss 2.3552\n",
      "step 50670: train loss 2.3459, val loss 2.3857\n",
      "step 50680: train loss 2.3206, val loss 2.3844\n",
      "step 50690: train loss 2.3600, val loss 2.3684\n",
      "step 50700: train loss 2.3458, val loss 2.3849\n",
      "step 50710: train loss 2.4229, val loss 2.3045\n",
      "step 50720: train loss 2.4210, val loss 2.3313\n",
      "step 50730: train loss 2.3976, val loss 2.2716\n",
      "step 50740: train loss 2.5135, val loss 2.3798\n",
      "step 50750: train loss 2.3991, val loss 2.4236\n",
      "step 50760: train loss 2.3475, val loss 2.3853\n",
      "step 50770: train loss 2.3644, val loss 2.3951\n",
      "step 50780: train loss 2.3834, val loss 2.4274\n",
      "step 50790: train loss 2.3481, val loss 2.3693\n",
      "step 50800: train loss 2.3532, val loss 2.3671\n",
      "step 50810: train loss 2.3140, val loss 2.3140\n",
      "step 50820: train loss 2.3154, val loss 2.3270\n",
      "step 50830: train loss 2.3881, val loss 2.3754\n",
      "step 50840: train loss 2.2885, val loss 2.4091\n",
      "step 50850: train loss 2.3417, val loss 2.4254\n",
      "step 50860: train loss 2.3458, val loss 2.3239\n",
      "step 50870: train loss 2.3362, val loss 2.3181\n",
      "step 50880: train loss 2.3412, val loss 2.3517\n",
      "step 50890: train loss 2.3468, val loss 2.2962\n",
      "step 50900: train loss 2.3653, val loss 2.3619\n",
      "step 50910: train loss 2.3600, val loss 2.3339\n",
      "step 50920: train loss 2.3193, val loss 2.3467\n",
      "step 50930: train loss 2.2825, val loss 2.3220\n",
      "step 50940: train loss 2.3553, val loss 2.4421\n",
      "step 50950: train loss 2.4021, val loss 2.4066\n",
      "step 50960: train loss 2.2913, val loss 2.3097\n",
      "step 50970: train loss 2.4134, val loss 2.3355\n",
      "step 50980: train loss 2.3552, val loss 2.3557\n",
      "step 50990: train loss 2.3995, val loss 2.3125\n",
      "step 51000: train loss 2.3792, val loss 2.3732\n",
      "step 51010: train loss 2.3840, val loss 2.3683\n",
      "step 51020: train loss 2.4235, val loss 2.3890\n",
      "step 51030: train loss 2.3811, val loss 2.3979\n",
      "step 51040: train loss 2.3050, val loss 2.3786\n",
      "step 51050: train loss 2.3880, val loss 2.4418\n",
      "step 51060: train loss 2.3425, val loss 2.2876\n",
      "step 51070: train loss 2.3174, val loss 2.3321\n",
      "step 51080: train loss 2.4097, val loss 2.3637\n",
      "step 51090: train loss 2.3709, val loss 2.3855\n",
      "step 51100: train loss 2.4177, val loss 2.3779\n",
      "step 51110: train loss 2.3860, val loss 2.2917\n",
      "step 51120: train loss 2.3691, val loss 2.3461\n",
      "step 51130: train loss 2.3984, val loss 2.2736\n",
      "step 51140: train loss 2.4300, val loss 2.3735\n",
      "step 51150: train loss 2.3684, val loss 2.5019\n",
      "step 51160: train loss 2.3459, val loss 2.4239\n",
      "step 51170: train loss 2.3388, val loss 2.3260\n",
      "step 51180: train loss 2.3401, val loss 2.3229\n",
      "step 51190: train loss 2.4196, val loss 2.3149\n",
      "step 51200: train loss 2.3759, val loss 2.4232\n",
      "step 51210: train loss 2.4333, val loss 2.3359\n",
      "step 51220: train loss 2.3613, val loss 2.3004\n",
      "step 51230: train loss 2.3780, val loss 2.4024\n",
      "step 51240: train loss 2.4446, val loss 2.3973\n",
      "step 51250: train loss 2.4519, val loss 2.3922\n",
      "step 51260: train loss 2.4150, val loss 2.3590\n",
      "step 51270: train loss 2.3607, val loss 2.3912\n",
      "step 51280: train loss 2.4399, val loss 2.3447\n",
      "step 51290: train loss 2.3681, val loss 2.3272\n",
      "step 51300: train loss 2.3353, val loss 2.4055\n",
      "step 51310: train loss 2.3839, val loss 2.3523\n",
      "step 51320: train loss 2.3684, val loss 2.3820\n",
      "step 51330: train loss 2.2691, val loss 2.3426\n",
      "step 51340: train loss 2.3710, val loss 2.3947\n",
      "step 51350: train loss 2.3733, val loss 2.4248\n",
      "step 51360: train loss 2.3697, val loss 2.3714\n",
      "step 51370: train loss 2.5106, val loss 2.4201\n",
      "step 51380: train loss 2.3541, val loss 2.3286\n",
      "step 51390: train loss 2.3222, val loss 2.3743\n",
      "step 51400: train loss 2.3585, val loss 2.4448\n",
      "step 51410: train loss 2.4336, val loss 2.3400\n",
      "step 51420: train loss 2.3939, val loss 2.3775\n",
      "step 51430: train loss 2.3440, val loss 2.3953\n",
      "step 51440: train loss 2.3541, val loss 2.3527\n",
      "step 51450: train loss 2.4282, val loss 2.3194\n",
      "step 51460: train loss 2.3308, val loss 2.3451\n",
      "step 51470: train loss 2.3782, val loss 2.4195\n",
      "step 51480: train loss 2.4275, val loss 2.3937\n",
      "step 51490: train loss 2.3853, val loss 2.2667\n",
      "step 51500: train loss 2.3361, val loss 2.3496\n",
      "step 51510: train loss 2.3810, val loss 2.3279\n",
      "step 51520: train loss 2.3326, val loss 2.3554\n",
      "step 51530: train loss 2.3913, val loss 2.3246\n",
      "step 51540: train loss 2.3938, val loss 2.3572\n",
      "step 51550: train loss 2.3791, val loss 2.3578\n",
      "step 51560: train loss 2.3921, val loss 2.3167\n",
      "step 51570: train loss 2.3822, val loss 2.2971\n",
      "step 51580: train loss 2.3203, val loss 2.3337\n",
      "step 51590: train loss 2.3778, val loss 2.3872\n",
      "step 51600: train loss 2.4107, val loss 2.3063\n",
      "step 51610: train loss 2.3664, val loss 2.3794\n",
      "step 51620: train loss 2.3339, val loss 2.3028\n",
      "step 51630: train loss 2.3740, val loss 2.3219\n",
      "step 51640: train loss 2.3546, val loss 2.3887\n",
      "step 51650: train loss 2.3732, val loss 2.3380\n",
      "step 51660: train loss 2.3673, val loss 2.4325\n",
      "step 51670: train loss 2.4275, val loss 2.3479\n",
      "step 51680: train loss 2.4069, val loss 2.3113\n",
      "step 51690: train loss 2.3517, val loss 2.4695\n",
      "step 51700: train loss 2.3520, val loss 2.3945\n",
      "step 51710: train loss 2.4135, val loss 2.3625\n",
      "step 51720: train loss 2.4042, val loss 2.3989\n",
      "step 51730: train loss 2.3509, val loss 2.3556\n",
      "step 51740: train loss 2.4422, val loss 2.3887\n",
      "step 51750: train loss 2.4045, val loss 2.3002\n",
      "step 51760: train loss 2.3724, val loss 2.4267\n",
      "step 51770: train loss 2.4499, val loss 2.3894\n",
      "step 51780: train loss 2.3257, val loss 2.3913\n",
      "step 51790: train loss 2.3402, val loss 2.3358\n",
      "step 51800: train loss 2.4135, val loss 2.4290\n",
      "step 51810: train loss 2.3250, val loss 2.3191\n",
      "step 51820: train loss 2.3079, val loss 2.3795\n",
      "step 51830: train loss 2.3286, val loss 2.3488\n",
      "step 51840: train loss 2.4070, val loss 2.3938\n",
      "step 51850: train loss 2.3910, val loss 2.4440\n",
      "step 51860: train loss 2.3446, val loss 2.4471\n",
      "step 51870: train loss 2.3589, val loss 2.3398\n",
      "step 51880: train loss 2.3686, val loss 2.3290\n",
      "step 51890: train loss 2.3425, val loss 2.4159\n",
      "step 51900: train loss 2.4004, val loss 2.3512\n",
      "step 51910: train loss 2.4358, val loss 2.3424\n",
      "step 51920: train loss 2.4344, val loss 2.3822\n",
      "step 51930: train loss 2.4363, val loss 2.3111\n",
      "step 51940: train loss 2.3722, val loss 2.3813\n",
      "step 51950: train loss 2.4562, val loss 2.4070\n",
      "step 51960: train loss 2.3401, val loss 2.4415\n",
      "step 51970: train loss 2.3373, val loss 2.3510\n",
      "step 51980: train loss 2.3630, val loss 2.4292\n",
      "step 51990: train loss 2.4529, val loss 2.3429\n",
      "step 52000: train loss 2.3117, val loss 2.4574\n",
      "step 52010: train loss 2.3268, val loss 2.4285\n",
      "step 52020: train loss 2.3916, val loss 2.2827\n",
      "step 52030: train loss 2.3637, val loss 2.3341\n",
      "step 52040: train loss 2.3478, val loss 2.2752\n",
      "step 52050: train loss 2.4208, val loss 2.3353\n",
      "step 52060: train loss 2.4247, val loss 2.4767\n",
      "step 52070: train loss 2.3586, val loss 2.4216\n",
      "step 52080: train loss 2.4013, val loss 2.3812\n",
      "step 52090: train loss 2.3794, val loss 2.4392\n",
      "step 52100: train loss 2.4025, val loss 2.3724\n",
      "step 52110: train loss 2.3996, val loss 2.3407\n",
      "step 52120: train loss 2.3723, val loss 2.3099\n",
      "step 52130: train loss 2.4111, val loss 2.2519\n",
      "step 52140: train loss 2.4375, val loss 2.4658\n",
      "step 52150: train loss 2.3768, val loss 2.3179\n",
      "step 52160: train loss 2.3533, val loss 2.3907\n",
      "step 52170: train loss 2.3854, val loss 2.3480\n",
      "step 52180: train loss 2.4441, val loss 2.3229\n",
      "step 52190: train loss 2.4691, val loss 2.4279\n",
      "step 52200: train loss 2.3547, val loss 2.3818\n",
      "step 52210: train loss 2.3296, val loss 2.3809\n",
      "step 52220: train loss 2.4566, val loss 2.3801\n",
      "step 52230: train loss 2.3595, val loss 2.3676\n",
      "step 52240: train loss 2.3051, val loss 2.3769\n",
      "step 52250: train loss 2.2777, val loss 2.4654\n",
      "step 52260: train loss 2.3900, val loss 2.3963\n",
      "step 52270: train loss 2.4024, val loss 2.3226\n",
      "step 52280: train loss 2.4118, val loss 2.3549\n",
      "step 52290: train loss 2.3722, val loss 2.2555\n",
      "step 52300: train loss 2.4237, val loss 2.3871\n",
      "step 52310: train loss 2.3689, val loss 2.3994\n",
      "step 52320: train loss 2.3423, val loss 2.3764\n",
      "step 52330: train loss 2.3483, val loss 2.3259\n",
      "step 52340: train loss 2.3688, val loss 2.3061\n",
      "step 52350: train loss 2.3716, val loss 2.3054\n",
      "step 52360: train loss 2.3005, val loss 2.3559\n",
      "step 52370: train loss 2.3702, val loss 2.3738\n",
      "step 52380: train loss 2.3157, val loss 2.3746\n",
      "step 52390: train loss 2.3338, val loss 2.3567\n",
      "step 52400: train loss 2.4209, val loss 2.3416\n",
      "step 52410: train loss 2.3558, val loss 2.4018\n",
      "step 52420: train loss 2.4213, val loss 2.3531\n",
      "step 52430: train loss 2.3449, val loss 2.3122\n",
      "step 52440: train loss 2.4428, val loss 2.4331\n",
      "step 52450: train loss 2.4283, val loss 2.3709\n",
      "step 52460: train loss 2.3840, val loss 2.3460\n",
      "step 52470: train loss 2.3562, val loss 2.3760\n",
      "step 52480: train loss 2.4149, val loss 2.4093\n",
      "step 52490: train loss 2.3548, val loss 2.2721\n",
      "step 52500: train loss 2.3688, val loss 2.4281\n",
      "step 52510: train loss 2.3246, val loss 2.3893\n",
      "step 52520: train loss 2.3921, val loss 2.4130\n",
      "step 52530: train loss 2.2867, val loss 2.3668\n",
      "step 52540: train loss 2.3542, val loss 2.3698\n",
      "step 52550: train loss 2.4068, val loss 2.3750\n",
      "step 52560: train loss 2.3310, val loss 2.4176\n",
      "step 52570: train loss 2.3945, val loss 2.3952\n",
      "step 52580: train loss 2.3872, val loss 2.3628\n",
      "step 52590: train loss 2.3567, val loss 2.3345\n",
      "step 52600: train loss 2.4580, val loss 2.3424\n",
      "step 52610: train loss 2.4076, val loss 2.3326\n",
      "step 52620: train loss 2.3482, val loss 2.3526\n",
      "step 52630: train loss 2.3345, val loss 2.3756\n",
      "step 52640: train loss 2.3895, val loss 2.3665\n",
      "step 52650: train loss 2.3553, val loss 2.2838\n",
      "step 52660: train loss 2.4454, val loss 2.3559\n",
      "step 52670: train loss 2.3843, val loss 2.4546\n",
      "step 52680: train loss 2.4014, val loss 2.4099\n",
      "step 52690: train loss 2.4284, val loss 2.3900\n",
      "step 52700: train loss 2.3813, val loss 2.3524\n",
      "step 52710: train loss 2.3887, val loss 2.3795\n",
      "step 52720: train loss 2.3460, val loss 2.3313\n",
      "step 52730: train loss 2.3714, val loss 2.3706\n",
      "step 52740: train loss 2.4039, val loss 2.4658\n",
      "step 52750: train loss 2.3727, val loss 2.4122\n",
      "step 52760: train loss 2.4112, val loss 2.3841\n",
      "step 52770: train loss 2.3342, val loss 2.3842\n",
      "step 52780: train loss 2.3115, val loss 2.2992\n",
      "step 52790: train loss 2.4214, val loss 2.3444\n",
      "step 52800: train loss 2.4409, val loss 2.3204\n",
      "step 52810: train loss 2.3178, val loss 2.3388\n",
      "step 52820: train loss 2.4116, val loss 2.4090\n",
      "step 52830: train loss 2.3953, val loss 2.3248\n",
      "step 52840: train loss 2.3803, val loss 2.3396\n",
      "step 52850: train loss 2.3698, val loss 2.4335\n",
      "step 52860: train loss 2.3970, val loss 2.3608\n",
      "step 52870: train loss 2.3536, val loss 2.3843\n",
      "step 52880: train loss 2.3707, val loss 2.4420\n",
      "step 52890: train loss 2.3811, val loss 2.3800\n",
      "step 52900: train loss 2.3935, val loss 2.3457\n",
      "step 52910: train loss 2.3949, val loss 2.3601\n",
      "step 52920: train loss 2.3262, val loss 2.3336\n",
      "step 52930: train loss 2.4224, val loss 2.4165\n",
      "step 52940: train loss 2.3469, val loss 2.4026\n",
      "step 52950: train loss 2.3574, val loss 2.3345\n",
      "step 52960: train loss 2.3644, val loss 2.4025\n",
      "step 52970: train loss 2.3700, val loss 2.3325\n",
      "step 52980: train loss 2.3146, val loss 2.3154\n",
      "step 52990: train loss 2.2848, val loss 2.3777\n",
      "step 53000: train loss 2.3781, val loss 2.3314\n",
      "step 53010: train loss 2.3862, val loss 2.3365\n",
      "step 53020: train loss 2.3864, val loss 2.3969\n",
      "step 53030: train loss 2.2995, val loss 2.4271\n",
      "step 53040: train loss 2.4120, val loss 2.2992\n",
      "step 53050: train loss 2.3692, val loss 2.3519\n",
      "step 53060: train loss 2.4524, val loss 2.3523\n",
      "step 53070: train loss 2.3575, val loss 2.4125\n",
      "step 53080: train loss 2.3910, val loss 2.3401\n",
      "step 53090: train loss 2.4495, val loss 2.3497\n",
      "step 53100: train loss 2.4211, val loss 2.4042\n",
      "step 53110: train loss 2.3954, val loss 2.3432\n",
      "step 53120: train loss 2.4179, val loss 2.3948\n",
      "step 53130: train loss 2.3537, val loss 2.3663\n",
      "step 53140: train loss 2.4373, val loss 2.3304\n",
      "step 53150: train loss 2.3538, val loss 2.3699\n",
      "step 53160: train loss 2.3974, val loss 2.3317\n",
      "step 53170: train loss 2.3936, val loss 2.3268\n",
      "step 53180: train loss 2.3891, val loss 2.3941\n",
      "step 53190: train loss 2.3849, val loss 2.4381\n",
      "step 53200: train loss 2.4025, val loss 2.4031\n",
      "step 53210: train loss 2.4040, val loss 2.3531\n",
      "step 53220: train loss 2.3609, val loss 2.3137\n",
      "step 53230: train loss 2.3318, val loss 2.3705\n",
      "step 53240: train loss 2.3846, val loss 2.2939\n",
      "step 53250: train loss 2.3809, val loss 2.3737\n",
      "step 53260: train loss 2.4311, val loss 2.3968\n",
      "step 53270: train loss 2.4113, val loss 2.3938\n",
      "step 53280: train loss 2.4071, val loss 2.3261\n",
      "step 53290: train loss 2.4112, val loss 2.3550\n",
      "step 53300: train loss 2.4308, val loss 2.3161\n",
      "step 53310: train loss 2.3367, val loss 2.3803\n",
      "step 53320: train loss 2.3522, val loss 2.3500\n",
      "step 53330: train loss 2.3881, val loss 2.3820\n",
      "step 53340: train loss 2.2965, val loss 2.4126\n",
      "step 53350: train loss 2.3139, val loss 2.3768\n",
      "step 53360: train loss 2.3134, val loss 2.3640\n",
      "step 53370: train loss 2.3950, val loss 2.3411\n",
      "step 53380: train loss 2.3683, val loss 2.4267\n",
      "step 53390: train loss 2.4725, val loss 2.4315\n",
      "step 53400: train loss 2.3249, val loss 2.4277\n",
      "step 53410: train loss 2.4737, val loss 2.3647\n",
      "step 53420: train loss 2.3556, val loss 2.3416\n",
      "step 53430: train loss 2.3542, val loss 2.3507\n",
      "step 53440: train loss 2.3145, val loss 2.3857\n",
      "step 53450: train loss 2.3505, val loss 2.4082\n",
      "step 53460: train loss 2.4182, val loss 2.3783\n",
      "step 53470: train loss 2.4018, val loss 2.3722\n",
      "step 53480: train loss 2.3662, val loss 2.3720\n",
      "step 53490: train loss 2.3458, val loss 2.3924\n",
      "step 53500: train loss 2.3194, val loss 2.3243\n",
      "step 53510: train loss 2.3917, val loss 2.3761\n",
      "step 53520: train loss 2.3132, val loss 2.3987\n",
      "step 53530: train loss 2.3444, val loss 2.3555\n",
      "step 53540: train loss 2.3800, val loss 2.3446\n",
      "step 53550: train loss 2.4002, val loss 2.4204\n",
      "step 53560: train loss 2.4041, val loss 2.3109\n",
      "step 53570: train loss 2.3613, val loss 2.3148\n",
      "step 53580: train loss 2.3573, val loss 2.3935\n",
      "step 53590: train loss 2.3396, val loss 2.3440\n",
      "step 53600: train loss 2.4282, val loss 2.4354\n",
      "step 53610: train loss 2.4143, val loss 2.4010\n",
      "step 53620: train loss 2.4883, val loss 2.3745\n",
      "step 53630: train loss 2.3154, val loss 2.4374\n",
      "step 53640: train loss 2.4081, val loss 2.3783\n",
      "step 53650: train loss 2.4272, val loss 2.3124\n",
      "step 53660: train loss 2.3751, val loss 2.3796\n",
      "step 53670: train loss 2.4534, val loss 2.3289\n",
      "step 53680: train loss 2.3379, val loss 2.3554\n",
      "step 53690: train loss 2.3455, val loss 2.2816\n",
      "step 53700: train loss 2.3697, val loss 2.3437\n",
      "step 53710: train loss 2.4022, val loss 2.3583\n",
      "step 53720: train loss 2.4343, val loss 2.3643\n",
      "step 53730: train loss 2.3425, val loss 2.3741\n",
      "step 53740: train loss 2.4198, val loss 2.3932\n",
      "step 53750: train loss 2.3816, val loss 2.3963\n",
      "step 53760: train loss 2.4423, val loss 2.3727\n",
      "step 53770: train loss 2.3593, val loss 2.4404\n",
      "step 53780: train loss 2.3869, val loss 2.4229\n",
      "step 53790: train loss 2.3793, val loss 2.3875\n",
      "step 53800: train loss 2.3932, val loss 2.4617\n",
      "step 53810: train loss 2.3556, val loss 2.3971\n",
      "step 53820: train loss 2.3920, val loss 2.2863\n",
      "step 53830: train loss 2.4251, val loss 2.3868\n",
      "step 53840: train loss 2.4190, val loss 2.3774\n",
      "step 53850: train loss 2.3244, val loss 2.3302\n",
      "step 53860: train loss 2.3426, val loss 2.4055\n",
      "step 53870: train loss 2.4025, val loss 2.4062\n",
      "step 53880: train loss 2.3357, val loss 2.3609\n",
      "step 53890: train loss 2.3741, val loss 2.3435\n",
      "step 53900: train loss 2.3966, val loss 2.3266\n",
      "step 53910: train loss 2.3190, val loss 2.3898\n",
      "step 53920: train loss 2.3572, val loss 2.4227\n",
      "step 53930: train loss 2.3070, val loss 2.4364\n",
      "step 53940: train loss 2.3265, val loss 2.4359\n",
      "step 53950: train loss 2.3843, val loss 2.3482\n",
      "step 53960: train loss 2.3856, val loss 2.3545\n",
      "step 53970: train loss 2.3938, val loss 2.3850\n",
      "step 53980: train loss 2.3411, val loss 2.3031\n",
      "step 53990: train loss 2.3485, val loss 2.3886\n",
      "step 54000: train loss 2.3975, val loss 2.4359\n",
      "step 54010: train loss 2.4032, val loss 2.3583\n",
      "step 54020: train loss 2.3461, val loss 2.3979\n",
      "step 54030: train loss 2.4296, val loss 2.3808\n",
      "step 54040: train loss 2.3728, val loss 2.3664\n",
      "step 54050: train loss 2.4222, val loss 2.4059\n",
      "step 54060: train loss 2.4572, val loss 2.3969\n",
      "step 54070: train loss 2.4003, val loss 2.3201\n",
      "step 54080: train loss 2.3279, val loss 2.3996\n",
      "step 54090: train loss 2.3745, val loss 2.3324\n",
      "step 54100: train loss 2.4118, val loss 2.3932\n",
      "step 54110: train loss 2.3222, val loss 2.4067\n",
      "step 54120: train loss 2.3704, val loss 2.4071\n",
      "step 54130: train loss 2.3240, val loss 2.4436\n",
      "step 54140: train loss 2.3406, val loss 2.3705\n",
      "step 54150: train loss 2.3690, val loss 2.4124\n",
      "step 54160: train loss 2.4612, val loss 2.3489\n",
      "step 54170: train loss 2.3679, val loss 2.3809\n",
      "step 54180: train loss 2.3860, val loss 2.3169\n",
      "step 54190: train loss 2.3795, val loss 2.4434\n",
      "step 54200: train loss 2.4107, val loss 2.3885\n",
      "step 54210: train loss 2.3572, val loss 2.3533\n",
      "step 54220: train loss 2.3897, val loss 2.4282\n",
      "step 54230: train loss 2.4374, val loss 2.4102\n",
      "step 54240: train loss 2.3207, val loss 2.3931\n",
      "step 54250: train loss 2.3795, val loss 2.5283\n",
      "step 54260: train loss 2.3470, val loss 2.3564\n",
      "step 54270: train loss 2.4282, val loss 2.3088\n",
      "step 54280: train loss 2.4304, val loss 2.3262\n",
      "step 54290: train loss 2.3095, val loss 2.3741\n",
      "step 54300: train loss 2.3936, val loss 2.3199\n",
      "step 54310: train loss 2.4185, val loss 2.3770\n",
      "step 54320: train loss 2.3218, val loss 2.3706\n",
      "step 54330: train loss 2.3617, val loss 2.2922\n",
      "step 54340: train loss 2.3973, val loss 2.3679\n",
      "step 54350: train loss 2.3599, val loss 2.3710\n",
      "step 54360: train loss 2.3656, val loss 2.3224\n",
      "step 54370: train loss 2.2827, val loss 2.3522\n",
      "step 54380: train loss 2.3374, val loss 2.4305\n",
      "step 54390: train loss 2.3787, val loss 2.3321\n",
      "step 54400: train loss 2.3724, val loss 2.3722\n",
      "step 54410: train loss 2.3473, val loss 2.2947\n",
      "step 54420: train loss 2.3005, val loss 2.3870\n",
      "step 54430: train loss 2.3786, val loss 2.3667\n",
      "step 54440: train loss 2.3716, val loss 2.3157\n",
      "step 54450: train loss 2.4695, val loss 2.3100\n",
      "step 54460: train loss 2.3545, val loss 2.3772\n",
      "step 54470: train loss 2.4254, val loss 2.3645\n",
      "step 54480: train loss 2.3347, val loss 2.4042\n",
      "step 54490: train loss 2.4094, val loss 2.4031\n",
      "step 54500: train loss 2.3909, val loss 2.4156\n",
      "step 54510: train loss 2.3610, val loss 2.4126\n",
      "step 54520: train loss 2.3561, val loss 2.3378\n",
      "step 54530: train loss 2.3775, val loss 2.3663\n",
      "step 54540: train loss 2.4506, val loss 2.3506\n",
      "step 54550: train loss 2.2623, val loss 2.3059\n",
      "step 54560: train loss 2.4294, val loss 2.3893\n",
      "step 54570: train loss 2.3540, val loss 2.4526\n",
      "step 54580: train loss 2.4935, val loss 2.3527\n",
      "step 54590: train loss 2.3558, val loss 2.3962\n",
      "step 54600: train loss 2.3189, val loss 2.3452\n",
      "step 54610: train loss 2.3624, val loss 2.3191\n",
      "step 54620: train loss 2.3936, val loss 2.3863\n",
      "step 54630: train loss 2.3882, val loss 2.3515\n",
      "step 54640: train loss 2.3784, val loss 2.3505\n",
      "step 54650: train loss 2.4039, val loss 2.4441\n",
      "step 54660: train loss 2.3467, val loss 2.3455\n",
      "step 54670: train loss 2.3822, val loss 2.3328\n",
      "step 54680: train loss 2.3473, val loss 2.4263\n",
      "step 54690: train loss 2.3506, val loss 2.3521\n",
      "step 54700: train loss 2.4192, val loss 2.3464\n",
      "step 54710: train loss 2.4535, val loss 2.3533\n",
      "step 54720: train loss 2.3830, val loss 2.3461\n",
      "step 54730: train loss 2.3191, val loss 2.3947\n",
      "step 54740: train loss 2.3235, val loss 2.4116\n",
      "step 54750: train loss 2.4209, val loss 2.3320\n",
      "step 54760: train loss 2.3632, val loss 2.3188\n",
      "step 54770: train loss 2.4058, val loss 2.3352\n",
      "step 54780: train loss 2.3824, val loss 2.3895\n",
      "step 54790: train loss 2.3406, val loss 2.3464\n",
      "step 54800: train loss 2.4985, val loss 2.4058\n",
      "step 54810: train loss 2.4479, val loss 2.3841\n",
      "step 54820: train loss 2.3996, val loss 2.4202\n",
      "step 54830: train loss 2.4358, val loss 2.4172\n",
      "step 54840: train loss 2.4604, val loss 2.3697\n",
      "step 54850: train loss 2.4311, val loss 2.3914\n",
      "step 54860: train loss 2.3089, val loss 2.3325\n",
      "step 54870: train loss 2.4056, val loss 2.4232\n",
      "step 54880: train loss 2.3148, val loss 2.4289\n",
      "step 54890: train loss 2.4074, val loss 2.3264\n",
      "step 54900: train loss 2.4072, val loss 2.3963\n",
      "step 54910: train loss 2.3160, val loss 2.3910\n",
      "step 54920: train loss 2.3045, val loss 2.3659\n",
      "step 54930: train loss 2.3714, val loss 2.3368\n",
      "step 54940: train loss 2.3485, val loss 2.3137\n",
      "step 54950: train loss 2.3837, val loss 2.3482\n",
      "step 54960: train loss 2.4703, val loss 2.4550\n",
      "step 54970: train loss 2.3314, val loss 2.3850\n",
      "step 54980: train loss 2.3700, val loss 2.3401\n",
      "step 54990: train loss 2.3784, val loss 2.3793\n",
      "step 55000: train loss 2.3828, val loss 2.4206\n",
      "step 55010: train loss 2.4400, val loss 2.3892\n",
      "step 55020: train loss 2.3626, val loss 2.3521\n",
      "step 55030: train loss 2.3814, val loss 2.3398\n",
      "step 55040: train loss 2.3749, val loss 2.4272\n",
      "step 55050: train loss 2.3586, val loss 2.3077\n",
      "step 55060: train loss 2.3956, val loss 2.3536\n",
      "step 55070: train loss 2.4505, val loss 2.3654\n",
      "step 55080: train loss 2.3659, val loss 2.3461\n",
      "step 55090: train loss 2.3929, val loss 2.3408\n",
      "step 55100: train loss 2.3517, val loss 2.3474\n",
      "step 55110: train loss 2.3405, val loss 2.4746\n",
      "step 55120: train loss 2.3438, val loss 2.3965\n",
      "step 55130: train loss 2.4616, val loss 2.3648\n",
      "step 55140: train loss 2.4349, val loss 2.4118\n",
      "step 55150: train loss 2.3271, val loss 2.4365\n",
      "step 55160: train loss 2.3266, val loss 2.3201\n",
      "step 55170: train loss 2.3685, val loss 2.3221\n",
      "step 55180: train loss 2.3239, val loss 2.2593\n",
      "step 55190: train loss 2.3741, val loss 2.3464\n",
      "step 55200: train loss 2.3755, val loss 2.3157\n",
      "step 55210: train loss 2.3280, val loss 2.2755\n",
      "step 55220: train loss 2.3926, val loss 2.3780\n",
      "step 55230: train loss 2.3162, val loss 2.4371\n",
      "step 55240: train loss 2.4067, val loss 2.3712\n",
      "step 55250: train loss 2.3514, val loss 2.3835\n",
      "step 55260: train loss 2.5021, val loss 2.2775\n",
      "step 55270: train loss 2.4267, val loss 2.3847\n",
      "step 55280: train loss 2.3469, val loss 2.3628\n",
      "step 55290: train loss 2.3569, val loss 2.2986\n",
      "step 55300: train loss 2.2871, val loss 2.3846\n",
      "step 55310: train loss 2.4355, val loss 2.3680\n",
      "step 55320: train loss 2.3805, val loss 2.3571\n",
      "step 55330: train loss 2.4051, val loss 2.3045\n",
      "step 55340: train loss 2.4236, val loss 2.2916\n",
      "step 55350: train loss 2.4882, val loss 2.4364\n",
      "step 55360: train loss 2.4461, val loss 2.3271\n",
      "step 55370: train loss 2.4711, val loss 2.2937\n",
      "step 55380: train loss 2.3255, val loss 2.4185\n",
      "step 55390: train loss 2.2995, val loss 2.3026\n",
      "step 55400: train loss 2.3580, val loss 2.3651\n",
      "step 55410: train loss 2.4146, val loss 2.4054\n",
      "step 55420: train loss 2.4426, val loss 2.4021\n",
      "step 55430: train loss 2.2971, val loss 2.3387\n",
      "step 55440: train loss 2.3231, val loss 2.3724\n",
      "step 55450: train loss 2.4211, val loss 2.3011\n",
      "step 55460: train loss 2.3238, val loss 2.3713\n",
      "step 55470: train loss 2.4252, val loss 2.3432\n",
      "step 55480: train loss 2.3985, val loss 2.4008\n",
      "step 55490: train loss 2.4055, val loss 2.3254\n",
      "step 55500: train loss 2.4071, val loss 2.3848\n",
      "step 55510: train loss 2.3864, val loss 2.3625\n",
      "step 55520: train loss 2.4368, val loss 2.3122\n",
      "step 55530: train loss 2.3874, val loss 2.3821\n",
      "step 55540: train loss 2.3212, val loss 2.3942\n",
      "step 55550: train loss 2.3748, val loss 2.3293\n",
      "step 55560: train loss 2.3780, val loss 2.3979\n",
      "step 55570: train loss 2.4215, val loss 2.3616\n",
      "step 55580: train loss 2.3563, val loss 2.3923\n",
      "step 55590: train loss 2.3742, val loss 2.3508\n",
      "step 55600: train loss 2.3389, val loss 2.3450\n",
      "step 55610: train loss 2.3128, val loss 2.3761\n",
      "step 55620: train loss 2.3360, val loss 2.3081\n",
      "step 55630: train loss 2.4613, val loss 2.3239\n",
      "step 55640: train loss 2.4202, val loss 2.3972\n",
      "step 55650: train loss 2.3400, val loss 2.3852\n",
      "step 55660: train loss 2.3136, val loss 2.4116\n",
      "step 55670: train loss 2.4531, val loss 2.3818\n",
      "step 55680: train loss 2.3796, val loss 2.2743\n",
      "step 55690: train loss 2.3773, val loss 2.4163\n",
      "step 55700: train loss 2.3675, val loss 2.3678\n",
      "step 55710: train loss 2.3533, val loss 2.4393\n",
      "step 55720: train loss 2.3998, val loss 2.3502\n",
      "step 55730: train loss 2.3671, val loss 2.3728\n",
      "step 55740: train loss 2.4970, val loss 2.4065\n",
      "step 55750: train loss 2.3174, val loss 2.3760\n",
      "step 55760: train loss 2.3908, val loss 2.3442\n",
      "step 55770: train loss 2.2774, val loss 2.4246\n",
      "step 55780: train loss 2.5001, val loss 2.4017\n",
      "step 55790: train loss 2.4611, val loss 2.3394\n",
      "step 55800: train loss 2.3860, val loss 2.3126\n",
      "step 55810: train loss 2.3385, val loss 2.3912\n",
      "step 55820: train loss 2.3304, val loss 2.3375\n",
      "step 55830: train loss 2.3705, val loss 2.3483\n",
      "step 55840: train loss 2.3490, val loss 2.2975\n",
      "step 55850: train loss 2.4187, val loss 2.3554\n",
      "step 55860: train loss 2.3871, val loss 2.3636\n",
      "step 55870: train loss 2.4193, val loss 2.4116\n",
      "step 55880: train loss 2.3283, val loss 2.4048\n",
      "step 55890: train loss 2.3309, val loss 2.3895\n",
      "step 55900: train loss 2.4675, val loss 2.4913\n",
      "step 55910: train loss 2.2749, val loss 2.3083\n",
      "step 55920: train loss 2.4135, val loss 2.3753\n",
      "step 55930: train loss 2.3757, val loss 2.3332\n",
      "step 55940: train loss 2.4002, val loss 2.3793\n",
      "step 55950: train loss 2.3672, val loss 2.5060\n",
      "step 55960: train loss 2.3333, val loss 2.3287\n",
      "step 55970: train loss 2.3680, val loss 2.4007\n",
      "step 55980: train loss 2.4443, val loss 2.4382\n",
      "step 55990: train loss 2.4461, val loss 2.3982\n",
      "step 56000: train loss 2.3744, val loss 2.3802\n",
      "step 56010: train loss 2.4161, val loss 2.4011\n",
      "step 56020: train loss 2.3911, val loss 2.3723\n",
      "step 56030: train loss 2.3539, val loss 2.3496\n",
      "step 56040: train loss 2.3996, val loss 2.3842\n",
      "step 56050: train loss 2.3703, val loss 2.3486\n",
      "step 56060: train loss 2.4108, val loss 2.3693\n",
      "step 56070: train loss 2.3747, val loss 2.3520\n",
      "step 56080: train loss 2.3716, val loss 2.4180\n",
      "step 56090: train loss 2.3434, val loss 2.4329\n",
      "step 56100: train loss 2.4342, val loss 2.3596\n",
      "step 56110: train loss 2.3835, val loss 2.3372\n",
      "step 56120: train loss 2.4263, val loss 2.3452\n",
      "step 56130: train loss 2.3399, val loss 2.4000\n",
      "step 56140: train loss 2.3388, val loss 2.4333\n",
      "step 56150: train loss 2.3647, val loss 2.3642\n",
      "step 56160: train loss 2.3279, val loss 2.3759\n",
      "step 56170: train loss 2.3956, val loss 2.3726\n",
      "step 56180: train loss 2.3743, val loss 2.4012\n",
      "step 56190: train loss 2.4062, val loss 2.3657\n",
      "step 56200: train loss 2.3878, val loss 2.3956\n",
      "step 56210: train loss 2.3261, val loss 2.3080\n",
      "step 56220: train loss 2.4062, val loss 2.3544\n",
      "step 56230: train loss 2.3397, val loss 2.4541\n",
      "step 56240: train loss 2.3845, val loss 2.3674\n",
      "step 56250: train loss 2.3080, val loss 2.4024\n",
      "step 56260: train loss 2.4588, val loss 2.3937\n",
      "step 56270: train loss 2.3563, val loss 2.4339\n",
      "step 56280: train loss 2.3348, val loss 2.3547\n",
      "step 56290: train loss 2.3404, val loss 2.3820\n",
      "step 56300: train loss 2.4551, val loss 2.3005\n",
      "step 56310: train loss 2.4548, val loss 2.3444\n",
      "step 56320: train loss 2.3776, val loss 2.3671\n",
      "step 56330: train loss 2.4150, val loss 2.3433\n",
      "step 56340: train loss 2.3937, val loss 2.2801\n",
      "step 56350: train loss 2.4092, val loss 2.3103\n",
      "step 56360: train loss 2.3603, val loss 2.3605\n",
      "step 56370: train loss 2.4381, val loss 2.3856\n",
      "step 56380: train loss 2.3694, val loss 2.3988\n",
      "step 56390: train loss 2.3035, val loss 2.2451\n",
      "step 56400: train loss 2.3466, val loss 2.3562\n",
      "step 56410: train loss 2.3712, val loss 2.3572\n",
      "step 56420: train loss 2.3540, val loss 2.3528\n",
      "step 56430: train loss 2.4535, val loss 2.3290\n",
      "step 56440: train loss 2.4149, val loss 2.3800\n",
      "step 56450: train loss 2.3783, val loss 2.4054\n",
      "step 56460: train loss 2.3321, val loss 2.3181\n",
      "step 56470: train loss 2.4805, val loss 2.4531\n",
      "step 56480: train loss 2.4321, val loss 2.3274\n",
      "step 56490: train loss 2.4332, val loss 2.3373\n",
      "step 56500: train loss 2.3643, val loss 2.3494\n",
      "step 56510: train loss 2.3906, val loss 2.3801\n",
      "step 56520: train loss 2.3959, val loss 2.3444\n",
      "step 56530: train loss 2.3785, val loss 2.4151\n",
      "step 56540: train loss 2.3640, val loss 2.3885\n",
      "step 56550: train loss 2.4241, val loss 2.3820\n",
      "step 56560: train loss 2.3431, val loss 2.3024\n",
      "step 56570: train loss 2.3445, val loss 2.4431\n",
      "step 56580: train loss 2.3313, val loss 2.4027\n",
      "step 56590: train loss 2.3768, val loss 2.4193\n",
      "step 56600: train loss 2.3583, val loss 2.3206\n",
      "step 56610: train loss 2.2910, val loss 2.4208\n",
      "step 56620: train loss 2.3210, val loss 2.3499\n",
      "step 56630: train loss 2.3866, val loss 2.3230\n",
      "step 56640: train loss 2.3251, val loss 2.3686\n",
      "step 56650: train loss 2.3732, val loss 2.3172\n",
      "step 56660: train loss 2.3086, val loss 2.2974\n",
      "step 56670: train loss 2.4598, val loss 2.3304\n",
      "step 56680: train loss 2.2955, val loss 2.4000\n",
      "step 56690: train loss 2.4268, val loss 2.2894\n",
      "step 56700: train loss 2.3433, val loss 2.3565\n",
      "step 56710: train loss 2.3699, val loss 2.3352\n",
      "step 56720: train loss 2.3618, val loss 2.3595\n",
      "step 56730: train loss 2.2656, val loss 2.4177\n",
      "step 56740: train loss 2.3433, val loss 2.3920\n",
      "step 56750: train loss 2.4180, val loss 2.3368\n",
      "step 56760: train loss 2.3804, val loss 2.2729\n",
      "step 56770: train loss 2.3296, val loss 2.3895\n",
      "step 56780: train loss 2.4099, val loss 2.3278\n",
      "step 56790: train loss 2.3654, val loss 2.3371\n",
      "step 56800: train loss 2.3656, val loss 2.3618\n",
      "step 56810: train loss 2.3910, val loss 2.3750\n",
      "step 56820: train loss 2.3134, val loss 2.3304\n",
      "step 56830: train loss 2.4291, val loss 2.2968\n",
      "step 56840: train loss 2.3674, val loss 2.3045\n",
      "step 56850: train loss 2.4510, val loss 2.4015\n",
      "step 56860: train loss 2.4571, val loss 2.3653\n",
      "step 56870: train loss 2.3831, val loss 2.4591\n",
      "step 56880: train loss 2.4587, val loss 2.3220\n",
      "step 56890: train loss 2.4181, val loss 2.3030\n",
      "step 56900: train loss 2.3880, val loss 2.3859\n",
      "step 56910: train loss 2.3572, val loss 2.3127\n",
      "step 56920: train loss 2.3144, val loss 2.2221\n",
      "step 56930: train loss 2.3522, val loss 2.3690\n",
      "step 56940: train loss 2.4422, val loss 2.4569\n",
      "step 56950: train loss 2.3127, val loss 2.3463\n",
      "step 56960: train loss 2.3475, val loss 2.3343\n",
      "step 56970: train loss 2.3516, val loss 2.4212\n",
      "step 56980: train loss 2.3522, val loss 2.3855\n",
      "step 56990: train loss 2.3898, val loss 2.3444\n",
      "step 57000: train loss 2.3919, val loss 2.3935\n",
      "step 57010: train loss 2.3615, val loss 2.4450\n",
      "step 57020: train loss 2.4200, val loss 2.2966\n",
      "step 57030: train loss 2.4399, val loss 2.3632\n",
      "step 57040: train loss 2.3722, val loss 2.4370\n",
      "step 57050: train loss 2.4228, val loss 2.3018\n",
      "step 57060: train loss 2.3803, val loss 2.3752\n",
      "step 57070: train loss 2.3748, val loss 2.4824\n",
      "step 57080: train loss 2.3780, val loss 2.3978\n",
      "step 57090: train loss 2.3825, val loss 2.2634\n",
      "step 57100: train loss 2.3648, val loss 2.3823\n",
      "step 57110: train loss 2.4045, val loss 2.3474\n",
      "step 57120: train loss 2.3080, val loss 2.4461\n",
      "step 57130: train loss 2.3521, val loss 2.4513\n",
      "step 57140: train loss 2.3738, val loss 2.3349\n",
      "step 57150: train loss 2.3513, val loss 2.3743\n",
      "step 57160: train loss 2.4968, val loss 2.3326\n",
      "step 57170: train loss 2.4199, val loss 2.3682\n",
      "step 57180: train loss 2.3758, val loss 2.4145\n",
      "step 57190: train loss 2.3856, val loss 2.4016\n",
      "step 57200: train loss 2.4151, val loss 2.4298\n",
      "step 57210: train loss 2.3592, val loss 2.3857\n",
      "step 57220: train loss 2.3318, val loss 2.3168\n",
      "step 57230: train loss 2.3653, val loss 2.3369\n",
      "step 57240: train loss 2.3718, val loss 2.3028\n",
      "step 57250: train loss 2.3239, val loss 2.3685\n",
      "step 57260: train loss 2.3745, val loss 2.3699\n",
      "step 57270: train loss 2.3918, val loss 2.3517\n",
      "step 57280: train loss 2.3494, val loss 2.3936\n",
      "step 57290: train loss 2.3763, val loss 2.3592\n",
      "step 57300: train loss 2.4290, val loss 2.4669\n",
      "step 57310: train loss 2.4186, val loss 2.4191\n",
      "step 57320: train loss 2.3975, val loss 2.3957\n",
      "step 57330: train loss 2.3529, val loss 2.3816\n",
      "step 57340: train loss 2.3830, val loss 2.3357\n",
      "step 57350: train loss 2.3574, val loss 2.3346\n",
      "step 57360: train loss 2.2446, val loss 2.4036\n",
      "step 57370: train loss 2.3226, val loss 2.4217\n",
      "step 57380: train loss 2.3607, val loss 2.4279\n",
      "step 57390: train loss 2.3441, val loss 2.3039\n",
      "step 57400: train loss 2.3903, val loss 2.3936\n",
      "step 57410: train loss 2.3047, val loss 2.4155\n",
      "step 57420: train loss 2.4589, val loss 2.3928\n",
      "step 57430: train loss 2.3314, val loss 2.4157\n",
      "step 57440: train loss 2.3510, val loss 2.3947\n",
      "step 57450: train loss 2.4099, val loss 2.4515\n",
      "step 57460: train loss 2.3749, val loss 2.3023\n",
      "step 57470: train loss 2.3525, val loss 2.3705\n",
      "step 57480: train loss 2.3990, val loss 2.3855\n",
      "step 57490: train loss 2.3620, val loss 2.4213\n",
      "step 57500: train loss 2.4154, val loss 2.4483\n",
      "step 57510: train loss 2.3265, val loss 2.2957\n",
      "step 57520: train loss 2.4422, val loss 2.4329\n",
      "step 57530: train loss 2.3805, val loss 2.4058\n",
      "step 57540: train loss 2.4089, val loss 2.3920\n",
      "step 57550: train loss 2.3585, val loss 2.3535\n",
      "step 57560: train loss 2.4277, val loss 2.4167\n",
      "step 57570: train loss 2.3578, val loss 2.3198\n",
      "step 57580: train loss 2.3129, val loss 2.3336\n",
      "step 57590: train loss 2.4082, val loss 2.3672\n",
      "step 57600: train loss 2.4152, val loss 2.3404\n",
      "step 57610: train loss 2.3786, val loss 2.3460\n",
      "step 57620: train loss 2.3359, val loss 2.4318\n",
      "step 57630: train loss 2.3948, val loss 2.4093\n",
      "step 57640: train loss 2.3543, val loss 2.3137\n",
      "step 57650: train loss 2.3699, val loss 2.3398\n",
      "step 57660: train loss 2.3398, val loss 2.4167\n",
      "step 57670: train loss 2.3462, val loss 2.3558\n",
      "step 57680: train loss 2.4241, val loss 2.3268\n",
      "step 57690: train loss 2.3782, val loss 2.3401\n",
      "step 57700: train loss 2.3637, val loss 2.3509\n",
      "step 57710: train loss 2.2723, val loss 2.2508\n",
      "step 57720: train loss 2.3851, val loss 2.3515\n",
      "step 57730: train loss 2.4236, val loss 2.4696\n",
      "step 57740: train loss 2.3887, val loss 2.3164\n",
      "step 57750: train loss 2.4111, val loss 2.3694\n",
      "step 57760: train loss 2.3222, val loss 2.3500\n",
      "step 57770: train loss 2.4140, val loss 2.3625\n",
      "step 57780: train loss 2.4368, val loss 2.3690\n",
      "step 57790: train loss 2.3857, val loss 2.4011\n",
      "step 57800: train loss 2.3195, val loss 2.4179\n",
      "step 57810: train loss 2.4490, val loss 2.3620\n",
      "step 57820: train loss 2.2987, val loss 2.4355\n",
      "step 57830: train loss 2.3694, val loss 2.3534\n",
      "step 57840: train loss 2.2870, val loss 2.4358\n",
      "step 57850: train loss 2.4342, val loss 2.4096\n",
      "step 57860: train loss 2.3701, val loss 2.3512\n",
      "step 57870: train loss 2.3692, val loss 2.4324\n",
      "step 57880: train loss 2.3898, val loss 2.4276\n",
      "step 57890: train loss 2.3835, val loss 2.3709\n",
      "step 57900: train loss 2.3890, val loss 2.3906\n",
      "step 57910: train loss 2.4046, val loss 2.4690\n",
      "step 57920: train loss 2.3839, val loss 2.4665\n",
      "step 57930: train loss 2.4555, val loss 2.3921\n",
      "step 57940: train loss 2.4475, val loss 2.3265\n",
      "step 57950: train loss 2.3117, val loss 2.4107\n",
      "step 57960: train loss 2.3325, val loss 2.3367\n",
      "step 57970: train loss 2.3606, val loss 2.3949\n",
      "step 57980: train loss 2.4098, val loss 2.4134\n",
      "step 57990: train loss 2.4046, val loss 2.3536\n",
      "step 58000: train loss 2.3964, val loss 2.5176\n",
      "step 58010: train loss 2.4096, val loss 2.3781\n",
      "step 58020: train loss 2.2716, val loss 2.4219\n",
      "step 58030: train loss 2.4529, val loss 2.3113\n",
      "step 58040: train loss 2.3527, val loss 2.3207\n",
      "step 58050: train loss 2.4027, val loss 2.4016\n",
      "step 58060: train loss 2.3590, val loss 2.3964\n",
      "step 58070: train loss 2.4242, val loss 2.3822\n",
      "step 58080: train loss 2.4244, val loss 2.3602\n",
      "step 58090: train loss 2.4664, val loss 2.3455\n",
      "step 58100: train loss 2.4081, val loss 2.3265\n",
      "step 58110: train loss 2.4347, val loss 2.3847\n",
      "step 58120: train loss 2.3526, val loss 2.3700\n",
      "step 58130: train loss 2.3508, val loss 2.2878\n",
      "step 58140: train loss 2.2881, val loss 2.3527\n",
      "step 58150: train loss 2.3411, val loss 2.3705\n",
      "step 58160: train loss 2.3266, val loss 2.3730\n",
      "step 58170: train loss 2.3445, val loss 2.3211\n",
      "step 58180: train loss 2.3879, val loss 2.3345\n",
      "step 58190: train loss 2.3939, val loss 2.4148\n",
      "step 58200: train loss 2.4062, val loss 2.4213\n",
      "step 58210: train loss 2.3286, val loss 2.4456\n",
      "step 58220: train loss 2.3787, val loss 2.3951\n",
      "step 58230: train loss 2.3060, val loss 2.3019\n",
      "step 58240: train loss 2.3372, val loss 2.4050\n",
      "step 58250: train loss 2.3935, val loss 2.4320\n",
      "step 58260: train loss 2.4344, val loss 2.3898\n",
      "step 58270: train loss 2.3913, val loss 2.3142\n",
      "step 58280: train loss 2.3657, val loss 2.2868\n",
      "step 58290: train loss 2.3545, val loss 2.3354\n",
      "step 58300: train loss 2.3626, val loss 2.3543\n",
      "step 58310: train loss 2.4246, val loss 2.3302\n",
      "step 58320: train loss 2.4518, val loss 2.3333\n",
      "step 58330: train loss 2.4291, val loss 2.3635\n",
      "step 58340: train loss 2.3268, val loss 2.3718\n",
      "step 58350: train loss 2.4246, val loss 2.3577\n",
      "step 58360: train loss 2.3706, val loss 2.4189\n",
      "step 58370: train loss 2.3765, val loss 2.3510\n",
      "step 58380: train loss 2.4040, val loss 2.3257\n",
      "step 58390: train loss 2.3932, val loss 2.3472\n",
      "step 58400: train loss 2.4032, val loss 2.4378\n",
      "step 58410: train loss 2.4087, val loss 2.3890\n",
      "step 58420: train loss 2.3162, val loss 2.4212\n",
      "step 58430: train loss 2.4324, val loss 2.3640\n",
      "step 58440: train loss 2.3812, val loss 2.3586\n",
      "step 58450: train loss 2.3552, val loss 2.4582\n",
      "step 58460: train loss 2.3408, val loss 2.3160\n",
      "step 58470: train loss 2.4467, val loss 2.3588\n",
      "step 58480: train loss 2.3897, val loss 2.3928\n",
      "step 58490: train loss 2.3642, val loss 2.3504\n",
      "step 58500: train loss 2.3691, val loss 2.3405\n",
      "step 58510: train loss 2.4321, val loss 2.3847\n",
      "step 58520: train loss 2.4102, val loss 2.3424\n",
      "step 58530: train loss 2.3596, val loss 2.3614\n",
      "step 58540: train loss 2.3375, val loss 2.4393\n",
      "step 58550: train loss 2.3716, val loss 2.3805\n",
      "step 58560: train loss 2.3537, val loss 2.2898\n",
      "step 58570: train loss 2.3642, val loss 2.2747\n",
      "step 58580: train loss 2.3693, val loss 2.3719\n",
      "step 58590: train loss 2.3620, val loss 2.3729\n",
      "step 58600: train loss 2.4092, val loss 2.3145\n",
      "step 58610: train loss 2.4321, val loss 2.3594\n",
      "step 58620: train loss 2.3434, val loss 2.3914\n",
      "step 58630: train loss 2.4414, val loss 2.4192\n",
      "step 58640: train loss 2.4078, val loss 2.3836\n",
      "step 58650: train loss 2.4429, val loss 2.3488\n",
      "step 58660: train loss 2.2846, val loss 2.4149\n",
      "step 58670: train loss 2.4739, val loss 2.3246\n",
      "step 58680: train loss 2.3812, val loss 2.3279\n",
      "step 58690: train loss 2.3910, val loss 2.3796\n",
      "step 58700: train loss 2.3467, val loss 2.3597\n",
      "step 58710: train loss 2.3873, val loss 2.3950\n",
      "step 58720: train loss 2.4358, val loss 2.4388\n",
      "step 58730: train loss 2.4033, val loss 2.3793\n",
      "step 58740: train loss 2.3427, val loss 2.3703\n",
      "step 58750: train loss 2.3088, val loss 2.3140\n",
      "step 58760: train loss 2.2995, val loss 2.3691\n",
      "step 58770: train loss 2.3889, val loss 2.3761\n",
      "step 58780: train loss 2.3740, val loss 2.3451\n",
      "step 58790: train loss 2.4237, val loss 2.3362\n",
      "step 58800: train loss 2.3840, val loss 2.3442\n",
      "step 58810: train loss 2.4366, val loss 2.4418\n",
      "step 58820: train loss 2.4252, val loss 2.3917\n",
      "step 58830: train loss 2.4333, val loss 2.3423\n",
      "step 58840: train loss 2.3869, val loss 2.3393\n",
      "step 58850: train loss 2.3084, val loss 2.3771\n",
      "step 58860: train loss 2.4182, val loss 2.4429\n",
      "step 58870: train loss 2.3734, val loss 2.3825\n",
      "step 58880: train loss 2.4568, val loss 2.3119\n",
      "step 58890: train loss 2.3829, val loss 2.3679\n",
      "step 58900: train loss 2.4603, val loss 2.3975\n",
      "step 58910: train loss 2.4291, val loss 2.4211\n",
      "step 58920: train loss 2.3409, val loss 2.3157\n",
      "step 58930: train loss 2.4149, val loss 2.3846\n",
      "step 58940: train loss 2.3130, val loss 2.4297\n",
      "step 58950: train loss 2.3313, val loss 2.4297\n",
      "step 58960: train loss 2.3896, val loss 2.3456\n",
      "step 58970: train loss 2.4467, val loss 2.4048\n",
      "step 58980: train loss 2.3652, val loss 2.4212\n",
      "step 58990: train loss 2.3246, val loss 2.3324\n",
      "step 59000: train loss 2.4266, val loss 2.3899\n",
      "step 59010: train loss 2.3665, val loss 2.3400\n",
      "step 59020: train loss 2.3327, val loss 2.4275\n",
      "step 59030: train loss 2.4099, val loss 2.4008\n",
      "step 59040: train loss 2.3577, val loss 2.3389\n",
      "step 59050: train loss 2.3991, val loss 2.3601\n",
      "step 59060: train loss 2.4098, val loss 2.3632\n",
      "step 59070: train loss 2.4266, val loss 2.3115\n",
      "step 59080: train loss 2.3893, val loss 2.3727\n",
      "step 59090: train loss 2.3163, val loss 2.3694\n",
      "step 59100: train loss 2.3540, val loss 2.3906\n",
      "step 59110: train loss 2.3837, val loss 2.3788\n",
      "step 59120: train loss 2.3874, val loss 2.3958\n",
      "step 59130: train loss 2.4160, val loss 2.3233\n",
      "step 59140: train loss 2.3256, val loss 2.3624\n",
      "step 59150: train loss 2.3466, val loss 2.3997\n",
      "step 59160: train loss 2.3246, val loss 2.2774\n",
      "step 59170: train loss 2.3135, val loss 2.3441\n",
      "step 59180: train loss 2.4503, val loss 2.3399\n",
      "step 59190: train loss 2.3931, val loss 2.3682\n",
      "step 59200: train loss 2.3597, val loss 2.3665\n",
      "step 59210: train loss 2.3886, val loss 2.3597\n",
      "step 59220: train loss 2.4622, val loss 2.3427\n",
      "step 59230: train loss 2.3813, val loss 2.3555\n",
      "step 59240: train loss 2.3553, val loss 2.3480\n",
      "step 59250: train loss 2.4091, val loss 2.3572\n",
      "step 59260: train loss 2.4275, val loss 2.3864\n",
      "step 59270: train loss 2.4152, val loss 2.4592\n",
      "step 59280: train loss 2.3343, val loss 2.3297\n",
      "step 59290: train loss 2.3917, val loss 2.4096\n",
      "step 59300: train loss 2.4363, val loss 2.3944\n",
      "step 59310: train loss 2.3907, val loss 2.3883\n",
      "step 59320: train loss 2.3065, val loss 2.3507\n",
      "step 59330: train loss 2.4552, val loss 2.3785\n",
      "step 59340: train loss 2.3711, val loss 2.3816\n",
      "step 59350: train loss 2.2987, val loss 2.4085\n",
      "step 59360: train loss 2.5451, val loss 2.3379\n",
      "step 59370: train loss 2.3425, val loss 2.3687\n",
      "step 59380: train loss 2.5008, val loss 2.3794\n",
      "step 59390: train loss 2.3883, val loss 2.4005\n",
      "step 59400: train loss 2.4217, val loss 2.3706\n",
      "step 59410: train loss 2.4197, val loss 2.3559\n",
      "step 59420: train loss 2.3977, val loss 2.3176\n",
      "step 59430: train loss 2.3555, val loss 2.4075\n",
      "step 59440: train loss 2.4256, val loss 2.3682\n",
      "step 59450: train loss 2.3484, val loss 2.3102\n",
      "step 59460: train loss 2.3543, val loss 2.3022\n",
      "step 59470: train loss 2.3882, val loss 2.4152\n",
      "step 59480: train loss 2.3914, val loss 2.3888\n",
      "step 59490: train loss 2.3979, val loss 2.3331\n",
      "step 59500: train loss 2.3335, val loss 2.3103\n",
      "step 59510: train loss 2.4432, val loss 2.4217\n",
      "step 59520: train loss 2.4275, val loss 2.4122\n",
      "step 59530: train loss 2.3899, val loss 2.3229\n",
      "step 59540: train loss 2.4014, val loss 2.3154\n",
      "step 59550: train loss 2.4178, val loss 2.3707\n",
      "step 59560: train loss 2.3244, val loss 2.3160\n",
      "step 59570: train loss 2.3690, val loss 2.3365\n",
      "step 59580: train loss 2.3638, val loss 2.3926\n",
      "step 59590: train loss 2.3770, val loss 2.3585\n",
      "step 59600: train loss 2.3376, val loss 2.3697\n",
      "step 59610: train loss 2.3964, val loss 2.3465\n",
      "step 59620: train loss 2.4014, val loss 2.3545\n",
      "step 59630: train loss 2.3872, val loss 2.3458\n",
      "step 59640: train loss 2.4721, val loss 2.4497\n",
      "step 59650: train loss 2.4055, val loss 2.4378\n",
      "step 59660: train loss 2.4333, val loss 2.3732\n",
      "step 59670: train loss 2.4417, val loss 2.3864\n",
      "step 59680: train loss 2.3521, val loss 2.3125\n",
      "step 59690: train loss 2.3462, val loss 2.3496\n",
      "step 59700: train loss 2.3872, val loss 2.3298\n",
      "step 59710: train loss 2.3345, val loss 2.3340\n",
      "step 59720: train loss 2.3777, val loss 2.3023\n",
      "step 59730: train loss 2.3899, val loss 2.3174\n",
      "step 59740: train loss 2.4338, val loss 2.3857\n",
      "step 59750: train loss 2.3520, val loss 2.3858\n",
      "step 59760: train loss 2.3872, val loss 2.3355\n",
      "step 59770: train loss 2.3940, val loss 2.3729\n",
      "step 59780: train loss 2.4052, val loss 2.3326\n",
      "step 59790: train loss 2.4142, val loss 2.3770\n",
      "step 59800: train loss 2.3547, val loss 2.3667\n",
      "step 59810: train loss 2.3601, val loss 2.2889\n",
      "step 59820: train loss 2.4284, val loss 2.3965\n",
      "step 59830: train loss 2.3750, val loss 2.4658\n",
      "step 59840: train loss 2.2924, val loss 2.4341\n",
      "step 59850: train loss 2.4176, val loss 2.3854\n",
      "step 59860: train loss 2.4368, val loss 2.3163\n",
      "step 59870: train loss 2.3210, val loss 2.3594\n",
      "step 59880: train loss 2.3961, val loss 2.3968\n",
      "step 59890: train loss 2.4024, val loss 2.3725\n",
      "step 59900: train loss 2.4143, val loss 2.3606\n",
      "step 59910: train loss 2.4018, val loss 2.3163\n",
      "step 59920: train loss 2.4534, val loss 2.3494\n",
      "step 59930: train loss 2.3582, val loss 2.3568\n",
      "step 59940: train loss 2.4553, val loss 2.3464\n",
      "step 59950: train loss 2.3672, val loss 2.3972\n",
      "step 59960: train loss 2.3358, val loss 2.2865\n",
      "step 59970: train loss 2.4351, val loss 2.3683\n",
      "step 59980: train loss 2.3628, val loss 2.3890\n",
      "step 59990: train loss 2.5086, val loss 2.3118\n",
      "step 60000: train loss 2.3226, val loss 2.2921\n",
      "step 60010: train loss 2.4257, val loss 2.4313\n",
      "step 60020: train loss 2.3874, val loss 2.3918\n",
      "step 60030: train loss 2.3502, val loss 2.3297\n",
      "step 60040: train loss 2.3755, val loss 2.3560\n",
      "step 60050: train loss 2.4518, val loss 2.3607\n",
      "step 60060: train loss 2.3150, val loss 2.4074\n",
      "step 60070: train loss 2.3185, val loss 2.3954\n",
      "step 60080: train loss 2.4428, val loss 2.4237\n",
      "step 60090: train loss 2.4784, val loss 2.3368\n",
      "step 60100: train loss 2.4465, val loss 2.3709\n",
      "step 60110: train loss 2.3943, val loss 2.3866\n",
      "step 60120: train loss 2.4551, val loss 2.4453\n",
      "step 60130: train loss 2.3572, val loss 2.3706\n",
      "step 60140: train loss 2.3708, val loss 2.4157\n",
      "step 60150: train loss 2.4156, val loss 2.3236\n",
      "step 60160: train loss 2.3644, val loss 2.3928\n",
      "step 60170: train loss 2.3162, val loss 2.3196\n",
      "step 60180: train loss 2.3525, val loss 2.3734\n",
      "step 60190: train loss 2.3953, val loss 2.3834\n",
      "step 60200: train loss 2.4440, val loss 2.3520\n",
      "step 60210: train loss 2.3929, val loss 2.4022\n",
      "step 60220: train loss 2.3658, val loss 2.3475\n",
      "step 60230: train loss 2.3514, val loss 2.3922\n",
      "step 60240: train loss 2.3407, val loss 2.3542\n",
      "step 60250: train loss 2.3612, val loss 2.3674\n",
      "step 60260: train loss 2.4081, val loss 2.3537\n",
      "step 60270: train loss 2.4350, val loss 2.3417\n",
      "step 60280: train loss 2.4221, val loss 2.4609\n",
      "step 60290: train loss 2.4066, val loss 2.3848\n",
      "step 60300: train loss 2.4083, val loss 2.3332\n",
      "step 60310: train loss 2.3657, val loss 2.3238\n",
      "step 60320: train loss 2.3983, val loss 2.3797\n",
      "step 60330: train loss 2.4776, val loss 2.4235\n",
      "step 60340: train loss 2.3430, val loss 2.2987\n",
      "step 60350: train loss 2.3775, val loss 2.4777\n",
      "step 60360: train loss 2.3483, val loss 2.3517\n",
      "step 60370: train loss 2.3519, val loss 2.2738\n",
      "step 60380: train loss 2.4845, val loss 2.3683\n",
      "step 60390: train loss 2.3583, val loss 2.3661\n",
      "step 60400: train loss 2.3638, val loss 2.4665\n",
      "step 60410: train loss 2.4189, val loss 2.3505\n",
      "step 60420: train loss 2.3647, val loss 2.4008\n",
      "step 60430: train loss 2.3719, val loss 2.3843\n",
      "step 60440: train loss 2.4180, val loss 2.3757\n",
      "step 60450: train loss 2.3448, val loss 2.3745\n",
      "step 60460: train loss 2.3674, val loss 2.3630\n",
      "step 60470: train loss 2.3681, val loss 2.3641\n",
      "step 60480: train loss 2.4481, val loss 2.3353\n",
      "step 60490: train loss 2.2953, val loss 2.3783\n",
      "step 60500: train loss 2.3692, val loss 2.3913\n",
      "step 60510: train loss 2.4650, val loss 2.4924\n",
      "step 60520: train loss 2.3538, val loss 2.3791\n",
      "step 60530: train loss 2.3711, val loss 2.3897\n",
      "step 60540: train loss 2.3913, val loss 2.3489\n",
      "step 60550: train loss 2.4147, val loss 2.3606\n",
      "step 60560: train loss 2.3235, val loss 2.3319\n",
      "step 60570: train loss 2.3711, val loss 2.2904\n",
      "step 60580: train loss 2.3568, val loss 2.3386\n",
      "step 60590: train loss 2.3493, val loss 2.3443\n",
      "step 60600: train loss 2.3563, val loss 2.2948\n",
      "step 60610: train loss 2.3888, val loss 2.4568\n",
      "step 60620: train loss 2.3680, val loss 2.2621\n",
      "step 60630: train loss 2.3790, val loss 2.3785\n",
      "step 60640: train loss 2.3926, val loss 2.3485\n",
      "step 60650: train loss 2.3340, val loss 2.3174\n",
      "step 60660: train loss 2.3529, val loss 2.3440\n",
      "step 60670: train loss 2.3339, val loss 2.3940\n",
      "step 60680: train loss 2.3969, val loss 2.3400\n",
      "step 60690: train loss 2.4017, val loss 2.3407\n",
      "step 60700: train loss 2.3258, val loss 2.4156\n",
      "step 60710: train loss 2.2757, val loss 2.3316\n",
      "step 60720: train loss 2.2975, val loss 2.4629\n",
      "step 60730: train loss 2.3575, val loss 2.4271\n",
      "step 60740: train loss 2.3022, val loss 2.4164\n",
      "step 60750: train loss 2.4112, val loss 2.3551\n",
      "step 60760: train loss 2.3521, val loss 2.4073\n",
      "step 60770: train loss 2.3231, val loss 2.3640\n",
      "step 60780: train loss 2.3025, val loss 2.3835\n",
      "step 60790: train loss 2.4322, val loss 2.3981\n",
      "step 60800: train loss 2.4459, val loss 2.3189\n",
      "step 60810: train loss 2.3794, val loss 2.3911\n",
      "step 60820: train loss 2.3398, val loss 2.3309\n",
      "step 60830: train loss 2.3791, val loss 2.3202\n",
      "step 60840: train loss 2.3379, val loss 2.3433\n",
      "step 60850: train loss 2.4131, val loss 2.3859\n",
      "step 60860: train loss 2.3388, val loss 2.3369\n",
      "step 60870: train loss 2.3566, val loss 2.4167\n",
      "step 60880: train loss 2.3533, val loss 2.4593\n",
      "step 60890: train loss 2.4573, val loss 2.4065\n",
      "step 60900: train loss 2.3722, val loss 2.4317\n",
      "step 60910: train loss 2.3656, val loss 2.4291\n",
      "step 60920: train loss 2.2958, val loss 2.3672\n",
      "step 60930: train loss 2.3248, val loss 2.3608\n",
      "step 60940: train loss 2.3835, val loss 2.3573\n",
      "step 60950: train loss 2.3644, val loss 2.3529\n",
      "step 60960: train loss 2.3377, val loss 2.4039\n",
      "step 60970: train loss 2.4014, val loss 2.3544\n",
      "step 60980: train loss 2.4611, val loss 2.3430\n",
      "step 60990: train loss 2.3585, val loss 2.4102\n",
      "step 61000: train loss 2.4148, val loss 2.3417\n",
      "step 61010: train loss 2.3831, val loss 2.3511\n",
      "step 61020: train loss 2.3613, val loss 2.3512\n",
      "step 61030: train loss 2.4463, val loss 2.3706\n",
      "step 61040: train loss 2.3130, val loss 2.3942\n",
      "step 61050: train loss 2.3157, val loss 2.3435\n",
      "step 61060: train loss 2.3647, val loss 2.3127\n",
      "step 61070: train loss 2.3695, val loss 2.3620\n",
      "step 61080: train loss 2.4255, val loss 2.3159\n",
      "step 61090: train loss 2.4025, val loss 2.4017\n",
      "step 61100: train loss 2.2856, val loss 2.3030\n",
      "step 61110: train loss 2.3832, val loss 2.3977\n",
      "step 61120: train loss 2.4289, val loss 2.3658\n",
      "step 61130: train loss 2.3456, val loss 2.3548\n",
      "step 61140: train loss 2.3517, val loss 2.3436\n",
      "step 61150: train loss 2.4686, val loss 2.3517\n",
      "step 61160: train loss 2.3889, val loss 2.3112\n",
      "step 61170: train loss 2.3782, val loss 2.3769\n",
      "step 61180: train loss 2.3384, val loss 2.3472\n",
      "step 61190: train loss 2.4221, val loss 2.3244\n",
      "step 61200: train loss 2.3625, val loss 2.2841\n",
      "step 61210: train loss 2.3493, val loss 2.3889\n",
      "step 61220: train loss 2.3687, val loss 2.3262\n",
      "step 61230: train loss 2.4116, val loss 2.3323\n",
      "step 61240: train loss 2.3585, val loss 2.3915\n",
      "step 61250: train loss 2.3013, val loss 2.3602\n",
      "step 61260: train loss 2.4316, val loss 2.3165\n",
      "step 61270: train loss 2.3905, val loss 2.3894\n",
      "step 61280: train loss 2.3947, val loss 2.3481\n",
      "step 61290: train loss 2.3093, val loss 2.4637\n",
      "step 61300: train loss 2.4493, val loss 2.3685\n",
      "step 61310: train loss 2.4261, val loss 2.3825\n",
      "step 61320: train loss 2.3680, val loss 2.3846\n",
      "step 61330: train loss 2.3563, val loss 2.3282\n",
      "step 61340: train loss 2.4633, val loss 2.3763\n",
      "step 61350: train loss 2.3247, val loss 2.3199\n",
      "step 61360: train loss 2.4066, val loss 2.3230\n",
      "step 61370: train loss 2.3860, val loss 2.3230\n",
      "step 61380: train loss 2.3545, val loss 2.3275\n",
      "step 61390: train loss 2.4032, val loss 2.2685\n",
      "step 61400: train loss 2.4466, val loss 2.3048\n",
      "step 61410: train loss 2.3792, val loss 2.3686\n",
      "step 61420: train loss 2.3824, val loss 2.2957\n",
      "step 61430: train loss 2.3503, val loss 2.3354\n",
      "step 61440: train loss 2.3906, val loss 2.3931\n",
      "step 61450: train loss 2.3790, val loss 2.4057\n",
      "step 61460: train loss 2.3420, val loss 2.3453\n",
      "step 61470: train loss 2.3383, val loss 2.3143\n",
      "step 61480: train loss 2.3970, val loss 2.3935\n",
      "step 61490: train loss 2.3383, val loss 2.3199\n",
      "step 61500: train loss 2.3956, val loss 2.3397\n",
      "step 61510: train loss 2.3433, val loss 2.2529\n",
      "step 61520: train loss 2.3744, val loss 2.3890\n",
      "step 61530: train loss 2.3086, val loss 2.3134\n",
      "step 61540: train loss 2.3650, val loss 2.3439\n",
      "step 61550: train loss 2.4264, val loss 2.2908\n",
      "step 61560: train loss 2.4391, val loss 2.3354\n",
      "step 61570: train loss 2.3955, val loss 2.4062\n",
      "step 61580: train loss 2.3662, val loss 2.4197\n",
      "step 61590: train loss 2.2617, val loss 2.3661\n",
      "step 61600: train loss 2.4523, val loss 2.2841\n",
      "step 61610: train loss 2.3250, val loss 2.3617\n",
      "step 61620: train loss 2.4503, val loss 2.3760\n",
      "step 61630: train loss 2.3399, val loss 2.3705\n",
      "step 61640: train loss 2.4114, val loss 2.4128\n",
      "step 61650: train loss 2.3440, val loss 2.4588\n",
      "step 61660: train loss 2.4197, val loss 2.3869\n",
      "step 61670: train loss 2.3297, val loss 2.3432\n",
      "step 61680: train loss 2.3773, val loss 2.3680\n",
      "step 61690: train loss 2.3537, val loss 2.3270\n",
      "step 61700: train loss 2.3831, val loss 2.3301\n",
      "step 61710: train loss 2.4280, val loss 2.4262\n",
      "step 61720: train loss 2.4137, val loss 2.3785\n",
      "step 61730: train loss 2.2984, val loss 2.4022\n",
      "step 61740: train loss 2.3769, val loss 2.3627\n",
      "step 61750: train loss 2.2839, val loss 2.3179\n",
      "step 61760: train loss 2.2956, val loss 2.2760\n",
      "step 61770: train loss 2.4037, val loss 2.3215\n",
      "step 61780: train loss 2.4177, val loss 2.3680\n",
      "step 61790: train loss 2.4406, val loss 2.3030\n",
      "step 61800: train loss 2.3814, val loss 2.4566\n",
      "step 61810: train loss 2.3997, val loss 2.3376\n",
      "step 61820: train loss 2.3844, val loss 2.3769\n",
      "step 61830: train loss 2.3338, val loss 2.3839\n",
      "step 61840: train loss 2.3404, val loss 2.3317\n",
      "step 61850: train loss 2.4409, val loss 2.3601\n",
      "step 61860: train loss 2.3764, val loss 2.4642\n",
      "step 61870: train loss 2.3401, val loss 2.3056\n",
      "step 61880: train loss 2.3211, val loss 2.3705\n",
      "step 61890: train loss 2.3529, val loss 2.3533\n",
      "step 61900: train loss 2.3526, val loss 2.5036\n",
      "step 61910: train loss 2.4125, val loss 2.3965\n",
      "step 61920: train loss 2.4290, val loss 2.3492\n",
      "step 61930: train loss 2.3666, val loss 2.3371\n",
      "step 61940: train loss 2.4081, val loss 2.3919\n",
      "step 61950: train loss 2.4093, val loss 2.3455\n",
      "step 61960: train loss 2.3565, val loss 2.2563\n",
      "step 61970: train loss 2.3667, val loss 2.3667\n",
      "step 61980: train loss 2.4679, val loss 2.4187\n",
      "step 61990: train loss 2.3220, val loss 2.4075\n",
      "step 62000: train loss 2.2952, val loss 2.3307\n",
      "step 62010: train loss 2.4365, val loss 2.3760\n",
      "step 62020: train loss 2.3680, val loss 2.3695\n",
      "step 62030: train loss 2.3911, val loss 2.3822\n",
      "step 62040: train loss 2.2931, val loss 2.4056\n",
      "step 62050: train loss 2.3742, val loss 2.3337\n",
      "step 62060: train loss 2.4014, val loss 2.3545\n",
      "step 62070: train loss 2.3697, val loss 2.3941\n",
      "step 62080: train loss 2.3972, val loss 2.3127\n",
      "step 62090: train loss 2.4237, val loss 2.3683\n",
      "step 62100: train loss 2.3509, val loss 2.3470\n",
      "step 62110: train loss 2.3872, val loss 2.3579\n",
      "step 62120: train loss 2.4313, val loss 2.3438\n",
      "step 62130: train loss 2.3343, val loss 2.3826\n",
      "step 62140: train loss 2.2790, val loss 2.3597\n",
      "step 62150: train loss 2.3714, val loss 2.3895\n",
      "step 62160: train loss 2.3844, val loss 2.3101\n",
      "step 62170: train loss 2.3542, val loss 2.4139\n",
      "step 62180: train loss 2.3408, val loss 2.3149\n",
      "step 62190: train loss 2.3672, val loss 2.3402\n",
      "step 62200: train loss 2.3410, val loss 2.3797\n",
      "step 62210: train loss 2.3925, val loss 2.4316\n",
      "step 62220: train loss 2.4018, val loss 2.3635\n",
      "step 62230: train loss 2.4016, val loss 2.3410\n",
      "step 62240: train loss 2.3829, val loss 2.3802\n",
      "step 62250: train loss 2.4076, val loss 2.3764\n",
      "step 62260: train loss 2.4126, val loss 2.4101\n",
      "step 62270: train loss 2.4419, val loss 2.3586\n",
      "step 62280: train loss 2.3875, val loss 2.4193\n",
      "step 62290: train loss 2.3357, val loss 2.3172\n",
      "step 62300: train loss 2.3659, val loss 2.4031\n",
      "step 62310: train loss 2.4213, val loss 2.3479\n",
      "step 62320: train loss 2.4051, val loss 2.3787\n",
      "step 62330: train loss 2.4466, val loss 2.3371\n",
      "step 62340: train loss 2.4374, val loss 2.3365\n",
      "step 62350: train loss 2.3932, val loss 2.3765\n",
      "step 62360: train loss 2.4013, val loss 2.4427\n",
      "step 62370: train loss 2.3455, val loss 2.3833\n",
      "step 62380: train loss 2.3863, val loss 2.3353\n",
      "step 62390: train loss 2.3669, val loss 2.3741\n",
      "step 62400: train loss 2.3618, val loss 2.4083\n",
      "step 62410: train loss 2.3367, val loss 2.3679\n",
      "step 62420: train loss 2.4096, val loss 2.3677\n",
      "step 62430: train loss 2.3797, val loss 2.3605\n",
      "step 62440: train loss 2.3688, val loss 2.3156\n",
      "step 62450: train loss 2.3536, val loss 2.3154\n",
      "step 62460: train loss 2.3616, val loss 2.3914\n",
      "step 62470: train loss 2.4147, val loss 2.4037\n",
      "step 62480: train loss 2.3380, val loss 2.3934\n",
      "step 62490: train loss 2.3310, val loss 2.3319\n",
      "step 62500: train loss 2.3851, val loss 2.4508\n",
      "step 62510: train loss 2.4168, val loss 2.3929\n",
      "step 62520: train loss 2.4088, val loss 2.4185\n",
      "step 62530: train loss 2.4504, val loss 2.3870\n",
      "step 62540: train loss 2.4268, val loss 2.3096\n",
      "step 62550: train loss 2.3452, val loss 2.3822\n",
      "step 62560: train loss 2.4413, val loss 2.4023\n",
      "step 62570: train loss 2.3220, val loss 2.3006\n",
      "step 62580: train loss 2.4281, val loss 2.3986\n",
      "step 62590: train loss 2.3781, val loss 2.4149\n",
      "step 62600: train loss 2.3931, val loss 2.4616\n",
      "step 62610: train loss 2.3530, val loss 2.3814\n",
      "step 62620: train loss 2.3107, val loss 2.3817\n",
      "step 62630: train loss 2.4119, val loss 2.3548\n",
      "step 62640: train loss 2.3543, val loss 2.3836\n",
      "step 62650: train loss 2.3811, val loss 2.3271\n",
      "step 62660: train loss 2.4283, val loss 2.3885\n",
      "step 62670: train loss 2.4860, val loss 2.2930\n",
      "step 62680: train loss 2.3882, val loss 2.3329\n",
      "step 62690: train loss 2.3374, val loss 2.3438\n",
      "step 62700: train loss 2.3343, val loss 2.4106\n",
      "step 62710: train loss 2.4210, val loss 2.3541\n",
      "step 62720: train loss 2.3848, val loss 2.3583\n",
      "step 62730: train loss 2.3624, val loss 2.3149\n",
      "step 62740: train loss 2.4169, val loss 2.3276\n",
      "step 62750: train loss 2.2926, val loss 2.3059\n",
      "step 62760: train loss 2.3206, val loss 2.3917\n",
      "step 62770: train loss 2.4508, val loss 2.3414\n",
      "step 62780: train loss 2.4197, val loss 2.4266\n",
      "step 62790: train loss 2.3919, val loss 2.3018\n",
      "step 62800: train loss 2.3384, val loss 2.4011\n",
      "step 62810: train loss 2.3632, val loss 2.3161\n",
      "step 62820: train loss 2.3382, val loss 2.3727\n",
      "step 62830: train loss 2.3650, val loss 2.3963\n",
      "step 62840: train loss 2.3185, val loss 2.3889\n",
      "step 62850: train loss 2.4757, val loss 2.3845\n",
      "step 62860: train loss 2.4423, val loss 2.3099\n",
      "step 62870: train loss 2.3872, val loss 2.3714\n",
      "step 62880: train loss 2.4316, val loss 2.3429\n",
      "step 62890: train loss 2.3191, val loss 2.4676\n",
      "step 62900: train loss 2.4079, val loss 2.4521\n",
      "step 62910: train loss 2.4436, val loss 2.4138\n",
      "step 62920: train loss 2.4101, val loss 2.3503\n",
      "step 62930: train loss 2.3516, val loss 2.3685\n",
      "step 62940: train loss 2.4075, val loss 2.4171\n",
      "step 62950: train loss 2.3841, val loss 2.4005\n",
      "step 62960: train loss 2.3970, val loss 2.3325\n",
      "step 62970: train loss 2.3997, val loss 2.3370\n",
      "step 62980: train loss 2.3270, val loss 2.2969\n",
      "step 62990: train loss 2.4156, val loss 2.3130\n",
      "step 63000: train loss 2.3631, val loss 2.3600\n",
      "step 63010: train loss 2.3417, val loss 2.2980\n",
      "step 63020: train loss 2.4231, val loss 2.3546\n",
      "step 63030: train loss 2.3928, val loss 2.3068\n",
      "step 63040: train loss 2.3760, val loss 2.3456\n",
      "step 63050: train loss 2.3900, val loss 2.3223\n",
      "step 63060: train loss 2.4231, val loss 2.3687\n",
      "step 63070: train loss 2.4539, val loss 2.3464\n",
      "step 63080: train loss 2.3340, val loss 2.3620\n",
      "step 63090: train loss 2.3427, val loss 2.3533\n",
      "step 63100: train loss 2.3571, val loss 2.3233\n",
      "step 63110: train loss 2.3279, val loss 2.3948\n",
      "step 63120: train loss 2.3988, val loss 2.3357\n",
      "step 63130: train loss 2.3694, val loss 2.3024\n",
      "step 63140: train loss 2.3634, val loss 2.4116\n",
      "step 63150: train loss 2.3825, val loss 2.3545\n",
      "step 63160: train loss 2.4301, val loss 2.3839\n",
      "step 63170: train loss 2.3139, val loss 2.3815\n",
      "step 63180: train loss 2.3560, val loss 2.3608\n",
      "step 63190: train loss 2.3233, val loss 2.3830\n",
      "step 63200: train loss 2.4526, val loss 2.2836\n",
      "step 63210: train loss 2.3830, val loss 2.3868\n",
      "step 63220: train loss 2.3279, val loss 2.3410\n",
      "step 63230: train loss 2.3583, val loss 2.3997\n",
      "step 63240: train loss 2.3945, val loss 2.3277\n",
      "step 63250: train loss 2.3599, val loss 2.2962\n",
      "step 63260: train loss 2.3420, val loss 2.3297\n",
      "step 63270: train loss 2.3141, val loss 2.3993\n",
      "step 63280: train loss 2.4216, val loss 2.3879\n",
      "step 63290: train loss 2.3085, val loss 2.3257\n",
      "step 63300: train loss 2.3990, val loss 2.3730\n",
      "step 63310: train loss 2.3453, val loss 2.3569\n",
      "step 63320: train loss 2.3414, val loss 2.4106\n",
      "step 63330: train loss 2.4483, val loss 2.3982\n",
      "step 63340: train loss 2.3663, val loss 2.3319\n",
      "step 63350: train loss 2.3642, val loss 2.2856\n",
      "step 63360: train loss 2.4248, val loss 2.3331\n",
      "step 63370: train loss 2.3110, val loss 2.3306\n",
      "step 63380: train loss 2.3724, val loss 2.4113\n",
      "step 63390: train loss 2.4128, val loss 2.4513\n",
      "step 63400: train loss 2.4652, val loss 2.4154\n",
      "step 63410: train loss 2.4034, val loss 2.3928\n",
      "step 63420: train loss 2.4037, val loss 2.3905\n",
      "step 63430: train loss 2.4267, val loss 2.3796\n",
      "step 63440: train loss 2.3890, val loss 2.3528\n",
      "step 63450: train loss 2.3358, val loss 2.4446\n",
      "step 63460: train loss 2.4220, val loss 2.4007\n",
      "step 63470: train loss 2.4087, val loss 2.3013\n",
      "step 63480: train loss 2.4057, val loss 2.3006\n",
      "step 63490: train loss 2.4096, val loss 2.3605\n",
      "step 63500: train loss 2.3763, val loss 2.3689\n",
      "step 63510: train loss 2.3541, val loss 2.3285\n",
      "step 63520: train loss 2.3624, val loss 2.3876\n",
      "step 63530: train loss 2.4241, val loss 2.3329\n",
      "step 63540: train loss 2.3687, val loss 2.4237\n",
      "step 63550: train loss 2.3715, val loss 2.3410\n",
      "step 63560: train loss 2.3208, val loss 2.3789\n",
      "step 63570: train loss 2.3607, val loss 2.4059\n",
      "step 63580: train loss 2.4161, val loss 2.4044\n",
      "step 63590: train loss 2.3746, val loss 2.4193\n",
      "step 63600: train loss 2.3684, val loss 2.3414\n",
      "step 63610: train loss 2.4864, val loss 2.3034\n",
      "step 63620: train loss 2.4238, val loss 2.3367\n",
      "step 63630: train loss 2.3558, val loss 2.3396\n",
      "step 63640: train loss 2.3835, val loss 2.4146\n",
      "step 63650: train loss 2.4356, val loss 2.3355\n",
      "step 63660: train loss 2.3478, val loss 2.3571\n",
      "step 63670: train loss 2.3393, val loss 2.3345\n",
      "step 63680: train loss 2.4009, val loss 2.3747\n",
      "step 63690: train loss 2.3622, val loss 2.3982\n",
      "step 63700: train loss 2.3692, val loss 2.3342\n",
      "step 63710: train loss 2.3910, val loss 2.3972\n",
      "step 63720: train loss 2.3968, val loss 2.2977\n",
      "step 63730: train loss 2.3435, val loss 2.3776\n",
      "step 63740: train loss 2.3678, val loss 2.3599\n",
      "step 63750: train loss 2.4363, val loss 2.3750\n",
      "step 63760: train loss 2.3650, val loss 2.3501\n",
      "step 63770: train loss 2.3586, val loss 2.3478\n",
      "step 63780: train loss 2.3305, val loss 2.4054\n",
      "step 63790: train loss 2.4063, val loss 2.3541\n",
      "step 63800: train loss 2.4018, val loss 2.3741\n",
      "step 63810: train loss 2.3163, val loss 2.3976\n",
      "step 63820: train loss 2.4039, val loss 2.3965\n",
      "step 63830: train loss 2.4015, val loss 2.3730\n",
      "step 63840: train loss 2.3809, val loss 2.4282\n",
      "step 63850: train loss 2.3750, val loss 2.3665\n",
      "step 63860: train loss 2.3468, val loss 2.3209\n",
      "step 63870: train loss 2.4001, val loss 2.3903\n",
      "step 63880: train loss 2.2994, val loss 2.3871\n",
      "step 63890: train loss 2.3876, val loss 2.3401\n",
      "step 63900: train loss 2.2994, val loss 2.3423\n",
      "step 63910: train loss 2.3689, val loss 2.3769\n",
      "step 63920: train loss 2.3860, val loss 2.4557\n",
      "step 63930: train loss 2.4241, val loss 2.3997\n",
      "step 63940: train loss 2.2972, val loss 2.4245\n",
      "step 63950: train loss 2.4352, val loss 2.3652\n",
      "step 63960: train loss 2.3725, val loss 2.3933\n",
      "step 63970: train loss 2.3280, val loss 2.3945\n",
      "step 63980: train loss 2.3659, val loss 2.3462\n",
      "step 63990: train loss 2.3819, val loss 2.3385\n",
      "step 64000: train loss 2.3782, val loss 2.3966\n",
      "step 64010: train loss 2.3125, val loss 2.3119\n",
      "step 64020: train loss 2.4367, val loss 2.5038\n",
      "step 64030: train loss 2.3012, val loss 2.4439\n",
      "step 64040: train loss 2.3523, val loss 2.3495\n",
      "step 64050: train loss 2.3746, val loss 2.3652\n",
      "step 64060: train loss 2.3882, val loss 2.3543\n",
      "step 64070: train loss 2.4117, val loss 2.2947\n",
      "step 64080: train loss 2.3800, val loss 2.3246\n",
      "step 64090: train loss 2.3804, val loss 2.3965\n",
      "step 64100: train loss 2.3766, val loss 2.3118\n",
      "step 64110: train loss 2.4151, val loss 2.3864\n",
      "step 64120: train loss 2.3603, val loss 2.3912\n",
      "step 64130: train loss 2.3557, val loss 2.3674\n",
      "step 64140: train loss 2.4135, val loss 2.3672\n",
      "step 64150: train loss 2.3561, val loss 2.4218\n",
      "step 64160: train loss 2.3948, val loss 2.3962\n",
      "step 64170: train loss 2.3583, val loss 2.3840\n",
      "step 64180: train loss 2.2845, val loss 2.3076\n",
      "step 64190: train loss 2.4093, val loss 2.3780\n",
      "step 64200: train loss 2.3730, val loss 2.3226\n",
      "step 64210: train loss 2.3732, val loss 2.3842\n",
      "step 64220: train loss 2.4060, val loss 2.3690\n",
      "step 64230: train loss 2.4090, val loss 2.3875\n",
      "step 64240: train loss 2.3659, val loss 2.3074\n",
      "step 64250: train loss 2.3887, val loss 2.3597\n",
      "step 64260: train loss 2.2955, val loss 2.3696\n",
      "step 64270: train loss 2.4122, val loss 2.3985\n",
      "step 64280: train loss 2.3768, val loss 2.4269\n",
      "step 64290: train loss 2.3723, val loss 2.3822\n",
      "step 64300: train loss 2.3460, val loss 2.3773\n",
      "step 64310: train loss 2.4214, val loss 2.3926\n",
      "step 64320: train loss 2.4329, val loss 2.3398\n",
      "step 64330: train loss 2.3785, val loss 2.3929\n",
      "step 64340: train loss 2.3893, val loss 2.3626\n",
      "step 64350: train loss 2.3100, val loss 2.3276\n",
      "step 64360: train loss 2.2810, val loss 2.3932\n",
      "step 64370: train loss 2.3432, val loss 2.3627\n",
      "step 64380: train loss 2.3593, val loss 2.3253\n",
      "step 64390: train loss 2.3564, val loss 2.3267\n",
      "step 64400: train loss 2.3647, val loss 2.2860\n",
      "step 64410: train loss 2.3782, val loss 2.3894\n",
      "step 64420: train loss 2.4701, val loss 2.3646\n",
      "step 64430: train loss 2.3257, val loss 2.2969\n",
      "step 64440: train loss 2.3845, val loss 2.4312\n",
      "step 64450: train loss 2.3831, val loss 2.2881\n",
      "step 64460: train loss 2.3306, val loss 2.3442\n",
      "step 64470: train loss 2.3576, val loss 2.4445\n",
      "step 64480: train loss 2.3473, val loss 2.4102\n",
      "step 64490: train loss 2.3481, val loss 2.4457\n",
      "step 64500: train loss 2.3644, val loss 2.4054\n",
      "step 64510: train loss 2.4892, val loss 2.3098\n",
      "step 64520: train loss 2.3635, val loss 2.3167\n",
      "step 64530: train loss 2.3385, val loss 2.3733\n",
      "step 64540: train loss 2.3383, val loss 2.4311\n",
      "step 64550: train loss 2.3825, val loss 2.3538\n",
      "step 64560: train loss 2.3901, val loss 2.3421\n",
      "step 64570: train loss 2.4190, val loss 2.3550\n",
      "step 64580: train loss 2.3874, val loss 2.3984\n",
      "step 64590: train loss 2.3898, val loss 2.3616\n",
      "step 64600: train loss 2.4345, val loss 2.3871\n",
      "step 64610: train loss 2.3033, val loss 2.3219\n",
      "step 64620: train loss 2.3664, val loss 2.3473\n",
      "step 64630: train loss 2.3750, val loss 2.3005\n",
      "step 64640: train loss 2.3481, val loss 2.4416\n",
      "step 64650: train loss 2.3838, val loss 2.3914\n",
      "step 64660: train loss 2.4005, val loss 2.3883\n",
      "step 64670: train loss 2.3064, val loss 2.4442\n",
      "step 64680: train loss 2.3660, val loss 2.3334\n",
      "step 64690: train loss 2.3925, val loss 2.3042\n",
      "step 64700: train loss 2.3637, val loss 2.3521\n",
      "step 64710: train loss 2.3395, val loss 2.3771\n",
      "step 64720: train loss 2.3210, val loss 2.3519\n",
      "step 64730: train loss 2.3520, val loss 2.3385\n",
      "step 64740: train loss 2.4566, val loss 2.3732\n",
      "step 64750: train loss 2.3062, val loss 2.3904\n",
      "step 64760: train loss 2.2925, val loss 2.3319\n",
      "step 64770: train loss 2.3399, val loss 2.3689\n",
      "step 64780: train loss 2.4317, val loss 2.3731\n",
      "step 64790: train loss 2.4019, val loss 2.3632\n",
      "step 64800: train loss 2.4474, val loss 2.3799\n",
      "step 64810: train loss 2.2940, val loss 2.3831\n",
      "step 64820: train loss 2.3750, val loss 2.3438\n",
      "step 64830: train loss 2.3768, val loss 2.2936\n",
      "step 64840: train loss 2.4496, val loss 2.3922\n",
      "step 64850: train loss 2.3945, val loss 2.3413\n",
      "step 64860: train loss 2.4280, val loss 2.3544\n",
      "step 64870: train loss 2.3512, val loss 2.4179\n",
      "step 64880: train loss 2.3923, val loss 2.3690\n",
      "step 64890: train loss 2.3923, val loss 2.4293\n",
      "step 64900: train loss 2.3243, val loss 2.4482\n",
      "step 64910: train loss 2.3604, val loss 2.3751\n",
      "step 64920: train loss 2.4227, val loss 2.3872\n",
      "step 64930: train loss 2.4313, val loss 2.3597\n",
      "step 64940: train loss 2.3855, val loss 2.3919\n",
      "step 64950: train loss 2.3132, val loss 2.4540\n",
      "step 64960: train loss 2.3809, val loss 2.4201\n",
      "step 64970: train loss 2.3916, val loss 2.3865\n",
      "step 64980: train loss 2.3059, val loss 2.4156\n",
      "step 64990: train loss 2.3647, val loss 2.3857\n",
      "step 65000: train loss 2.3447, val loss 2.2939\n",
      "step 65010: train loss 2.4238, val loss 2.3880\n",
      "step 65020: train loss 2.3321, val loss 2.3146\n",
      "step 65030: train loss 2.3407, val loss 2.3071\n",
      "step 65040: train loss 2.3611, val loss 2.3810\n",
      "step 65050: train loss 2.3368, val loss 2.3921\n",
      "step 65060: train loss 2.3038, val loss 2.3788\n",
      "step 65070: train loss 2.3991, val loss 2.3811\n",
      "step 65080: train loss 2.3886, val loss 2.4110\n",
      "step 65090: train loss 2.3812, val loss 2.3819\n",
      "step 65100: train loss 2.3592, val loss 2.3117\n",
      "step 65110: train loss 2.3751, val loss 2.3530\n",
      "step 65120: train loss 2.3789, val loss 2.2959\n",
      "step 65130: train loss 2.4478, val loss 2.3245\n",
      "step 65140: train loss 2.4122, val loss 2.3245\n",
      "step 65150: train loss 2.4421, val loss 2.3709\n",
      "step 65160: train loss 2.2875, val loss 2.3787\n",
      "step 65170: train loss 2.3499, val loss 2.3832\n",
      "step 65180: train loss 2.5116, val loss 2.3620\n",
      "step 65190: train loss 2.3723, val loss 2.3187\n",
      "step 65200: train loss 2.3648, val loss 2.3765\n",
      "step 65210: train loss 2.3489, val loss 2.4699\n",
      "step 65220: train loss 2.3793, val loss 2.3707\n",
      "step 65230: train loss 2.3759, val loss 2.4341\n",
      "step 65240: train loss 2.4263, val loss 2.4280\n",
      "step 65250: train loss 2.3475, val loss 2.3700\n",
      "step 65260: train loss 2.4165, val loss 2.3683\n",
      "step 65270: train loss 2.3158, val loss 2.3667\n",
      "step 65280: train loss 2.3436, val loss 2.3780\n",
      "step 65290: train loss 2.3474, val loss 2.4265\n",
      "step 65300: train loss 2.4268, val loss 2.3660\n",
      "step 65310: train loss 2.3942, val loss 2.3443\n",
      "step 65320: train loss 2.4000, val loss 2.4036\n",
      "step 65330: train loss 2.4140, val loss 2.3777\n",
      "step 65340: train loss 2.4005, val loss 2.3357\n",
      "step 65350: train loss 2.4212, val loss 2.3670\n",
      "step 65360: train loss 2.3571, val loss 2.3840\n",
      "step 65370: train loss 2.4120, val loss 2.3221\n",
      "step 65380: train loss 2.3991, val loss 2.3533\n",
      "step 65390: train loss 2.3476, val loss 2.3799\n",
      "step 65400: train loss 2.3698, val loss 2.3167\n",
      "step 65410: train loss 2.4329, val loss 2.4402\n",
      "step 65420: train loss 2.3673, val loss 2.3580\n",
      "step 65430: train loss 2.3613, val loss 2.3424\n",
      "step 65440: train loss 2.3432, val loss 2.3742\n",
      "step 65450: train loss 2.3612, val loss 2.3851\n",
      "step 65460: train loss 2.3897, val loss 2.3820\n",
      "step 65470: train loss 2.3768, val loss 2.4016\n",
      "step 65480: train loss 2.3891, val loss 2.3171\n",
      "step 65490: train loss 2.4263, val loss 2.3496\n",
      "step 65500: train loss 2.4005, val loss 2.4163\n",
      "step 65510: train loss 2.3134, val loss 2.3808\n",
      "step 65520: train loss 2.3556, val loss 2.3145\n",
      "step 65530: train loss 2.4186, val loss 2.4077\n",
      "step 65540: train loss 2.3820, val loss 2.3599\n",
      "step 65550: train loss 2.3894, val loss 2.3424\n",
      "step 65560: train loss 2.3536, val loss 2.4561\n",
      "step 65570: train loss 2.3938, val loss 2.4459\n",
      "step 65580: train loss 2.4124, val loss 2.4387\n",
      "step 65590: train loss 2.3373, val loss 2.3827\n",
      "step 65600: train loss 2.3996, val loss 2.3790\n",
      "step 65610: train loss 2.4586, val loss 2.3885\n",
      "step 65620: train loss 2.3133, val loss 2.4661\n",
      "step 65630: train loss 2.4213, val loss 2.3109\n",
      "step 65640: train loss 2.2880, val loss 2.4341\n",
      "step 65650: train loss 2.4253, val loss 2.3528\n",
      "step 65660: train loss 2.3718, val loss 2.3065\n",
      "step 65670: train loss 2.3483, val loss 2.4133\n",
      "step 65680: train loss 2.4236, val loss 2.3854\n",
      "step 65690: train loss 2.3063, val loss 2.3984\n",
      "step 65700: train loss 2.3785, val loss 2.4104\n",
      "step 65710: train loss 2.3759, val loss 2.3342\n",
      "step 65720: train loss 2.3512, val loss 2.3942\n",
      "step 65730: train loss 2.3966, val loss 2.3763\n",
      "step 65740: train loss 2.3776, val loss 2.3937\n",
      "step 65750: train loss 2.3562, val loss 2.3531\n",
      "step 65760: train loss 2.4749, val loss 2.3896\n",
      "step 65770: train loss 2.3742, val loss 2.3521\n",
      "step 65780: train loss 2.3736, val loss 2.3878\n",
      "step 65790: train loss 2.3969, val loss 2.3435\n",
      "step 65800: train loss 2.3920, val loss 2.4039\n",
      "step 65810: train loss 2.3492, val loss 2.3138\n",
      "step 65820: train loss 2.4290, val loss 2.3448\n",
      "step 65830: train loss 2.3616, val loss 2.3693\n",
      "step 65840: train loss 2.3998, val loss 2.3457\n",
      "step 65850: train loss 2.3674, val loss 2.3806\n",
      "step 65860: train loss 2.3923, val loss 2.3449\n",
      "step 65870: train loss 2.3678, val loss 2.3964\n",
      "step 65880: train loss 2.3470, val loss 2.3854\n",
      "step 65890: train loss 2.3227, val loss 2.3259\n",
      "step 65900: train loss 2.4014, val loss 2.3582\n",
      "step 65910: train loss 2.3344, val loss 2.4125\n",
      "step 65920: train loss 2.3417, val loss 2.3676\n",
      "step 65930: train loss 2.3815, val loss 2.3761\n",
      "step 65940: train loss 2.4010, val loss 2.3465\n",
      "step 65950: train loss 2.3295, val loss 2.3282\n",
      "step 65960: train loss 2.3643, val loss 2.3450\n",
      "step 65970: train loss 2.3170, val loss 2.3221\n",
      "step 65980: train loss 2.4637, val loss 2.3726\n",
      "step 65990: train loss 2.3633, val loss 2.3390\n",
      "step 66000: train loss 2.3790, val loss 2.3288\n",
      "step 66010: train loss 2.3978, val loss 2.3369\n",
      "step 66020: train loss 2.3889, val loss 2.3157\n",
      "step 66030: train loss 2.4170, val loss 2.3404\n",
      "step 66040: train loss 2.3673, val loss 2.3155\n",
      "step 66050: train loss 2.4225, val loss 2.3540\n",
      "step 66060: train loss 2.4353, val loss 2.4124\n",
      "step 66070: train loss 2.3640, val loss 2.3565\n",
      "step 66080: train loss 2.3774, val loss 2.4405\n",
      "step 66090: train loss 2.3692, val loss 2.4030\n",
      "step 66100: train loss 2.3448, val loss 2.4148\n",
      "step 66110: train loss 2.3507, val loss 2.4026\n",
      "step 66120: train loss 2.3847, val loss 2.3273\n",
      "step 66130: train loss 2.3478, val loss 2.3644\n",
      "step 66140: train loss 2.3451, val loss 2.4312\n",
      "step 66150: train loss 2.4028, val loss 2.3536\n",
      "step 66160: train loss 2.4436, val loss 2.3592\n",
      "step 66170: train loss 2.3547, val loss 2.3974\n",
      "step 66180: train loss 2.3581, val loss 2.4067\n",
      "step 66190: train loss 2.4347, val loss 2.3443\n",
      "step 66200: train loss 2.3919, val loss 2.4484\n",
      "step 66210: train loss 2.4246, val loss 2.3166\n",
      "step 66220: train loss 2.3874, val loss 2.3577\n",
      "step 66230: train loss 2.3683, val loss 2.3652\n",
      "step 66240: train loss 2.3533, val loss 2.3466\n",
      "step 66250: train loss 2.3461, val loss 2.3405\n",
      "step 66260: train loss 2.3003, val loss 2.4248\n",
      "step 66270: train loss 2.3431, val loss 2.3270\n",
      "step 66280: train loss 2.3633, val loss 2.3594\n",
      "step 66290: train loss 2.3774, val loss 2.3026\n",
      "step 66300: train loss 2.3198, val loss 2.3273\n",
      "step 66310: train loss 2.3805, val loss 2.3647\n",
      "step 66320: train loss 2.4676, val loss 2.3807\n",
      "step 66330: train loss 2.3640, val loss 2.3694\n",
      "step 66340: train loss 2.4419, val loss 2.3007\n",
      "step 66350: train loss 2.3298, val loss 2.3631\n",
      "step 66360: train loss 2.3479, val loss 2.3422\n",
      "step 66370: train loss 2.4144, val loss 2.3167\n",
      "step 66380: train loss 2.3506, val loss 2.3755\n",
      "step 66390: train loss 2.4139, val loss 2.4487\n",
      "step 66400: train loss 2.3134, val loss 2.3380\n",
      "step 66410: train loss 2.3792, val loss 2.3514\n",
      "step 66420: train loss 2.4492, val loss 2.3477\n",
      "step 66430: train loss 2.4133, val loss 2.3526\n",
      "step 66440: train loss 2.3266, val loss 2.3775\n",
      "step 66450: train loss 2.3926, val loss 2.3561\n",
      "step 66460: train loss 2.3727, val loss 2.3098\n",
      "step 66470: train loss 2.3525, val loss 2.4201\n",
      "step 66480: train loss 2.3878, val loss 2.3611\n",
      "step 66490: train loss 2.3842, val loss 2.3597\n",
      "step 66500: train loss 2.3949, val loss 2.3833\n",
      "step 66510: train loss 2.3758, val loss 2.4046\n",
      "step 66520: train loss 2.3069, val loss 2.3768\n",
      "step 66530: train loss 2.3441, val loss 2.3541\n",
      "step 66540: train loss 2.4477, val loss 2.2911\n",
      "step 66550: train loss 2.3163, val loss 2.3559\n",
      "step 66560: train loss 2.3589, val loss 2.4333\n",
      "step 66570: train loss 2.4362, val loss 2.4201\n",
      "step 66580: train loss 2.3720, val loss 2.3446\n",
      "step 66590: train loss 2.4204, val loss 2.3438\n",
      "step 66600: train loss 2.4401, val loss 2.3727\n",
      "step 66610: train loss 2.3476, val loss 2.3401\n",
      "step 66620: train loss 2.3492, val loss 2.3577\n",
      "step 66630: train loss 2.3863, val loss 2.4170\n",
      "step 66640: train loss 2.4273, val loss 2.3541\n",
      "step 66650: train loss 2.3714, val loss 2.4068\n",
      "step 66660: train loss 2.3653, val loss 2.3789\n",
      "step 66670: train loss 2.3804, val loss 2.3071\n",
      "step 66680: train loss 2.3492, val loss 2.3707\n",
      "step 66690: train loss 2.4049, val loss 2.4099\n",
      "step 66700: train loss 2.4298, val loss 2.3492\n",
      "step 66710: train loss 2.3996, val loss 2.3681\n",
      "step 66720: train loss 2.3876, val loss 2.3581\n",
      "step 66730: train loss 2.3767, val loss 2.2870\n",
      "step 66740: train loss 2.3823, val loss 2.3749\n",
      "step 66750: train loss 2.3539, val loss 2.3365\n",
      "step 66760: train loss 2.3899, val loss 2.3065\n",
      "step 66770: train loss 2.3419, val loss 2.3254\n",
      "step 66780: train loss 2.4318, val loss 2.3674\n",
      "step 66790: train loss 2.4301, val loss 2.4043\n",
      "step 66800: train loss 2.2539, val loss 2.3495\n",
      "step 66810: train loss 2.3889, val loss 2.4183\n",
      "step 66820: train loss 2.3746, val loss 2.4168\n",
      "step 66830: train loss 2.3657, val loss 2.3634\n",
      "step 66840: train loss 2.3318, val loss 2.2763\n",
      "step 66850: train loss 2.3561, val loss 2.4130\n",
      "step 66860: train loss 2.3923, val loss 2.3636\n",
      "step 66870: train loss 2.4433, val loss 2.3165\n",
      "step 66880: train loss 2.3683, val loss 2.3057\n",
      "step 66890: train loss 2.3180, val loss 2.3073\n",
      "step 66900: train loss 2.3731, val loss 2.2849\n",
      "step 66910: train loss 2.4011, val loss 2.3677\n",
      "step 66920: train loss 2.2491, val loss 2.4486\n",
      "step 66930: train loss 2.3661, val loss 2.4551\n",
      "step 66940: train loss 2.3848, val loss 2.3362\n",
      "step 66950: train loss 2.3405, val loss 2.3834\n",
      "step 66960: train loss 2.4089, val loss 2.3554\n",
      "step 66970: train loss 2.4248, val loss 2.3406\n",
      "step 66980: train loss 2.4146, val loss 2.3666\n",
      "step 66990: train loss 2.3340, val loss 2.4198\n",
      "step 67000: train loss 2.4231, val loss 2.4562\n",
      "step 67010: train loss 2.3368, val loss 2.5035\n",
      "step 67020: train loss 2.3053, val loss 2.3333\n",
      "step 67030: train loss 2.4209, val loss 2.3269\n",
      "step 67040: train loss 2.3332, val loss 2.3899\n",
      "step 67050: train loss 2.3605, val loss 2.2942\n",
      "step 67060: train loss 2.3693, val loss 2.4316\n",
      "step 67070: train loss 2.4211, val loss 2.3597\n",
      "step 67080: train loss 2.3454, val loss 2.3784\n",
      "step 67090: train loss 2.3455, val loss 2.2681\n",
      "step 67100: train loss 2.3614, val loss 2.3358\n",
      "step 67110: train loss 2.3784, val loss 2.3348\n",
      "step 67120: train loss 2.3956, val loss 2.3658\n",
      "step 67130: train loss 2.3593, val loss 2.2523\n",
      "step 67140: train loss 2.2896, val loss 2.5171\n",
      "step 67150: train loss 2.3470, val loss 2.3523\n",
      "step 67160: train loss 2.4679, val loss 2.3869\n",
      "step 67170: train loss 2.3878, val loss 2.4660\n",
      "step 67180: train loss 2.3812, val loss 2.4341\n",
      "step 67190: train loss 2.4275, val loss 2.4039\n",
      "step 67200: train loss 2.3277, val loss 2.3147\n",
      "step 67210: train loss 2.3436, val loss 2.4126\n",
      "step 67220: train loss 2.4027, val loss 2.4229\n",
      "step 67230: train loss 2.5180, val loss 2.2684\n",
      "step 67240: train loss 2.4552, val loss 2.3592\n",
      "step 67250: train loss 2.3418, val loss 2.3631\n",
      "step 67260: train loss 2.3908, val loss 2.3633\n",
      "step 67270: train loss 2.3120, val loss 2.3897\n",
      "step 67280: train loss 2.3447, val loss 2.4478\n",
      "step 67290: train loss 2.4051, val loss 2.4279\n",
      "step 67300: train loss 2.3566, val loss 2.3303\n",
      "step 67310: train loss 2.4419, val loss 2.3304\n",
      "step 67320: train loss 2.4130, val loss 2.3755\n",
      "step 67330: train loss 2.3830, val loss 2.3935\n",
      "step 67340: train loss 2.3836, val loss 2.3982\n",
      "step 67350: train loss 2.4588, val loss 2.3446\n",
      "step 67360: train loss 2.3812, val loss 2.3604\n",
      "step 67370: train loss 2.4279, val loss 2.3946\n",
      "step 67380: train loss 2.3421, val loss 2.3642\n",
      "step 67390: train loss 2.3605, val loss 2.3583\n",
      "step 67400: train loss 2.4059, val loss 2.3831\n",
      "step 67410: train loss 2.3315, val loss 2.3710\n",
      "step 67420: train loss 2.3917, val loss 2.3425\n",
      "step 67430: train loss 2.5190, val loss 2.3761\n",
      "step 67440: train loss 2.3749, val loss 2.3903\n",
      "step 67450: train loss 2.3565, val loss 2.3836\n",
      "step 67460: train loss 2.3769, val loss 2.3307\n",
      "step 67470: train loss 2.4177, val loss 2.3403\n",
      "step 67480: train loss 2.3568, val loss 2.4662\n",
      "step 67490: train loss 2.4017, val loss 2.3818\n",
      "step 67500: train loss 2.3755, val loss 2.3187\n",
      "step 67510: train loss 2.3435, val loss 2.4065\n",
      "step 67520: train loss 2.4192, val loss 2.4038\n",
      "step 67530: train loss 2.3950, val loss 2.4042\n",
      "step 67540: train loss 2.3229, val loss 2.2902\n",
      "step 67550: train loss 2.3623, val loss 2.4277\n",
      "step 67560: train loss 2.2952, val loss 2.3204\n",
      "step 67570: train loss 2.4231, val loss 2.3403\n",
      "step 67580: train loss 2.4216, val loss 2.3357\n",
      "step 67590: train loss 2.3966, val loss 2.4636\n",
      "step 67600: train loss 2.4227, val loss 2.3212\n",
      "step 67610: train loss 2.4318, val loss 2.4025\n",
      "step 67620: train loss 2.3948, val loss 2.3664\n",
      "step 67630: train loss 2.3576, val loss 2.3531\n",
      "step 67640: train loss 2.4173, val loss 2.3271\n",
      "step 67650: train loss 2.3674, val loss 2.2974\n",
      "step 67660: train loss 2.3083, val loss 2.3810\n",
      "step 67670: train loss 2.3384, val loss 2.3848\n",
      "step 67680: train loss 2.4693, val loss 2.3342\n",
      "step 67690: train loss 2.3715, val loss 2.3916\n",
      "step 67700: train loss 2.4073, val loss 2.4218\n",
      "step 67710: train loss 2.4093, val loss 2.3019\n",
      "step 67720: train loss 2.3854, val loss 2.3927\n",
      "step 67730: train loss 2.3782, val loss 2.3834\n",
      "step 67740: train loss 2.3463, val loss 2.3274\n",
      "step 67750: train loss 2.3581, val loss 2.3855\n",
      "step 67760: train loss 2.3057, val loss 2.3833\n",
      "step 67770: train loss 2.3843, val loss 2.4124\n",
      "step 67780: train loss 2.3608, val loss 2.3951\n",
      "step 67790: train loss 2.3301, val loss 2.3433\n",
      "step 67800: train loss 2.3436, val loss 2.3349\n",
      "step 67810: train loss 2.4049, val loss 2.4354\n",
      "step 67820: train loss 2.3051, val loss 2.3729\n",
      "step 67830: train loss 2.4240, val loss 2.3244\n",
      "step 67840: train loss 2.3747, val loss 2.4066\n",
      "step 67850: train loss 2.4216, val loss 2.4340\n",
      "step 67860: train loss 2.3419, val loss 2.4017\n",
      "step 67870: train loss 2.3735, val loss 2.4153\n",
      "step 67880: train loss 2.3531, val loss 2.3609\n",
      "step 67890: train loss 2.4594, val loss 2.3893\n",
      "step 67900: train loss 2.4855, val loss 2.3400\n",
      "step 67910: train loss 2.5323, val loss 2.3545\n",
      "step 67920: train loss 2.3764, val loss 2.3504\n",
      "step 67930: train loss 2.3686, val loss 2.3493\n",
      "step 67940: train loss 2.3597, val loss 2.4051\n",
      "step 67950: train loss 2.3118, val loss 2.4317\n",
      "step 67960: train loss 2.3550, val loss 2.2961\n",
      "step 67970: train loss 2.3906, val loss 2.3526\n",
      "step 67980: train loss 2.3360, val loss 2.3218\n",
      "step 67990: train loss 2.3354, val loss 2.3156\n",
      "step 68000: train loss 2.4027, val loss 2.2866\n",
      "step 68010: train loss 2.3688, val loss 2.3829\n",
      "step 68020: train loss 2.3777, val loss 2.3922\n",
      "step 68030: train loss 2.3713, val loss 2.2585\n",
      "step 68040: train loss 2.4145, val loss 2.2752\n",
      "step 68050: train loss 2.4548, val loss 2.3856\n",
      "step 68060: train loss 2.3605, val loss 2.3923\n",
      "step 68070: train loss 2.3469, val loss 2.3532\n",
      "step 68080: train loss 2.4142, val loss 2.3736\n",
      "step 68090: train loss 2.3623, val loss 2.3946\n",
      "step 68100: train loss 2.4000, val loss 2.2994\n",
      "step 68110: train loss 2.3262, val loss 2.3742\n",
      "step 68120: train loss 2.3057, val loss 2.3709\n",
      "step 68130: train loss 2.3847, val loss 2.2978\n",
      "step 68140: train loss 2.4476, val loss 2.4218\n",
      "step 68150: train loss 2.4207, val loss 2.3648\n",
      "step 68160: train loss 2.3515, val loss 2.4122\n",
      "step 68170: train loss 2.3374, val loss 2.4218\n",
      "step 68180: train loss 2.4328, val loss 2.2649\n",
      "step 68190: train loss 2.3504, val loss 2.3847\n",
      "step 68200: train loss 2.3399, val loss 2.3062\n",
      "step 68210: train loss 2.4642, val loss 2.3681\n",
      "step 68220: train loss 2.3240, val loss 2.3381\n",
      "step 68230: train loss 2.3632, val loss 2.3949\n",
      "step 68240: train loss 2.3473, val loss 2.4040\n",
      "step 68250: train loss 2.3940, val loss 2.4745\n",
      "step 68260: train loss 2.3406, val loss 2.3097\n",
      "step 68270: train loss 2.3513, val loss 2.3943\n",
      "step 68280: train loss 2.3301, val loss 2.3824\n",
      "step 68290: train loss 2.3252, val loss 2.3153\n",
      "step 68300: train loss 2.3608, val loss 2.3549\n",
      "step 68310: train loss 2.3512, val loss 2.3519\n",
      "step 68320: train loss 2.3746, val loss 2.3997\n",
      "step 68330: train loss 2.4317, val loss 2.4074\n",
      "step 68340: train loss 2.4335, val loss 2.3219\n",
      "step 68350: train loss 2.3918, val loss 2.3352\n",
      "step 68360: train loss 2.4236, val loss 2.3263\n",
      "step 68370: train loss 2.3553, val loss 2.3431\n",
      "step 68380: train loss 2.4254, val loss 2.3617\n",
      "step 68390: train loss 2.3104, val loss 2.3858\n",
      "step 68400: train loss 2.4663, val loss 2.3480\n",
      "step 68410: train loss 2.3918, val loss 2.3919\n",
      "step 68420: train loss 2.4113, val loss 2.3465\n",
      "step 68430: train loss 2.3202, val loss 2.3754\n",
      "step 68440: train loss 2.3639, val loss 2.3414\n",
      "step 68450: train loss 2.3760, val loss 2.3839\n",
      "step 68460: train loss 2.4049, val loss 2.3644\n",
      "step 68470: train loss 2.3627, val loss 2.4032\n",
      "step 68480: train loss 2.3884, val loss 2.2991\n",
      "step 68490: train loss 2.3660, val loss 2.3770\n",
      "step 68500: train loss 2.4354, val loss 2.3476\n",
      "step 68510: train loss 2.3714, val loss 2.3619\n",
      "step 68520: train loss 2.3159, val loss 2.3694\n",
      "step 68530: train loss 2.3663, val loss 2.3016\n",
      "step 68540: train loss 2.3915, val loss 2.4461\n",
      "step 68550: train loss 2.3412, val loss 2.3155\n",
      "step 68560: train loss 2.4361, val loss 2.3024\n",
      "step 68570: train loss 2.4554, val loss 2.4025\n",
      "step 68580: train loss 2.3616, val loss 2.3362\n",
      "step 68590: train loss 2.4071, val loss 2.3613\n",
      "step 68600: train loss 2.3798, val loss 2.3747\n",
      "step 68610: train loss 2.3774, val loss 2.4329\n",
      "step 68620: train loss 2.3948, val loss 2.4111\n",
      "step 68630: train loss 2.4176, val loss 2.3655\n",
      "step 68640: train loss 2.2964, val loss 2.3452\n",
      "step 68650: train loss 2.3185, val loss 2.3638\n",
      "step 68660: train loss 2.3984, val loss 2.4484\n",
      "step 68670: train loss 2.3692, val loss 2.3515\n",
      "step 68680: train loss 2.3488, val loss 2.3200\n",
      "step 68690: train loss 2.3923, val loss 2.3546\n",
      "step 68700: train loss 2.3730, val loss 2.3680\n",
      "step 68710: train loss 2.4292, val loss 2.3004\n",
      "step 68720: train loss 2.3451, val loss 2.4139\n",
      "step 68730: train loss 2.3774, val loss 2.3863\n",
      "step 68740: train loss 2.3082, val loss 2.3902\n",
      "step 68750: train loss 2.4120, val loss 2.4203\n",
      "step 68760: train loss 2.3631, val loss 2.3854\n",
      "step 68770: train loss 2.3309, val loss 2.3559\n",
      "step 68780: train loss 2.3720, val loss 2.3786\n",
      "step 68790: train loss 2.3443, val loss 2.4466\n",
      "step 68800: train loss 2.3669, val loss 2.3998\n",
      "step 68810: train loss 2.4040, val loss 2.3207\n",
      "step 68820: train loss 2.4215, val loss 2.3878\n",
      "step 68830: train loss 2.4146, val loss 2.4014\n",
      "step 68840: train loss 2.3434, val loss 2.4025\n",
      "step 68850: train loss 2.4411, val loss 2.3593\n",
      "step 68860: train loss 2.4176, val loss 2.4327\n",
      "step 68870: train loss 2.3792, val loss 2.4036\n",
      "step 68880: train loss 2.4088, val loss 2.3478\n",
      "step 68890: train loss 2.3749, val loss 2.3635\n",
      "step 68900: train loss 2.4373, val loss 2.3594\n",
      "step 68910: train loss 2.3336, val loss 2.3958\n",
      "step 68920: train loss 2.3768, val loss 2.3070\n",
      "step 68930: train loss 2.3276, val loss 2.3577\n",
      "step 68940: train loss 2.3549, val loss 2.3241\n",
      "step 68950: train loss 2.3501, val loss 2.4358\n",
      "step 68960: train loss 2.4099, val loss 2.3400\n",
      "step 68970: train loss 2.3613, val loss 2.3395\n",
      "step 68980: train loss 2.4067, val loss 2.4145\n",
      "step 68990: train loss 2.4072, val loss 2.4041\n",
      "step 69000: train loss 2.3424, val loss 2.3899\n",
      "step 69010: train loss 2.4115, val loss 2.4154\n",
      "step 69020: train loss 2.2859, val loss 2.3574\n",
      "step 69030: train loss 2.3631, val loss 2.3647\n",
      "step 69040: train loss 2.3859, val loss 2.4712\n",
      "step 69050: train loss 2.3236, val loss 2.3391\n",
      "step 69060: train loss 2.3928, val loss 2.3529\n",
      "step 69070: train loss 2.3673, val loss 2.3072\n",
      "step 69080: train loss 2.3674, val loss 2.4597\n",
      "step 69090: train loss 2.3760, val loss 2.3372\n",
      "step 69100: train loss 2.3535, val loss 2.3796\n",
      "step 69110: train loss 2.4507, val loss 2.4909\n",
      "step 69120: train loss 2.4478, val loss 2.3654\n",
      "step 69130: train loss 2.3498, val loss 2.3810\n",
      "step 69140: train loss 2.3385, val loss 2.4077\n",
      "step 69150: train loss 2.3450, val loss 2.3233\n",
      "step 69160: train loss 2.4404, val loss 2.4039\n",
      "step 69170: train loss 2.3435, val loss 2.3687\n",
      "step 69180: train loss 2.4191, val loss 2.3929\n",
      "step 69190: train loss 2.3371, val loss 2.3003\n",
      "step 69200: train loss 2.3337, val loss 2.4063\n",
      "step 69210: train loss 2.4331, val loss 2.4609\n",
      "step 69220: train loss 2.3424, val loss 2.4020\n",
      "step 69230: train loss 2.4005, val loss 2.4218\n",
      "step 69240: train loss 2.4404, val loss 2.3825\n",
      "step 69250: train loss 2.3055, val loss 2.3614\n",
      "step 69260: train loss 2.3745, val loss 2.2839\n",
      "step 69270: train loss 2.3710, val loss 2.3806\n",
      "step 69280: train loss 2.3418, val loss 2.4720\n",
      "step 69290: train loss 2.3934, val loss 2.3938\n",
      "step 69300: train loss 2.4454, val loss 2.2905\n",
      "step 69310: train loss 2.3756, val loss 2.5036\n",
      "step 69320: train loss 2.3355, val loss 2.3695\n",
      "step 69330: train loss 2.2924, val loss 2.3818\n",
      "step 69340: train loss 2.4184, val loss 2.4540\n",
      "step 69350: train loss 2.4359, val loss 2.3279\n",
      "step 69360: train loss 2.3288, val loss 2.3130\n",
      "step 69370: train loss 2.3633, val loss 2.3882\n",
      "step 69380: train loss 2.3518, val loss 2.2944\n",
      "step 69390: train loss 2.3134, val loss 2.3158\n",
      "step 69400: train loss 2.3287, val loss 2.3710\n",
      "step 69410: train loss 2.4030, val loss 2.3295\n",
      "step 69420: train loss 2.3814, val loss 2.3594\n",
      "step 69430: train loss 2.3604, val loss 2.3690\n",
      "step 69440: train loss 2.3174, val loss 2.3085\n",
      "step 69450: train loss 2.4887, val loss 2.4148\n",
      "step 69460: train loss 2.3839, val loss 2.3830\n",
      "step 69470: train loss 2.3740, val loss 2.4626\n",
      "step 69480: train loss 2.3494, val loss 2.4225\n",
      "step 69490: train loss 2.4023, val loss 2.2839\n",
      "step 69500: train loss 2.4471, val loss 2.3409\n",
      "step 69510: train loss 2.4186, val loss 2.3422\n",
      "step 69520: train loss 2.3822, val loss 2.4429\n",
      "step 69530: train loss 2.4041, val loss 2.3519\n",
      "step 69540: train loss 2.4059, val loss 2.3504\n",
      "step 69550: train loss 2.3837, val loss 2.4001\n",
      "step 69560: train loss 2.3893, val loss 2.3472\n",
      "step 69570: train loss 2.4001, val loss 2.3184\n",
      "step 69580: train loss 2.3257, val loss 2.3396\n",
      "step 69590: train loss 2.3870, val loss 2.3046\n",
      "step 69600: train loss 2.3383, val loss 2.3503\n",
      "step 69610: train loss 2.3825, val loss 2.3344\n",
      "step 69620: train loss 2.4047, val loss 2.3958\n",
      "step 69630: train loss 2.4189, val loss 2.4251\n",
      "step 69640: train loss 2.4389, val loss 2.4020\n",
      "step 69650: train loss 2.4163, val loss 2.4176\n",
      "step 69660: train loss 2.4103, val loss 2.4405\n",
      "step 69670: train loss 2.3702, val loss 2.3063\n",
      "step 69680: train loss 2.3798, val loss 2.4487\n",
      "step 69690: train loss 2.3850, val loss 2.4020\n",
      "step 69700: train loss 2.4193, val loss 2.3829\n",
      "step 69710: train loss 2.3830, val loss 2.3603\n",
      "step 69720: train loss 2.3828, val loss 2.3822\n",
      "step 69730: train loss 2.4304, val loss 2.3307\n",
      "step 69740: train loss 2.2965, val loss 2.4038\n",
      "step 69750: train loss 2.4880, val loss 2.3589\n",
      "step 69760: train loss 2.4005, val loss 2.3222\n",
      "step 69770: train loss 2.4074, val loss 2.4631\n",
      "step 69780: train loss 2.3948, val loss 2.4011\n",
      "step 69790: train loss 2.3377, val loss 2.4065\n",
      "step 69800: train loss 2.4060, val loss 2.3856\n",
      "step 69810: train loss 2.3490, val loss 2.2863\n",
      "step 69820: train loss 2.3610, val loss 2.3316\n",
      "step 69830: train loss 2.4023, val loss 2.4352\n",
      "step 69840: train loss 2.3877, val loss 2.3633\n",
      "step 69850: train loss 2.4735, val loss 2.3898\n",
      "step 69860: train loss 2.3339, val loss 2.2797\n",
      "step 69870: train loss 2.3765, val loss 2.2858\n",
      "step 69880: train loss 2.4175, val loss 2.3490\n",
      "step 69890: train loss 2.3252, val loss 2.3940\n",
      "step 69900: train loss 2.3686, val loss 2.3726\n",
      "step 69910: train loss 2.4121, val loss 2.3622\n",
      "step 69920: train loss 2.3786, val loss 2.3322\n",
      "step 69930: train loss 2.4633, val loss 2.4056\n",
      "step 69940: train loss 2.4018, val loss 2.3593\n",
      "step 69950: train loss 2.4092, val loss 2.4190\n",
      "step 69960: train loss 2.4138, val loss 2.2695\n",
      "step 69970: train loss 2.3250, val loss 2.4282\n",
      "step 69980: train loss 2.4227, val loss 2.2781\n",
      "step 69990: train loss 2.3671, val loss 2.4287\n",
      "step 70000: train loss 2.3850, val loss 2.3573\n",
      "step 70010: train loss 2.4685, val loss 2.4585\n",
      "step 70020: train loss 2.3623, val loss 2.4119\n",
      "step 70030: train loss 2.3795, val loss 2.3526\n",
      "step 70040: train loss 2.3523, val loss 2.4143\n",
      "step 70050: train loss 2.3192, val loss 2.3038\n",
      "step 70060: train loss 2.3679, val loss 2.3755\n",
      "step 70070: train loss 2.4133, val loss 2.3454\n",
      "step 70080: train loss 2.3929, val loss 2.3810\n",
      "step 70090: train loss 2.3046, val loss 2.3623\n",
      "step 70100: train loss 2.4120, val loss 2.2714\n",
      "step 70110: train loss 2.3548, val loss 2.3428\n",
      "step 70120: train loss 2.3729, val loss 2.4149\n",
      "step 70130: train loss 2.3802, val loss 2.3458\n",
      "step 70140: train loss 2.4936, val loss 2.3627\n",
      "step 70150: train loss 2.4082, val loss 2.3957\n",
      "step 70160: train loss 2.3625, val loss 2.3917\n",
      "step 70170: train loss 2.4309, val loss 2.4095\n",
      "step 70180: train loss 2.3341, val loss 2.4507\n",
      "step 70190: train loss 2.4645, val loss 2.3327\n",
      "step 70200: train loss 2.3980, val loss 2.3425\n",
      "step 70210: train loss 2.4150, val loss 2.2960\n",
      "step 70220: train loss 2.3790, val loss 2.3163\n",
      "step 70230: train loss 2.3556, val loss 2.2346\n",
      "step 70240: train loss 2.3625, val loss 2.4149\n",
      "step 70250: train loss 2.3885, val loss 2.4373\n",
      "step 70260: train loss 2.4115, val loss 2.3479\n",
      "step 70270: train loss 2.3931, val loss 2.3524\n",
      "step 70280: train loss 2.4107, val loss 2.4089\n",
      "step 70290: train loss 2.4113, val loss 2.3603\n",
      "step 70300: train loss 2.3311, val loss 2.3611\n",
      "step 70310: train loss 2.4151, val loss 2.3991\n",
      "step 70320: train loss 2.4044, val loss 2.3864\n",
      "step 70330: train loss 2.3782, val loss 2.3988\n",
      "step 70340: train loss 2.4018, val loss 2.4441\n",
      "step 70350: train loss 2.4406, val loss 2.4289\n",
      "step 70360: train loss 2.3913, val loss 2.3206\n",
      "step 70370: train loss 2.3569, val loss 2.3089\n",
      "step 70380: train loss 2.4005, val loss 2.3250\n",
      "step 70390: train loss 2.3757, val loss 2.3449\n",
      "step 70400: train loss 2.3926, val loss 2.2819\n",
      "step 70410: train loss 2.3861, val loss 2.3413\n",
      "step 70420: train loss 2.3789, val loss 2.3084\n",
      "step 70430: train loss 2.3321, val loss 2.3448\n",
      "step 70440: train loss 2.4241, val loss 2.3751\n",
      "step 70450: train loss 2.4792, val loss 2.4235\n",
      "step 70460: train loss 2.3863, val loss 2.4127\n",
      "step 70470: train loss 2.4767, val loss 2.3547\n",
      "step 70480: train loss 2.3809, val loss 2.3868\n",
      "step 70490: train loss 2.3959, val loss 2.3331\n",
      "step 70500: train loss 2.3014, val loss 2.3017\n",
      "step 70510: train loss 2.4229, val loss 2.3192\n",
      "step 70520: train loss 2.4060, val loss 2.4636\n",
      "step 70530: train loss 2.4512, val loss 2.4346\n",
      "step 70540: train loss 2.3720, val loss 2.3887\n",
      "step 70550: train loss 2.4351, val loss 2.4007\n",
      "step 70560: train loss 2.3721, val loss 2.2924\n",
      "step 70570: train loss 2.4230, val loss 2.3696\n",
      "step 70580: train loss 2.3776, val loss 2.4234\n",
      "step 70590: train loss 2.3772, val loss 2.4114\n",
      "step 70600: train loss 2.3966, val loss 2.4442\n",
      "step 70610: train loss 2.3971, val loss 2.3407\n",
      "step 70620: train loss 2.4039, val loss 2.3748\n",
      "step 70630: train loss 2.3990, val loss 2.3980\n",
      "step 70640: train loss 2.3216, val loss 2.4680\n",
      "step 70650: train loss 2.3323, val loss 2.3516\n",
      "step 70660: train loss 2.2939, val loss 2.3378\n",
      "step 70670: train loss 2.3992, val loss 2.3809\n",
      "step 70680: train loss 2.2819, val loss 2.3806\n",
      "step 70690: train loss 2.3043, val loss 2.2775\n",
      "step 70700: train loss 2.4501, val loss 2.4093\n",
      "step 70710: train loss 2.4278, val loss 2.3538\n",
      "step 70720: train loss 2.3110, val loss 2.4163\n",
      "step 70730: train loss 2.4539, val loss 2.3775\n",
      "step 70740: train loss 2.4633, val loss 2.4383\n",
      "step 70750: train loss 2.4568, val loss 2.3555\n",
      "step 70760: train loss 2.3281, val loss 2.3965\n",
      "step 70770: train loss 2.3556, val loss 2.4280\n",
      "step 70780: train loss 2.4162, val loss 2.3723\n",
      "step 70790: train loss 2.2947, val loss 2.3728\n",
      "step 70800: train loss 2.3235, val loss 2.3226\n",
      "step 70810: train loss 2.3283, val loss 2.3762\n",
      "step 70820: train loss 2.3753, val loss 2.3088\n",
      "step 70830: train loss 2.4105, val loss 2.3105\n",
      "step 70840: train loss 2.3674, val loss 2.3561\n",
      "step 70850: train loss 2.4219, val loss 2.4327\n",
      "step 70860: train loss 2.4226, val loss 2.3724\n",
      "step 70870: train loss 2.3605, val loss 2.3470\n",
      "step 70880: train loss 2.3184, val loss 2.3938\n",
      "step 70890: train loss 2.4290, val loss 2.3076\n",
      "step 70900: train loss 2.3351, val loss 2.3890\n",
      "step 70910: train loss 2.4619, val loss 2.3882\n",
      "step 70920: train loss 2.3142, val loss 2.3281\n",
      "step 70930: train loss 2.2932, val loss 2.3601\n",
      "step 70940: train loss 2.3504, val loss 2.3313\n",
      "step 70950: train loss 2.3038, val loss 2.3419\n",
      "step 70960: train loss 2.3238, val loss 2.3810\n",
      "step 70970: train loss 2.3916, val loss 2.4072\n",
      "step 70980: train loss 2.4165, val loss 2.4050\n",
      "step 70990: train loss 2.4243, val loss 2.4319\n",
      "step 71000: train loss 2.3882, val loss 2.4041\n",
      "step 71010: train loss 2.3644, val loss 2.3690\n",
      "step 71020: train loss 2.3591, val loss 2.3598\n",
      "step 71030: train loss 2.3999, val loss 2.2785\n",
      "step 71040: train loss 2.3292, val loss 2.3257\n",
      "step 71050: train loss 2.3446, val loss 2.4340\n",
      "step 71060: train loss 2.3727, val loss 2.4346\n",
      "step 71070: train loss 2.2909, val loss 2.3879\n",
      "step 71080: train loss 2.4598, val loss 2.3791\n",
      "step 71090: train loss 2.4111, val loss 2.3239\n",
      "step 71100: train loss 2.3264, val loss 2.4623\n",
      "step 71110: train loss 2.3328, val loss 2.3554\n",
      "step 71120: train loss 2.3965, val loss 2.4236\n",
      "step 71130: train loss 2.4136, val loss 2.3418\n",
      "step 71140: train loss 2.3744, val loss 2.3454\n",
      "step 71150: train loss 2.3781, val loss 2.3362\n",
      "step 71160: train loss 2.3457, val loss 2.3687\n",
      "step 71170: train loss 2.3921, val loss 2.4752\n",
      "step 71180: train loss 2.3868, val loss 2.3517\n",
      "step 71190: train loss 2.3213, val loss 2.3537\n",
      "step 71200: train loss 2.3942, val loss 2.3604\n",
      "step 71210: train loss 2.3898, val loss 2.3733\n",
      "step 71220: train loss 2.3285, val loss 2.3829\n",
      "step 71230: train loss 2.3490, val loss 2.3654\n",
      "step 71240: train loss 2.3441, val loss 2.5046\n",
      "step 71250: train loss 2.3787, val loss 2.3753\n",
      "step 71260: train loss 2.3209, val loss 2.3957\n",
      "step 71270: train loss 2.3683, val loss 2.4252\n",
      "step 71280: train loss 2.4541, val loss 2.3924\n",
      "step 71290: train loss 2.3643, val loss 2.3596\n",
      "step 71300: train loss 2.4166, val loss 2.4328\n",
      "step 71310: train loss 2.4715, val loss 2.3290\n",
      "step 71320: train loss 2.4163, val loss 2.3983\n",
      "step 71330: train loss 2.3766, val loss 2.3421\n",
      "step 71340: train loss 2.4414, val loss 2.3711\n",
      "step 71350: train loss 2.3525, val loss 2.4341\n",
      "step 71360: train loss 2.4216, val loss 2.3191\n",
      "step 71370: train loss 2.3748, val loss 2.3806\n",
      "step 71380: train loss 2.4108, val loss 2.3498\n",
      "step 71390: train loss 2.3782, val loss 2.3725\n",
      "step 71400: train loss 2.3450, val loss 2.2904\n",
      "step 71410: train loss 2.3325, val loss 2.3533\n",
      "step 71420: train loss 2.3734, val loss 2.3117\n",
      "step 71430: train loss 2.4005, val loss 2.3795\n",
      "step 71440: train loss 2.4032, val loss 2.4148\n",
      "step 71450: train loss 2.4377, val loss 2.4508\n",
      "step 71460: train loss 2.3767, val loss 2.3291\n",
      "step 71470: train loss 2.4211, val loss 2.3741\n",
      "step 71480: train loss 2.3580, val loss 2.3537\n",
      "step 71490: train loss 2.3797, val loss 2.3503\n",
      "step 71500: train loss 2.4344, val loss 2.3903\n",
      "step 71510: train loss 2.3977, val loss 2.3528\n",
      "step 71520: train loss 2.4538, val loss 2.4302\n",
      "step 71530: train loss 2.4010, val loss 2.3600\n",
      "step 71540: train loss 2.3589, val loss 2.3847\n",
      "step 71550: train loss 2.4370, val loss 2.3532\n",
      "step 71560: train loss 2.3571, val loss 2.3544\n",
      "step 71570: train loss 2.3424, val loss 2.3576\n",
      "step 71580: train loss 2.3270, val loss 2.4212\n",
      "step 71590: train loss 2.3562, val loss 2.3560\n",
      "step 71600: train loss 2.3173, val loss 2.3681\n",
      "step 71610: train loss 2.3333, val loss 2.3876\n",
      "step 71620: train loss 2.3638, val loss 2.4026\n",
      "step 71630: train loss 2.3185, val loss 2.3596\n",
      "step 71640: train loss 2.4561, val loss 2.4082\n",
      "step 71650: train loss 2.3588, val loss 2.4377\n",
      "step 71660: train loss 2.3229, val loss 2.3027\n",
      "step 71670: train loss 2.3137, val loss 2.3075\n",
      "step 71680: train loss 2.3246, val loss 2.4522\n",
      "step 71690: train loss 2.3674, val loss 2.3407\n",
      "step 71700: train loss 2.4968, val loss 2.3779\n",
      "step 71710: train loss 2.3933, val loss 2.4162\n",
      "step 71720: train loss 2.3529, val loss 2.3332\n",
      "step 71730: train loss 2.3615, val loss 2.4110\n",
      "step 71740: train loss 2.3559, val loss 2.3745\n",
      "step 71750: train loss 2.3900, val loss 2.3934\n",
      "step 71760: train loss 2.4027, val loss 2.2786\n",
      "step 71770: train loss 2.3874, val loss 2.3606\n",
      "step 71780: train loss 2.3192, val loss 2.4127\n",
      "step 71790: train loss 2.3711, val loss 2.2708\n",
      "step 71800: train loss 2.3512, val loss 2.3682\n",
      "step 71810: train loss 2.4241, val loss 2.3631\n",
      "step 71820: train loss 2.3571, val loss 2.3737\n",
      "step 71830: train loss 2.3086, val loss 2.3525\n",
      "step 71840: train loss 2.3069, val loss 2.3609\n",
      "step 71850: train loss 2.3562, val loss 2.4083\n",
      "step 71860: train loss 2.3400, val loss 2.3883\n",
      "step 71870: train loss 2.3894, val loss 2.4168\n",
      "step 71880: train loss 2.3538, val loss 2.3982\n",
      "step 71890: train loss 2.2994, val loss 2.3738\n",
      "step 71900: train loss 2.4005, val loss 2.3946\n",
      "step 71910: train loss 2.3724, val loss 2.4261\n",
      "step 71920: train loss 2.3054, val loss 2.3279\n",
      "step 71930: train loss 2.3710, val loss 2.3832\n",
      "step 71940: train loss 2.4077, val loss 2.3526\n",
      "step 71950: train loss 2.3912, val loss 2.4060\n",
      "step 71960: train loss 2.3836, val loss 2.4553\n",
      "step 71970: train loss 2.3689, val loss 2.4018\n",
      "step 71980: train loss 2.4518, val loss 2.3876\n",
      "step 71990: train loss 2.4459, val loss 2.3891\n",
      "step 72000: train loss 2.3634, val loss 2.3067\n",
      "step 72010: train loss 2.3290, val loss 2.3646\n",
      "step 72020: train loss 2.3400, val loss 2.3819\n",
      "step 72030: train loss 2.3112, val loss 2.3855\n",
      "step 72040: train loss 2.3336, val loss 2.3802\n",
      "step 72050: train loss 2.3799, val loss 2.3587\n",
      "step 72060: train loss 2.3434, val loss 2.3665\n",
      "step 72070: train loss 2.3332, val loss 2.3426\n",
      "step 72080: train loss 2.4552, val loss 2.4091\n",
      "step 72090: train loss 2.4219, val loss 2.2815\n",
      "step 72100: train loss 2.3402, val loss 2.3400\n",
      "step 72110: train loss 2.3810, val loss 2.3407\n",
      "step 72120: train loss 2.3498, val loss 2.3428\n",
      "step 72130: train loss 2.4145, val loss 2.3195\n",
      "step 72140: train loss 2.4150, val loss 2.3748\n",
      "step 72150: train loss 2.4367, val loss 2.5139\n",
      "step 72160: train loss 2.3613, val loss 2.2780\n",
      "step 72170: train loss 2.3707, val loss 2.2982\n",
      "step 72180: train loss 2.4058, val loss 2.4814\n",
      "step 72190: train loss 2.4808, val loss 2.4155\n",
      "step 72200: train loss 2.3505, val loss 2.3418\n",
      "step 72210: train loss 2.3264, val loss 2.3153\n",
      "step 72220: train loss 2.4112, val loss 2.2561\n",
      "step 72230: train loss 2.3378, val loss 2.4473\n",
      "step 72240: train loss 2.4074, val loss 2.3528\n",
      "step 72250: train loss 2.4339, val loss 2.3142\n",
      "step 72260: train loss 2.3578, val loss 2.3819\n",
      "step 72270: train loss 2.3588, val loss 2.4271\n",
      "step 72280: train loss 2.3673, val loss 2.3689\n",
      "step 72290: train loss 2.3051, val loss 2.3134\n",
      "step 72300: train loss 2.4256, val loss 2.3125\n",
      "step 72310: train loss 2.3969, val loss 2.5045\n",
      "step 72320: train loss 2.3881, val loss 2.3610\n",
      "step 72330: train loss 2.3553, val loss 2.4987\n",
      "step 72340: train loss 2.3431, val loss 2.4108\n",
      "step 72350: train loss 2.3783, val loss 2.3875\n",
      "step 72360: train loss 2.4090, val loss 2.3658\n",
      "step 72370: train loss 2.3657, val loss 2.3753\n",
      "step 72380: train loss 2.4439, val loss 2.3076\n",
      "step 72390: train loss 2.4556, val loss 2.4383\n",
      "step 72400: train loss 2.4574, val loss 2.3971\n",
      "step 72410: train loss 2.4427, val loss 2.4000\n",
      "step 72420: train loss 2.4268, val loss 2.2818\n",
      "step 72430: train loss 2.3518, val loss 2.3932\n",
      "step 72440: train loss 2.3579, val loss 2.3768\n",
      "step 72450: train loss 2.3782, val loss 2.3731\n",
      "step 72460: train loss 2.3548, val loss 2.3726\n",
      "step 72470: train loss 2.3374, val loss 2.4752\n",
      "step 72480: train loss 2.3890, val loss 2.4336\n",
      "step 72490: train loss 2.4148, val loss 2.4385\n",
      "step 72500: train loss 2.4381, val loss 2.4316\n",
      "step 72510: train loss 2.4175, val loss 2.4234\n",
      "step 72520: train loss 2.4052, val loss 2.3262\n",
      "step 72530: train loss 2.3675, val loss 2.3292\n",
      "step 72540: train loss 2.2666, val loss 2.3960\n",
      "step 72550: train loss 2.4192, val loss 2.3427\n",
      "step 72560: train loss 2.3613, val loss 2.3780\n",
      "step 72570: train loss 2.3991, val loss 2.4127\n",
      "step 72580: train loss 2.4426, val loss 2.3442\n",
      "step 72590: train loss 2.3845, val loss 2.3532\n",
      "step 72600: train loss 2.3308, val loss 2.3116\n",
      "step 72610: train loss 2.3466, val loss 2.3242\n",
      "step 72620: train loss 2.3751, val loss 2.3516\n",
      "step 72630: train loss 2.2853, val loss 2.3265\n",
      "step 72640: train loss 2.4466, val loss 2.4019\n",
      "step 72650: train loss 2.4253, val loss 2.3142\n",
      "step 72660: train loss 2.4051, val loss 2.3584\n",
      "step 72670: train loss 2.4118, val loss 2.3830\n",
      "step 72680: train loss 2.3637, val loss 2.3435\n",
      "step 72690: train loss 2.3458, val loss 2.2990\n",
      "step 72700: train loss 2.4232, val loss 2.3733\n",
      "step 72710: train loss 2.4220, val loss 2.4180\n",
      "step 72720: train loss 2.3750, val loss 2.3044\n",
      "step 72730: train loss 2.3070, val loss 2.3770\n",
      "step 72740: train loss 2.3841, val loss 2.3055\n",
      "step 72750: train loss 2.4523, val loss 2.2928\n",
      "step 72760: train loss 2.3290, val loss 2.3648\n",
      "step 72770: train loss 2.3142, val loss 2.4270\n",
      "step 72780: train loss 2.4194, val loss 2.3884\n",
      "step 72790: train loss 2.3850, val loss 2.3137\n",
      "step 72800: train loss 2.3932, val loss 2.4313\n",
      "step 72810: train loss 2.2886, val loss 2.3493\n",
      "step 72820: train loss 2.3357, val loss 2.4361\n",
      "step 72830: train loss 2.3452, val loss 2.4050\n",
      "step 72840: train loss 2.3777, val loss 2.4076\n",
      "step 72850: train loss 2.3902, val loss 2.3743\n",
      "step 72860: train loss 2.4000, val loss 2.4359\n",
      "step 72870: train loss 2.4282, val loss 2.3632\n",
      "step 72880: train loss 2.4029, val loss 2.3762\n",
      "step 72890: train loss 2.4368, val loss 2.3996\n",
      "step 72900: train loss 2.3416, val loss 2.4391\n",
      "step 72910: train loss 2.3795, val loss 2.3677\n",
      "step 72920: train loss 2.2977, val loss 2.3328\n",
      "step 72930: train loss 2.4654, val loss 2.3109\n",
      "step 72940: train loss 2.3407, val loss 2.3453\n",
      "step 72950: train loss 2.4251, val loss 2.4365\n",
      "step 72960: train loss 2.4277, val loss 2.4302\n",
      "step 72970: train loss 2.3940, val loss 2.4331\n",
      "step 72980: train loss 2.3947, val loss 2.3452\n",
      "step 72990: train loss 2.4155, val loss 2.3977\n",
      "step 73000: train loss 2.3147, val loss 2.3773\n",
      "step 73010: train loss 2.3894, val loss 2.5246\n",
      "step 73020: train loss 2.3811, val loss 2.3386\n",
      "step 73030: train loss 2.2995, val loss 2.2834\n",
      "step 73040: train loss 2.3721, val loss 2.3345\n",
      "step 73050: train loss 2.4136, val loss 2.4760\n",
      "step 73060: train loss 2.5165, val loss 2.3774\n",
      "step 73070: train loss 2.3896, val loss 2.3379\n",
      "step 73080: train loss 2.3733, val loss 2.4131\n",
      "step 73090: train loss 2.4845, val loss 2.3235\n",
      "step 73100: train loss 2.3440, val loss 2.3012\n",
      "step 73110: train loss 2.3192, val loss 2.3666\n",
      "step 73120: train loss 2.4608, val loss 2.4010\n",
      "step 73130: train loss 2.4548, val loss 2.3179\n",
      "step 73140: train loss 2.3515, val loss 2.3541\n",
      "step 73150: train loss 2.3352, val loss 2.3745\n",
      "step 73160: train loss 2.2917, val loss 2.3673\n",
      "step 73170: train loss 2.3392, val loss 2.3735\n",
      "step 73180: train loss 2.3635, val loss 2.3813\n",
      "step 73190: train loss 2.4059, val loss 2.4097\n",
      "step 73200: train loss 2.3556, val loss 2.3677\n",
      "step 73210: train loss 2.3773, val loss 2.3457\n",
      "step 73220: train loss 2.4047, val loss 2.3884\n",
      "step 73230: train loss 2.3150, val loss 2.3322\n",
      "step 73240: train loss 2.3790, val loss 2.3286\n",
      "step 73250: train loss 2.3542, val loss 2.3463\n",
      "step 73260: train loss 2.3618, val loss 2.3629\n",
      "step 73270: train loss 2.3279, val loss 2.3163\n",
      "step 73280: train loss 2.3556, val loss 2.3730\n",
      "step 73290: train loss 2.4207, val loss 2.4563\n",
      "step 73300: train loss 2.3568, val loss 2.4672\n",
      "step 73310: train loss 2.4245, val loss 2.4031\n",
      "step 73320: train loss 2.3813, val loss 2.3587\n",
      "step 73330: train loss 2.4118, val loss 2.3907\n",
      "step 73340: train loss 2.3559, val loss 2.3706\n",
      "step 73350: train loss 2.4152, val loss 2.3346\n",
      "step 73360: train loss 2.4250, val loss 2.3508\n",
      "step 73370: train loss 2.3544, val loss 2.3207\n",
      "step 73380: train loss 2.3078, val loss 2.3518\n",
      "step 73390: train loss 2.3330, val loss 2.2825\n",
      "step 73400: train loss 2.3921, val loss 2.2966\n",
      "step 73410: train loss 2.3091, val loss 2.4078\n",
      "step 73420: train loss 2.4425, val loss 2.3171\n",
      "step 73430: train loss 2.3650, val loss 2.4556\n",
      "step 73440: train loss 2.4197, val loss 2.4164\n",
      "step 73450: train loss 2.4624, val loss 2.3965\n",
      "step 73460: train loss 2.3931, val loss 2.3961\n",
      "step 73470: train loss 2.3354, val loss 2.4327\n",
      "step 73480: train loss 2.3939, val loss 2.3482\n",
      "step 73490: train loss 2.3438, val loss 2.3813\n",
      "step 73500: train loss 2.3815, val loss 2.3712\n",
      "step 73510: train loss 2.4448, val loss 2.3486\n",
      "step 73520: train loss 2.3372, val loss 2.3977\n",
      "step 73530: train loss 2.3762, val loss 2.3361\n",
      "step 73540: train loss 2.4105, val loss 2.3147\n",
      "step 73550: train loss 2.3819, val loss 2.3011\n",
      "step 73560: train loss 2.3878, val loss 2.3472\n",
      "step 73570: train loss 2.3697, val loss 2.3752\n",
      "step 73580: train loss 2.4258, val loss 2.3436\n",
      "step 73590: train loss 2.3208, val loss 2.4042\n",
      "step 73600: train loss 2.4315, val loss 2.4063\n",
      "step 73610: train loss 2.3863, val loss 2.3395\n",
      "step 73620: train loss 2.3404, val loss 2.3608\n",
      "step 73630: train loss 2.3971, val loss 2.3399\n",
      "step 73640: train loss 2.3811, val loss 2.3919\n",
      "step 73650: train loss 2.4072, val loss 2.2889\n",
      "step 73660: train loss 2.4055, val loss 2.2828\n",
      "step 73670: train loss 2.3913, val loss 2.4038\n",
      "step 73680: train loss 2.4013, val loss 2.3565\n",
      "step 73690: train loss 2.4276, val loss 2.3549\n",
      "step 73700: train loss 2.3340, val loss 2.4317\n",
      "step 73710: train loss 2.3685, val loss 2.3135\n",
      "step 73720: train loss 2.3418, val loss 2.2798\n",
      "step 73730: train loss 2.3869, val loss 2.3086\n",
      "step 73740: train loss 2.3984, val loss 2.4404\n",
      "step 73750: train loss 2.3604, val loss 2.3605\n",
      "step 73760: train loss 2.4072, val loss 2.3934\n",
      "step 73770: train loss 2.4610, val loss 2.3852\n",
      "step 73780: train loss 2.3873, val loss 2.3263\n",
      "step 73790: train loss 2.3454, val loss 2.3979\n",
      "step 73800: train loss 2.4240, val loss 2.4103\n",
      "step 73810: train loss 2.3473, val loss 2.2956\n",
      "step 73820: train loss 2.3925, val loss 2.3859\n",
      "step 73830: train loss 2.4097, val loss 2.3514\n",
      "step 73840: train loss 2.4124, val loss 2.3710\n",
      "step 73850: train loss 2.3686, val loss 2.3728\n",
      "step 73860: train loss 2.4011, val loss 2.3575\n",
      "step 73870: train loss 2.4179, val loss 2.3586\n",
      "step 73880: train loss 2.4404, val loss 2.3961\n",
      "step 73890: train loss 2.4086, val loss 2.3698\n",
      "step 73900: train loss 2.3243, val loss 2.3918\n",
      "step 73910: train loss 2.3557, val loss 2.3750\n",
      "step 73920: train loss 2.4047, val loss 2.3760\n",
      "step 73930: train loss 2.3787, val loss 2.4222\n",
      "step 73940: train loss 2.4403, val loss 2.3526\n",
      "step 73950: train loss 2.3381, val loss 2.2547\n",
      "step 73960: train loss 2.4165, val loss 2.4164\n",
      "step 73970: train loss 2.3963, val loss 2.4091\n",
      "step 73980: train loss 2.4370, val loss 2.3040\n",
      "step 73990: train loss 2.4153, val loss 2.3649\n",
      "step 74000: train loss 2.3466, val loss 2.3244\n",
      "step 74010: train loss 2.4143, val loss 2.3314\n",
      "step 74020: train loss 2.3583, val loss 2.4718\n",
      "step 74030: train loss 2.3895, val loss 2.3603\n",
      "step 74040: train loss 2.4193, val loss 2.4484\n",
      "step 74050: train loss 2.4337, val loss 2.4001\n",
      "step 74060: train loss 2.4168, val loss 2.3736\n",
      "step 74070: train loss 2.3456, val loss 2.3505\n",
      "step 74080: train loss 2.3372, val loss 2.3650\n",
      "step 74090: train loss 2.3327, val loss 2.3916\n",
      "step 74100: train loss 2.3935, val loss 2.4188\n",
      "step 74110: train loss 2.3524, val loss 2.3633\n",
      "step 74120: train loss 2.3659, val loss 2.3806\n",
      "step 74130: train loss 2.4034, val loss 2.4251\n",
      "step 74140: train loss 2.4507, val loss 2.3702\n",
      "step 74150: train loss 2.3788, val loss 2.3242\n",
      "step 74160: train loss 2.3824, val loss 2.3078\n",
      "step 74170: train loss 2.3593, val loss 2.3910\n",
      "step 74180: train loss 2.3780, val loss 2.3229\n",
      "step 74190: train loss 2.3875, val loss 2.3677\n",
      "step 74200: train loss 2.3464, val loss 2.3723\n",
      "step 74210: train loss 2.3397, val loss 2.3469\n",
      "step 74220: train loss 2.3830, val loss 2.3340\n",
      "step 74230: train loss 2.3888, val loss 2.3918\n",
      "step 74240: train loss 2.3715, val loss 2.4364\n",
      "step 74250: train loss 2.3981, val loss 2.3670\n",
      "step 74260: train loss 2.4648, val loss 2.2922\n",
      "step 74270: train loss 2.3688, val loss 2.4562\n",
      "step 74280: train loss 2.3676, val loss 2.3857\n",
      "step 74290: train loss 2.4214, val loss 2.2935\n",
      "step 74300: train loss 2.3532, val loss 2.3774\n",
      "step 74310: train loss 2.4325, val loss 2.3199\n",
      "step 74320: train loss 2.3082, val loss 2.3600\n",
      "step 74330: train loss 2.3459, val loss 2.2878\n",
      "step 74340: train loss 2.3445, val loss 2.4043\n",
      "step 74350: train loss 2.3076, val loss 2.3091\n",
      "step 74360: train loss 2.3236, val loss 2.3356\n",
      "step 74370: train loss 2.3937, val loss 2.4162\n",
      "step 74380: train loss 2.3066, val loss 2.4028\n",
      "step 74390: train loss 2.3286, val loss 2.2987\n",
      "step 74400: train loss 2.3040, val loss 2.3827\n",
      "step 74410: train loss 2.3674, val loss 2.3152\n",
      "step 74420: train loss 2.3830, val loss 2.4200\n",
      "step 74430: train loss 2.3513, val loss 2.4211\n",
      "step 74440: train loss 2.3562, val loss 2.3426\n",
      "step 74450: train loss 2.3589, val loss 2.3637\n",
      "step 74460: train loss 2.4259, val loss 2.4232\n",
      "step 74470: train loss 2.4126, val loss 2.3564\n",
      "step 74480: train loss 2.2787, val loss 2.3906\n",
      "step 74490: train loss 2.3780, val loss 2.3504\n",
      "step 74500: train loss 2.3727, val loss 2.3903\n",
      "step 74510: train loss 2.3447, val loss 2.4085\n",
      "step 74520: train loss 2.4266, val loss 2.4070\n",
      "step 74530: train loss 2.3836, val loss 2.3577\n",
      "step 74540: train loss 2.3915, val loss 2.2762\n",
      "step 74550: train loss 2.3428, val loss 2.4640\n",
      "step 74560: train loss 2.3770, val loss 2.3725\n",
      "step 74570: train loss 2.3877, val loss 2.3871\n",
      "step 74580: train loss 2.3103, val loss 2.3771\n",
      "step 74590: train loss 2.3288, val loss 2.4103\n",
      "step 74600: train loss 2.3514, val loss 2.4384\n",
      "step 74610: train loss 2.4410, val loss 2.2883\n",
      "step 74620: train loss 2.3448, val loss 2.3246\n",
      "step 74630: train loss 2.3832, val loss 2.3830\n",
      "step 74640: train loss 2.3413, val loss 2.3752\n",
      "step 74650: train loss 2.3355, val loss 2.4588\n",
      "step 74660: train loss 2.3965, val loss 2.4701\n",
      "step 74670: train loss 2.4217, val loss 2.3824\n",
      "step 74680: train loss 2.3732, val loss 2.3941\n",
      "step 74690: train loss 2.3573, val loss 2.3959\n",
      "step 74700: train loss 2.3856, val loss 2.3696\n",
      "step 74710: train loss 2.4878, val loss 2.3848\n",
      "step 74720: train loss 2.3553, val loss 2.3438\n",
      "step 74730: train loss 2.3959, val loss 2.3226\n",
      "step 74740: train loss 2.3672, val loss 2.3268\n",
      "step 74750: train loss 2.3693, val loss 2.3440\n",
      "step 74760: train loss 2.4846, val loss 2.3382\n",
      "step 74770: train loss 2.3288, val loss 2.3373\n",
      "step 74780: train loss 2.4084, val loss 2.3933\n",
      "step 74790: train loss 2.4206, val loss 2.5277\n",
      "step 74800: train loss 2.3892, val loss 2.3509\n",
      "step 74810: train loss 2.3585, val loss 2.3490\n",
      "step 74820: train loss 2.3205, val loss 2.4162\n",
      "step 74830: train loss 2.4396, val loss 2.3539\n",
      "step 74840: train loss 2.4564, val loss 2.3871\n",
      "step 74850: train loss 2.3371, val loss 2.3273\n",
      "step 74860: train loss 2.4958, val loss 2.4088\n",
      "step 74870: train loss 2.3392, val loss 2.3569\n",
      "step 74880: train loss 2.4097, val loss 2.3440\n",
      "step 74890: train loss 2.3223, val loss 2.3402\n",
      "step 74900: train loss 2.4256, val loss 2.3249\n",
      "step 74910: train loss 2.3800, val loss 2.3102\n",
      "step 74920: train loss 2.3839, val loss 2.3559\n",
      "step 74930: train loss 2.3871, val loss 2.3384\n",
      "step 74940: train loss 2.3806, val loss 2.3952\n",
      "step 74950: train loss 2.3328, val loss 2.3421\n",
      "step 74960: train loss 2.3449, val loss 2.3708\n",
      "step 74970: train loss 2.3959, val loss 2.4363\n",
      "step 74980: train loss 2.3477, val loss 2.3582\n",
      "step 74990: train loss 2.3144, val loss 2.3862\n",
      "step 75000: train loss 2.3579, val loss 2.3347\n",
      "step 75010: train loss 2.4278, val loss 2.4784\n",
      "step 75020: train loss 2.3450, val loss 2.3456\n",
      "step 75030: train loss 2.3698, val loss 2.3753\n",
      "step 75040: train loss 2.3257, val loss 2.3470\n",
      "step 75050: train loss 2.3479, val loss 2.3919\n",
      "step 75060: train loss 2.4179, val loss 2.3598\n",
      "step 75070: train loss 2.3713, val loss 2.3994\n",
      "step 75080: train loss 2.3708, val loss 2.3923\n",
      "step 75090: train loss 2.4120, val loss 2.4193\n",
      "step 75100: train loss 2.3594, val loss 2.4435\n",
      "step 75110: train loss 2.4232, val loss 2.3571\n",
      "step 75120: train loss 2.3559, val loss 2.3735\n",
      "step 75130: train loss 2.4278, val loss 2.3817\n",
      "step 75140: train loss 2.2984, val loss 2.3371\n",
      "step 75150: train loss 2.2848, val loss 2.3684\n",
      "step 75160: train loss 2.3947, val loss 2.3603\n",
      "step 75170: train loss 2.4538, val loss 2.3728\n",
      "step 75180: train loss 2.4040, val loss 2.3662\n",
      "step 75190: train loss 2.3652, val loss 2.3571\n",
      "step 75200: train loss 2.3152, val loss 2.3246\n",
      "step 75210: train loss 2.3749, val loss 2.3742\n",
      "step 75220: train loss 2.3905, val loss 2.3944\n",
      "step 75230: train loss 2.4447, val loss 2.4213\n",
      "step 75240: train loss 2.3067, val loss 2.3087\n",
      "step 75250: train loss 2.4096, val loss 2.3861\n",
      "step 75260: train loss 2.3086, val loss 2.3680\n",
      "step 75270: train loss 2.4163, val loss 2.3752\n",
      "step 75280: train loss 2.4312, val loss 2.4366\n",
      "step 75290: train loss 2.3134, val loss 2.3742\n",
      "step 75300: train loss 2.3231, val loss 2.4019\n",
      "step 75310: train loss 2.3713, val loss 2.3858\n",
      "step 75320: train loss 2.4051, val loss 2.3062\n",
      "step 75330: train loss 2.3874, val loss 2.3474\n",
      "step 75340: train loss 2.4018, val loss 2.3070\n",
      "step 75350: train loss 2.3682, val loss 2.3135\n",
      "step 75360: train loss 2.3385, val loss 2.3011\n",
      "step 75370: train loss 2.4070, val loss 2.3400\n",
      "step 75380: train loss 2.4753, val loss 2.3402\n",
      "step 75390: train loss 2.4273, val loss 2.3710\n",
      "step 75400: train loss 2.3901, val loss 2.3949\n",
      "step 75410: train loss 2.3940, val loss 2.3431\n",
      "step 75420: train loss 2.4513, val loss 2.3551\n",
      "step 75430: train loss 2.3780, val loss 2.3351\n",
      "step 75440: train loss 2.3833, val loss 2.3447\n",
      "step 75450: train loss 2.4171, val loss 2.3739\n",
      "step 75460: train loss 2.3488, val loss 2.3672\n",
      "step 75470: train loss 2.3643, val loss 2.3502\n",
      "step 75480: train loss 2.4463, val loss 2.3043\n",
      "step 75490: train loss 2.3976, val loss 2.4361\n",
      "step 75500: train loss 2.3981, val loss 2.3658\n",
      "step 75510: train loss 2.2952, val loss 2.3487\n",
      "step 75520: train loss 2.4286, val loss 2.3509\n",
      "step 75530: train loss 2.3165, val loss 2.4135\n",
      "step 75540: train loss 2.2987, val loss 2.3372\n",
      "step 75550: train loss 2.5156, val loss 2.3292\n",
      "step 75560: train loss 2.3933, val loss 2.3346\n",
      "step 75570: train loss 2.3115, val loss 2.3835\n",
      "step 75580: train loss 2.3689, val loss 2.3921\n",
      "step 75590: train loss 2.4005, val loss 2.4609\n",
      "step 75600: train loss 2.3854, val loss 2.3217\n",
      "step 75610: train loss 2.3693, val loss 2.4385\n",
      "step 75620: train loss 2.3900, val loss 2.3945\n",
      "step 75630: train loss 2.2896, val loss 2.3838\n",
      "step 75640: train loss 2.3134, val loss 2.4127\n",
      "step 75650: train loss 2.3248, val loss 2.3822\n",
      "step 75660: train loss 2.3538, val loss 2.4200\n",
      "step 75670: train loss 2.3613, val loss 2.3375\n",
      "step 75680: train loss 2.3046, val loss 2.4196\n",
      "step 75690: train loss 2.3717, val loss 2.3808\n",
      "step 75700: train loss 2.3239, val loss 2.3231\n",
      "step 75710: train loss 2.4388, val loss 2.3528\n",
      "step 75720: train loss 2.3204, val loss 2.3939\n",
      "step 75730: train loss 2.4835, val loss 2.3688\n",
      "step 75740: train loss 2.3413, val loss 2.3398\n",
      "step 75750: train loss 2.3731, val loss 2.3071\n",
      "step 75760: train loss 2.3680, val loss 2.4048\n",
      "step 75770: train loss 2.3728, val loss 2.3881\n",
      "step 75780: train loss 2.4512, val loss 2.3280\n",
      "step 75790: train loss 2.4357, val loss 2.3397\n",
      "step 75800: train loss 2.3759, val loss 2.3673\n",
      "step 75810: train loss 2.3354, val loss 2.3540\n",
      "step 75820: train loss 2.3559, val loss 2.3792\n",
      "step 75830: train loss 2.3906, val loss 2.2843\n",
      "step 75840: train loss 2.3473, val loss 2.3896\n",
      "step 75850: train loss 2.3743, val loss 2.4156\n",
      "step 75860: train loss 2.3253, val loss 2.4407\n",
      "step 75870: train loss 2.3521, val loss 2.3527\n",
      "step 75880: train loss 2.4285, val loss 2.3620\n",
      "step 75890: train loss 2.3999, val loss 2.3982\n",
      "step 75900: train loss 2.4063, val loss 2.4088\n",
      "step 75910: train loss 2.4283, val loss 2.3019\n",
      "step 75920: train loss 2.3943, val loss 2.3900\n",
      "step 75930: train loss 2.3776, val loss 2.4023\n",
      "step 75940: train loss 2.4087, val loss 2.3297\n",
      "step 75950: train loss 2.3516, val loss 2.4145\n",
      "step 75960: train loss 2.3140, val loss 2.3958\n",
      "step 75970: train loss 2.3348, val loss 2.3685\n",
      "step 75980: train loss 2.4305, val loss 2.2801\n",
      "step 75990: train loss 2.4174, val loss 2.3054\n",
      "step 76000: train loss 2.3258, val loss 2.3799\n",
      "step 76010: train loss 2.3745, val loss 2.2934\n",
      "step 76020: train loss 2.3098, val loss 2.2778\n",
      "step 76030: train loss 2.3468, val loss 2.3206\n",
      "step 76040: train loss 2.4521, val loss 2.3907\n",
      "step 76050: train loss 2.3479, val loss 2.3647\n",
      "step 76060: train loss 2.3612, val loss 2.3669\n",
      "step 76070: train loss 2.3917, val loss 2.3740\n",
      "step 76080: train loss 2.4605, val loss 2.3241\n",
      "step 76090: train loss 2.4046, val loss 2.4683\n",
      "step 76100: train loss 2.3107, val loss 2.4407\n",
      "step 76110: train loss 2.3116, val loss 2.3872\n",
      "step 76120: train loss 2.3452, val loss 2.3713\n",
      "step 76130: train loss 2.4045, val loss 2.3018\n",
      "step 76140: train loss 2.4114, val loss 2.3274\n",
      "step 76150: train loss 2.4042, val loss 2.2910\n",
      "step 76160: train loss 2.3747, val loss 2.3709\n",
      "step 76170: train loss 2.3799, val loss 2.4384\n",
      "step 76180: train loss 2.3722, val loss 2.3948\n",
      "step 76190: train loss 2.3752, val loss 2.3680\n",
      "step 76200: train loss 2.4122, val loss 2.4033\n",
      "step 76210: train loss 2.3532, val loss 2.3880\n",
      "step 76220: train loss 2.3627, val loss 2.3383\n",
      "step 76230: train loss 2.3631, val loss 2.4008\n",
      "step 76240: train loss 2.3972, val loss 2.3784\n",
      "step 76250: train loss 2.3486, val loss 2.3587\n",
      "step 76260: train loss 2.3098, val loss 2.3987\n",
      "step 76270: train loss 2.3518, val loss 2.3769\n",
      "step 76280: train loss 2.4765, val loss 2.3392\n",
      "step 76290: train loss 2.4059, val loss 2.3770\n",
      "step 76300: train loss 2.3578, val loss 2.3070\n",
      "step 76310: train loss 2.3934, val loss 2.4274\n",
      "step 76320: train loss 2.3790, val loss 2.3357\n",
      "step 76330: train loss 2.3797, val loss 2.3296\n",
      "step 76340: train loss 2.3579, val loss 2.3388\n",
      "step 76350: train loss 2.4366, val loss 2.3720\n",
      "step 76360: train loss 2.3097, val loss 2.3635\n",
      "step 76370: train loss 2.3260, val loss 2.3161\n",
      "step 76380: train loss 2.3671, val loss 2.3282\n",
      "step 76390: train loss 2.4016, val loss 2.2800\n",
      "step 76400: train loss 2.3935, val loss 2.3133\n",
      "step 76410: train loss 2.4395, val loss 2.3715\n",
      "step 76420: train loss 2.4186, val loss 2.3475\n",
      "step 76430: train loss 2.3933, val loss 2.3470\n",
      "step 76440: train loss 2.4040, val loss 2.3599\n",
      "step 76450: train loss 2.4068, val loss 2.3437\n",
      "step 76460: train loss 2.4017, val loss 2.3734\n",
      "step 76470: train loss 2.4037, val loss 2.2843\n",
      "step 76480: train loss 2.4839, val loss 2.4208\n",
      "step 76490: train loss 2.4069, val loss 2.3983\n",
      "step 76500: train loss 2.3519, val loss 2.4241\n",
      "step 76510: train loss 2.4036, val loss 2.3228\n",
      "step 76520: train loss 2.3211, val loss 2.4153\n",
      "step 76530: train loss 2.4071, val loss 2.3727\n",
      "step 76540: train loss 2.3573, val loss 2.3899\n",
      "step 76550: train loss 2.3817, val loss 2.3052\n",
      "step 76560: train loss 2.3885, val loss 2.3354\n",
      "step 76570: train loss 2.3782, val loss 2.3690\n",
      "step 76580: train loss 2.3838, val loss 2.3033\n",
      "step 76590: train loss 2.3713, val loss 2.4354\n",
      "step 76600: train loss 2.4027, val loss 2.3510\n",
      "step 76610: train loss 2.3554, val loss 2.3851\n",
      "step 76620: train loss 2.4332, val loss 2.3857\n",
      "step 76630: train loss 2.4059, val loss 2.4860\n",
      "step 76640: train loss 2.4804, val loss 2.3514\n",
      "step 76650: train loss 2.3822, val loss 2.3879\n",
      "step 76660: train loss 2.4212, val loss 2.3493\n",
      "step 76670: train loss 2.3823, val loss 2.3901\n",
      "step 76680: train loss 2.3317, val loss 2.3555\n",
      "step 76690: train loss 2.3378, val loss 2.4073\n",
      "step 76700: train loss 2.4226, val loss 2.5183\n",
      "step 76710: train loss 2.4643, val loss 2.3753\n",
      "step 76720: train loss 2.4150, val loss 2.4122\n",
      "step 76730: train loss 2.4250, val loss 2.3567\n",
      "step 76740: train loss 2.3802, val loss 2.3567\n",
      "step 76750: train loss 2.3980, val loss 2.3087\n",
      "step 76760: train loss 2.3168, val loss 2.3523\n",
      "step 76770: train loss 2.3381, val loss 2.3549\n",
      "step 76780: train loss 2.3638, val loss 2.3372\n",
      "step 76790: train loss 2.3989, val loss 2.3764\n",
      "step 76800: train loss 2.3096, val loss 2.3527\n",
      "step 76810: train loss 2.3643, val loss 2.3200\n",
      "step 76820: train loss 2.3800, val loss 2.3807\n",
      "step 76830: train loss 2.3485, val loss 2.4080\n",
      "step 76840: train loss 2.4322, val loss 2.3566\n",
      "step 76850: train loss 2.3580, val loss 2.3634\n",
      "step 76860: train loss 2.3544, val loss 2.3220\n",
      "step 76870: train loss 2.3970, val loss 2.3903\n",
      "step 76880: train loss 2.3780, val loss 2.3768\n",
      "step 76890: train loss 2.3884, val loss 2.3530\n",
      "step 76900: train loss 2.3812, val loss 2.3686\n",
      "step 76910: train loss 2.3496, val loss 2.3283\n",
      "step 76920: train loss 2.4034, val loss 2.4184\n",
      "step 76930: train loss 2.3649, val loss 2.2872\n",
      "step 76940: train loss 2.3743, val loss 2.3289\n",
      "step 76950: train loss 2.4094, val loss 2.3178\n",
      "step 76960: train loss 2.4727, val loss 2.3223\n",
      "step 76970: train loss 2.3878, val loss 2.3049\n",
      "step 76980: train loss 2.4847, val loss 2.3525\n",
      "step 76990: train loss 2.3006, val loss 2.3479\n",
      "step 77000: train loss 2.3905, val loss 2.3553\n",
      "step 77010: train loss 2.3331, val loss 2.3873\n",
      "step 77020: train loss 2.4209, val loss 2.3489\n",
      "step 77030: train loss 2.4171, val loss 2.3968\n",
      "step 77040: train loss 2.3579, val loss 2.3922\n",
      "step 77050: train loss 2.3359, val loss 2.3014\n",
      "step 77060: train loss 2.4046, val loss 2.3957\n",
      "step 77070: train loss 2.4112, val loss 2.3177\n",
      "step 77080: train loss 2.4680, val loss 2.3697\n",
      "step 77090: train loss 2.4719, val loss 2.4271\n",
      "step 77100: train loss 2.3493, val loss 2.4003\n",
      "step 77110: train loss 2.4165, val loss 2.2967\n",
      "step 77120: train loss 2.3507, val loss 2.4415\n",
      "step 77130: train loss 2.3501, val loss 2.2960\n",
      "step 77140: train loss 2.3603, val loss 2.3635\n",
      "step 77150: train loss 2.3306, val loss 2.2974\n",
      "step 77160: train loss 2.3539, val loss 2.4239\n",
      "step 77170: train loss 2.3277, val loss 2.3620\n",
      "step 77180: train loss 2.4296, val loss 2.3623\n",
      "step 77190: train loss 2.3335, val loss 2.4258\n",
      "step 77200: train loss 2.3771, val loss 2.3593\n",
      "step 77210: train loss 2.3073, val loss 2.4435\n",
      "step 77220: train loss 2.4331, val loss 2.3334\n",
      "step 77230: train loss 2.4016, val loss 2.3815\n",
      "step 77240: train loss 2.3819, val loss 2.4445\n",
      "step 77250: train loss 2.3674, val loss 2.2689\n",
      "step 77260: train loss 2.3597, val loss 2.3893\n",
      "step 77270: train loss 2.3563, val loss 2.3786\n",
      "step 77280: train loss 2.3927, val loss 2.3017\n",
      "step 77290: train loss 2.3795, val loss 2.3918\n",
      "step 77300: train loss 2.3477, val loss 2.4010\n",
      "step 77310: train loss 2.3618, val loss 2.3803\n",
      "step 77320: train loss 2.3194, val loss 2.3626\n",
      "step 77330: train loss 2.3296, val loss 2.3358\n",
      "step 77340: train loss 2.3952, val loss 2.3831\n",
      "step 77350: train loss 2.3347, val loss 2.3552\n",
      "step 77360: train loss 2.4001, val loss 2.4135\n",
      "step 77370: train loss 2.3832, val loss 2.3562\n",
      "step 77380: train loss 2.4130, val loss 2.3244\n",
      "step 77390: train loss 2.4897, val loss 2.2771\n",
      "step 77400: train loss 2.4949, val loss 2.4136\n",
      "step 77410: train loss 2.3516, val loss 2.3533\n",
      "step 77420: train loss 2.4140, val loss 2.4913\n",
      "step 77430: train loss 2.4321, val loss 2.3663\n",
      "step 77440: train loss 2.4088, val loss 2.3319\n",
      "step 77450: train loss 2.3577, val loss 2.4508\n",
      "step 77460: train loss 2.3265, val loss 2.3673\n",
      "step 77470: train loss 2.4179, val loss 2.3308\n",
      "step 77480: train loss 2.3410, val loss 2.3859\n",
      "step 77490: train loss 2.3334, val loss 2.4030\n",
      "step 77500: train loss 2.3583, val loss 2.3297\n",
      "step 77510: train loss 2.3367, val loss 2.3208\n",
      "step 77520: train loss 2.3833, val loss 2.3202\n",
      "step 77530: train loss 2.3687, val loss 2.3441\n",
      "step 77540: train loss 2.3657, val loss 2.4288\n",
      "step 77550: train loss 2.4332, val loss 2.3573\n",
      "step 77560: train loss 2.3160, val loss 2.2751\n",
      "step 77570: train loss 2.3150, val loss 2.3426\n",
      "step 77580: train loss 2.3424, val loss 2.3856\n",
      "step 77590: train loss 2.4002, val loss 2.4760\n",
      "step 77600: train loss 2.3972, val loss 2.3750\n",
      "step 77610: train loss 2.2985, val loss 2.2895\n",
      "step 77620: train loss 2.4778, val loss 2.3726\n",
      "step 77630: train loss 2.3888, val loss 2.3214\n",
      "step 77640: train loss 2.3589, val loss 2.3641\n",
      "step 77650: train loss 2.4567, val loss 2.3719\n",
      "step 77660: train loss 2.3411, val loss 2.3468\n",
      "step 77670: train loss 2.3876, val loss 2.3312\n",
      "step 77680: train loss 2.3057, val loss 2.4324\n",
      "step 77690: train loss 2.3625, val loss 2.3652\n",
      "step 77700: train loss 2.3698, val loss 2.3056\n",
      "step 77710: train loss 2.3960, val loss 2.3418\n",
      "step 77720: train loss 2.3529, val loss 2.3760\n",
      "step 77730: train loss 2.3966, val loss 2.3427\n",
      "step 77740: train loss 2.4211, val loss 2.3989\n",
      "step 77750: train loss 2.4513, val loss 2.4019\n",
      "step 77760: train loss 2.3491, val loss 2.4085\n",
      "step 77770: train loss 2.3875, val loss 2.3645\n",
      "step 77780: train loss 2.3736, val loss 2.4198\n",
      "step 77790: train loss 2.4488, val loss 2.3715\n",
      "step 77800: train loss 2.3590, val loss 2.4034\n",
      "step 77810: train loss 2.3401, val loss 2.3437\n",
      "step 77820: train loss 2.3617, val loss 2.4084\n",
      "step 77830: train loss 2.3721, val loss 2.3368\n",
      "step 77840: train loss 2.4245, val loss 2.2825\n",
      "step 77850: train loss 2.3414, val loss 2.4166\n",
      "step 77860: train loss 2.3980, val loss 2.3718\n",
      "step 77870: train loss 2.3917, val loss 2.4207\n",
      "step 77880: train loss 2.4237, val loss 2.3663\n",
      "step 77890: train loss 2.3341, val loss 2.3485\n",
      "step 77900: train loss 2.3937, val loss 2.3200\n",
      "step 77910: train loss 2.3707, val loss 2.4111\n",
      "step 77920: train loss 2.3870, val loss 2.3305\n",
      "step 77930: train loss 2.3565, val loss 2.4200\n",
      "step 77940: train loss 2.3912, val loss 2.4123\n",
      "step 77950: train loss 2.3704, val loss 2.3510\n",
      "step 77960: train loss 2.4403, val loss 2.3555\n",
      "step 77970: train loss 2.4065, val loss 2.4345\n",
      "step 77980: train loss 2.3674, val loss 2.3299\n",
      "step 77990: train loss 2.3542, val loss 2.4404\n",
      "step 78000: train loss 2.3926, val loss 2.3734\n",
      "step 78010: train loss 2.3374, val loss 2.4307\n",
      "step 78020: train loss 2.4048, val loss 2.2913\n",
      "step 78030: train loss 2.2848, val loss 2.4000\n",
      "step 78040: train loss 2.4214, val loss 2.3200\n",
      "step 78050: train loss 2.4517, val loss 2.3461\n",
      "step 78060: train loss 2.3724, val loss 2.2864\n",
      "step 78070: train loss 2.4281, val loss 2.3850\n",
      "step 78080: train loss 2.3638, val loss 2.3659\n",
      "step 78090: train loss 2.3850, val loss 2.3630\n",
      "step 78100: train loss 2.3579, val loss 2.3173\n",
      "step 78110: train loss 2.3449, val loss 2.3750\n",
      "step 78120: train loss 2.3498, val loss 2.3553\n",
      "step 78130: train loss 2.4261, val loss 2.4863\n",
      "step 78140: train loss 2.3881, val loss 2.3205\n",
      "step 78150: train loss 2.4875, val loss 2.2472\n",
      "step 78160: train loss 2.3813, val loss 2.2572\n",
      "step 78170: train loss 2.3232, val loss 2.3202\n",
      "step 78180: train loss 2.3943, val loss 2.4228\n",
      "step 78190: train loss 2.3606, val loss 2.3378\n",
      "step 78200: train loss 2.4314, val loss 2.4487\n",
      "step 78210: train loss 2.3455, val loss 2.4731\n",
      "step 78220: train loss 2.3622, val loss 2.3782\n",
      "step 78230: train loss 2.4120, val loss 2.3408\n",
      "step 78240: train loss 2.3712, val loss 2.4203\n",
      "step 78250: train loss 2.2976, val loss 2.3395\n",
      "step 78260: train loss 2.3209, val loss 2.4029\n",
      "step 78270: train loss 2.4360, val loss 2.4150\n",
      "step 78280: train loss 2.3984, val loss 2.4254\n",
      "step 78290: train loss 2.4233, val loss 2.3808\n",
      "step 78300: train loss 2.3352, val loss 2.3687\n",
      "step 78310: train loss 2.4109, val loss 2.3769\n",
      "step 78320: train loss 2.3177, val loss 2.2761\n",
      "step 78330: train loss 2.2891, val loss 2.3430\n",
      "step 78340: train loss 2.4199, val loss 2.3467\n",
      "step 78350: train loss 2.3881, val loss 2.3664\n",
      "step 78360: train loss 2.2658, val loss 2.3619\n",
      "step 78370: train loss 2.3268, val loss 2.3544\n",
      "step 78380: train loss 2.4277, val loss 2.3287\n",
      "step 78390: train loss 2.3935, val loss 2.4024\n",
      "step 78400: train loss 2.3549, val loss 2.3332\n",
      "step 78410: train loss 2.3501, val loss 2.3449\n",
      "step 78420: train loss 2.4013, val loss 2.3444\n",
      "step 78430: train loss 2.4490, val loss 2.2856\n",
      "step 78440: train loss 2.3920, val loss 2.4299\n",
      "step 78450: train loss 2.3359, val loss 2.2970\n",
      "step 78460: train loss 2.3124, val loss 2.3873\n",
      "step 78470: train loss 2.4068, val loss 2.4512\n",
      "step 78480: train loss 2.4398, val loss 2.4080\n",
      "step 78490: train loss 2.3685, val loss 2.4120\n",
      "step 78500: train loss 2.3439, val loss 2.4017\n",
      "step 78510: train loss 2.3828, val loss 2.3741\n",
      "step 78520: train loss 2.3696, val loss 2.4103\n",
      "step 78530: train loss 2.4020, val loss 2.4401\n",
      "step 78540: train loss 2.3482, val loss 2.4013\n",
      "step 78550: train loss 2.3735, val loss 2.3717\n",
      "step 78560: train loss 2.3342, val loss 2.3894\n",
      "step 78570: train loss 2.3677, val loss 2.4088\n",
      "step 78580: train loss 2.3544, val loss 2.3144\n",
      "step 78590: train loss 2.3622, val loss 2.4193\n",
      "step 78600: train loss 2.3735, val loss 2.3127\n",
      "step 78610: train loss 2.4095, val loss 2.2515\n",
      "step 78620: train loss 2.3953, val loss 2.3472\n",
      "step 78630: train loss 2.4572, val loss 2.2975\n",
      "step 78640: train loss 2.3007, val loss 2.4210\n",
      "step 78650: train loss 2.3134, val loss 2.4880\n",
      "step 78660: train loss 2.4094, val loss 2.3684\n",
      "step 78670: train loss 2.3573, val loss 2.3121\n",
      "step 78680: train loss 2.3049, val loss 2.4471\n",
      "step 78690: train loss 2.3886, val loss 2.4214\n",
      "step 78700: train loss 2.3885, val loss 2.3421\n",
      "step 78710: train loss 2.3240, val loss 2.2934\n",
      "step 78720: train loss 2.3541, val loss 2.3773\n",
      "step 78730: train loss 2.3485, val loss 2.4029\n",
      "step 78740: train loss 2.3770, val loss 2.3666\n",
      "step 78750: train loss 2.4486, val loss 2.3614\n",
      "step 78760: train loss 2.3577, val loss 2.4695\n",
      "step 78770: train loss 2.3804, val loss 2.3876\n",
      "step 78780: train loss 2.4359, val loss 2.4207\n",
      "step 78790: train loss 2.3520, val loss 2.3435\n",
      "step 78800: train loss 2.3625, val loss 2.4243\n",
      "step 78810: train loss 2.3015, val loss 2.4949\n",
      "step 78820: train loss 2.3339, val loss 2.2892\n",
      "step 78830: train loss 2.3921, val loss 2.4071\n",
      "step 78840: train loss 2.3757, val loss 2.2913\n",
      "step 78850: train loss 2.3255, val loss 2.3795\n",
      "step 78860: train loss 2.3788, val loss 2.3892\n",
      "step 78870: train loss 2.3922, val loss 2.3999\n",
      "step 78880: train loss 2.4289, val loss 2.3571\n",
      "step 78890: train loss 2.4853, val loss 2.3075\n",
      "step 78900: train loss 2.3320, val loss 2.4175\n",
      "step 78910: train loss 2.3462, val loss 2.3438\n",
      "step 78920: train loss 2.3348, val loss 2.3519\n",
      "step 78930: train loss 2.4284, val loss 2.3091\n",
      "step 78940: train loss 2.4444, val loss 2.4402\n",
      "step 78950: train loss 2.3586, val loss 2.3569\n",
      "step 78960: train loss 2.3954, val loss 2.3561\n",
      "step 78970: train loss 2.3460, val loss 2.3434\n",
      "step 78980: train loss 2.4628, val loss 2.4384\n",
      "step 78990: train loss 2.3296, val loss 2.4194\n",
      "step 79000: train loss 2.3848, val loss 2.4277\n",
      "step 79010: train loss 2.3430, val loss 2.2603\n",
      "step 79020: train loss 2.4318, val loss 2.4160\n",
      "step 79030: train loss 2.4137, val loss 2.3107\n",
      "step 79040: train loss 2.3605, val loss 2.3361\n",
      "step 79050: train loss 2.4512, val loss 2.3553\n",
      "step 79060: train loss 2.3528, val loss 2.4725\n",
      "step 79070: train loss 2.3577, val loss 2.2976\n",
      "step 79080: train loss 2.3343, val loss 2.3140\n",
      "step 79090: train loss 2.3493, val loss 2.3713\n",
      "step 79100: train loss 2.4258, val loss 2.3702\n",
      "step 79110: train loss 2.3739, val loss 2.3224\n",
      "step 79120: train loss 2.3577, val loss 2.4147\n",
      "step 79130: train loss 2.3481, val loss 2.3568\n",
      "step 79140: train loss 2.3757, val loss 2.3786\n",
      "step 79150: train loss 2.4098, val loss 2.4259\n",
      "step 79160: train loss 2.3285, val loss 2.3315\n",
      "step 79170: train loss 2.3704, val loss 2.3510\n",
      "step 79180: train loss 2.3510, val loss 2.3886\n",
      "step 79190: train loss 2.3541, val loss 2.3736\n",
      "step 79200: train loss 2.3475, val loss 2.4481\n",
      "step 79210: train loss 2.3883, val loss 2.3346\n",
      "step 79220: train loss 2.3401, val loss 2.3843\n",
      "step 79230: train loss 2.3557, val loss 2.3632\n",
      "step 79240: train loss 2.4047, val loss 2.4769\n",
      "step 79250: train loss 2.3617, val loss 2.3210\n",
      "step 79260: train loss 2.4771, val loss 2.3527\n",
      "step 79270: train loss 2.3361, val loss 2.3784\n",
      "step 79280: train loss 2.4148, val loss 2.3151\n",
      "step 79290: train loss 2.2902, val loss 2.4423\n",
      "step 79300: train loss 2.4112, val loss 2.3262\n",
      "step 79310: train loss 2.3467, val loss 2.3976\n",
      "step 79320: train loss 2.3903, val loss 2.3878\n",
      "step 79330: train loss 2.4144, val loss 2.3600\n",
      "step 79340: train loss 2.3747, val loss 2.4078\n",
      "step 79350: train loss 2.4133, val loss 2.3838\n",
      "step 79360: train loss 2.3433, val loss 2.3920\n",
      "step 79370: train loss 2.3981, val loss 2.3782\n",
      "step 79380: train loss 2.3186, val loss 2.3583\n",
      "step 79390: train loss 2.3944, val loss 2.3237\n",
      "step 79400: train loss 2.3488, val loss 2.4063\n",
      "step 79410: train loss 2.3340, val loss 2.3761\n",
      "step 79420: train loss 2.3659, val loss 2.3641\n",
      "step 79430: train loss 2.3454, val loss 2.4109\n",
      "step 79440: train loss 2.3570, val loss 2.3612\n",
      "step 79450: train loss 2.4557, val loss 2.3735\n",
      "step 79460: train loss 2.3373, val loss 2.4404\n",
      "step 79470: train loss 2.4112, val loss 2.4276\n",
      "step 79480: train loss 2.3802, val loss 2.4570\n",
      "step 79490: train loss 2.4080, val loss 2.5003\n",
      "step 79500: train loss 2.4416, val loss 2.3606\n",
      "step 79510: train loss 2.3783, val loss 2.3033\n",
      "step 79520: train loss 2.2979, val loss 2.3859\n",
      "step 79530: train loss 2.3477, val loss 2.4503\n",
      "step 79540: train loss 2.3017, val loss 2.3448\n",
      "step 79550: train loss 2.4346, val loss 2.5222\n",
      "step 79560: train loss 2.4595, val loss 2.3992\n",
      "step 79570: train loss 2.3334, val loss 2.3488\n",
      "step 79580: train loss 2.3470, val loss 2.3882\n",
      "step 79590: train loss 2.4207, val loss 2.4220\n",
      "step 79600: train loss 2.3427, val loss 2.3569\n",
      "step 79610: train loss 2.4043, val loss 2.4088\n",
      "step 79620: train loss 2.4467, val loss 2.4019\n",
      "step 79630: train loss 2.4014, val loss 2.3284\n",
      "step 79640: train loss 2.3282, val loss 2.3870\n",
      "step 79650: train loss 2.3306, val loss 2.2595\n",
      "step 79660: train loss 2.3823, val loss 2.3941\n",
      "step 79670: train loss 2.3901, val loss 2.3736\n",
      "step 79680: train loss 2.3484, val loss 2.3610\n",
      "step 79690: train loss 2.4104, val loss 2.4163\n",
      "step 79700: train loss 2.4089, val loss 2.3177\n",
      "step 79710: train loss 2.3043, val loss 2.3096\n",
      "step 79720: train loss 2.3721, val loss 2.3490\n",
      "step 79730: train loss 2.4190, val loss 2.3325\n",
      "step 79740: train loss 2.3794, val loss 2.3126\n",
      "step 79750: train loss 2.3945, val loss 2.3258\n",
      "step 79760: train loss 2.3223, val loss 2.3876\n",
      "step 79770: train loss 2.3610, val loss 2.3704\n",
      "step 79780: train loss 2.3449, val loss 2.3955\n",
      "step 79790: train loss 2.3867, val loss 2.3092\n",
      "step 79800: train loss 2.3443, val loss 2.3307\n",
      "step 79810: train loss 2.3324, val loss 2.3534\n",
      "step 79820: train loss 2.4046, val loss 2.3828\n",
      "step 79830: train loss 2.3837, val loss 2.2784\n",
      "step 79840: train loss 2.3618, val loss 2.4092\n",
      "step 79850: train loss 2.4133, val loss 2.4264\n",
      "step 79860: train loss 2.3680, val loss 2.4157\n",
      "step 79870: train loss 2.4450, val loss 2.3395\n",
      "step 79880: train loss 2.4062, val loss 2.3827\n",
      "step 79890: train loss 2.4255, val loss 2.3793\n",
      "step 79900: train loss 2.3957, val loss 2.3568\n",
      "step 79910: train loss 2.3720, val loss 2.3947\n",
      "step 79920: train loss 2.4416, val loss 2.4357\n",
      "step 79930: train loss 2.4206, val loss 2.3575\n",
      "step 79940: train loss 2.3991, val loss 2.3659\n",
      "step 79950: train loss 2.3724, val loss 2.3426\n",
      "step 79960: train loss 2.3709, val loss 2.3090\n",
      "step 79970: train loss 2.3983, val loss 2.3962\n",
      "step 79980: train loss 2.3820, val loss 2.3272\n",
      "step 79990: train loss 2.3467, val loss 2.3547\n",
      "step 80000: train loss 2.3727, val loss 2.3807\n",
      "step 80010: train loss 2.3739, val loss 2.3677\n",
      "step 80020: train loss 2.3591, val loss 2.3328\n",
      "step 80030: train loss 2.3686, val loss 2.4045\n",
      "step 80040: train loss 2.3339, val loss 2.4627\n",
      "step 80050: train loss 2.3639, val loss 2.3251\n",
      "step 80060: train loss 2.4082, val loss 2.3282\n",
      "step 80070: train loss 2.3453, val loss 2.3856\n",
      "step 80080: train loss 2.3908, val loss 2.3458\n",
      "step 80090: train loss 2.4226, val loss 2.3482\n",
      "step 80100: train loss 2.3792, val loss 2.3817\n",
      "step 80110: train loss 2.4295, val loss 2.3279\n",
      "step 80120: train loss 2.2647, val loss 2.3731\n",
      "step 80130: train loss 2.3442, val loss 2.3241\n",
      "step 80140: train loss 2.3306, val loss 2.2919\n",
      "step 80150: train loss 2.3601, val loss 2.3331\n",
      "step 80160: train loss 2.3619, val loss 2.4292\n",
      "step 80170: train loss 2.3573, val loss 2.3033\n",
      "step 80180: train loss 2.3790, val loss 2.3953\n",
      "step 80190: train loss 2.3602, val loss 2.3496\n",
      "step 80200: train loss 2.4276, val loss 2.3383\n",
      "step 80210: train loss 2.3541, val loss 2.3436\n",
      "step 80220: train loss 2.3629, val loss 2.3934\n",
      "step 80230: train loss 2.4269, val loss 2.4668\n",
      "step 80240: train loss 2.3728, val loss 2.3502\n",
      "step 80250: train loss 2.3560, val loss 2.3660\n",
      "step 80260: train loss 2.4511, val loss 2.3741\n",
      "step 80270: train loss 2.4248, val loss 2.2622\n",
      "step 80280: train loss 2.3640, val loss 2.3544\n",
      "step 80290: train loss 2.3544, val loss 2.3503\n",
      "step 80300: train loss 2.3911, val loss 2.3554\n",
      "step 80310: train loss 2.4007, val loss 2.3042\n",
      "step 80320: train loss 2.3619, val loss 2.3488\n",
      "step 80330: train loss 2.3070, val loss 2.3858\n",
      "step 80340: train loss 2.3050, val loss 2.4351\n",
      "step 80350: train loss 2.3233, val loss 2.2751\n",
      "step 80360: train loss 2.4045, val loss 2.3009\n",
      "step 80370: train loss 2.3446, val loss 2.3748\n",
      "step 80380: train loss 2.3752, val loss 2.3151\n",
      "step 80390: train loss 2.4108, val loss 2.3413\n",
      "step 80400: train loss 2.3633, val loss 2.3102\n",
      "step 80410: train loss 2.3527, val loss 2.2674\n",
      "step 80420: train loss 2.4510, val loss 2.3361\n",
      "step 80430: train loss 2.3333, val loss 2.3590\n",
      "step 80440: train loss 2.3862, val loss 2.3900\n",
      "step 80450: train loss 2.3485, val loss 2.3497\n",
      "step 80460: train loss 2.3369, val loss 2.3395\n",
      "step 80470: train loss 2.4003, val loss 2.2952\n",
      "step 80480: train loss 2.4084, val loss 2.2926\n",
      "step 80490: train loss 2.3803, val loss 2.3487\n",
      "step 80500: train loss 2.4194, val loss 2.4267\n",
      "step 80510: train loss 2.3959, val loss 2.3624\n",
      "step 80520: train loss 2.3589, val loss 2.4276\n",
      "step 80530: train loss 2.3605, val loss 2.3380\n",
      "step 80540: train loss 2.4074, val loss 2.4664\n",
      "step 80550: train loss 2.3330, val loss 2.3719\n",
      "step 80560: train loss 2.4034, val loss 2.3114\n",
      "step 80570: train loss 2.3314, val loss 2.3088\n",
      "step 80580: train loss 2.3923, val loss 2.3621\n",
      "step 80590: train loss 2.2993, val loss 2.4006\n",
      "step 80600: train loss 2.2877, val loss 2.4026\n",
      "step 80610: train loss 2.4354, val loss 2.2954\n",
      "step 80620: train loss 2.3833, val loss 2.3197\n",
      "step 80630: train loss 2.3838, val loss 2.3797\n",
      "step 80640: train loss 2.3018, val loss 2.5324\n",
      "step 80650: train loss 2.3816, val loss 2.3434\n",
      "step 80660: train loss 2.3882, val loss 2.3643\n",
      "step 80670: train loss 2.4658, val loss 2.3534\n",
      "step 80680: train loss 2.3602, val loss 2.4257\n",
      "step 80690: train loss 2.3967, val loss 2.3191\n",
      "step 80700: train loss 2.3751, val loss 2.4247\n",
      "step 80710: train loss 2.3527, val loss 2.3768\n",
      "step 80720: train loss 2.3046, val loss 2.3233\n",
      "step 80730: train loss 2.3526, val loss 2.3942\n",
      "step 80740: train loss 2.4802, val loss 2.3692\n",
      "step 80750: train loss 2.3123, val loss 2.3198\n",
      "step 80760: train loss 2.4248, val loss 2.3720\n",
      "step 80770: train loss 2.4467, val loss 2.3008\n",
      "step 80780: train loss 2.4111, val loss 2.4597\n",
      "step 80790: train loss 2.4046, val loss 2.3477\n",
      "step 80800: train loss 2.3752, val loss 2.3454\n",
      "step 80810: train loss 2.3342, val loss 2.3878\n",
      "step 80820: train loss 2.3783, val loss 2.4059\n",
      "step 80830: train loss 2.3966, val loss 2.3516\n",
      "step 80840: train loss 2.4243, val loss 2.3423\n",
      "step 80850: train loss 2.4344, val loss 2.3760\n",
      "step 80860: train loss 2.4119, val loss 2.3901\n",
      "step 80870: train loss 2.3636, val loss 2.3838\n",
      "step 80880: train loss 2.3115, val loss 2.3051\n",
      "step 80890: train loss 2.3798, val loss 2.3164\n",
      "step 80900: train loss 2.3891, val loss 2.5185\n",
      "step 80910: train loss 2.4080, val loss 2.3167\n",
      "step 80920: train loss 2.3659, val loss 2.4616\n",
      "step 80930: train loss 2.3730, val loss 2.4231\n",
      "step 80940: train loss 2.4019, val loss 2.3585\n",
      "step 80950: train loss 2.4139, val loss 2.4120\n",
      "step 80960: train loss 2.4029, val loss 2.3698\n",
      "step 80970: train loss 2.4000, val loss 2.3156\n",
      "step 80980: train loss 2.4085, val loss 2.4227\n",
      "step 80990: train loss 2.3185, val loss 2.3423\n",
      "step 81000: train loss 2.3840, val loss 2.3554\n",
      "step 81010: train loss 2.3421, val loss 2.3249\n",
      "step 81020: train loss 2.3694, val loss 2.3663\n",
      "step 81030: train loss 2.3743, val loss 2.4215\n",
      "step 81040: train loss 2.3752, val loss 2.3590\n",
      "step 81050: train loss 2.4110, val loss 2.3750\n",
      "step 81060: train loss 2.4241, val loss 2.4026\n",
      "step 81070: train loss 2.3633, val loss 2.3462\n",
      "step 81080: train loss 2.3726, val loss 2.3548\n",
      "step 81090: train loss 2.3809, val loss 2.4611\n",
      "step 81100: train loss 2.3788, val loss 2.4194\n",
      "step 81110: train loss 2.3608, val loss 2.3834\n",
      "step 81120: train loss 2.3689, val loss 2.4282\n",
      "step 81130: train loss 2.3850, val loss 2.3294\n",
      "step 81140: train loss 2.4270, val loss 2.4163\n",
      "step 81150: train loss 2.3485, val loss 2.3305\n",
      "step 81160: train loss 2.3727, val loss 2.3740\n",
      "step 81170: train loss 2.3720, val loss 2.3043\n",
      "step 81180: train loss 2.4490, val loss 2.4319\n",
      "step 81190: train loss 2.3166, val loss 2.3759\n",
      "step 81200: train loss 2.3214, val loss 2.4438\n",
      "step 81210: train loss 2.4331, val loss 2.3328\n",
      "step 81220: train loss 2.2890, val loss 2.3486\n",
      "step 81230: train loss 2.3775, val loss 2.3439\n",
      "step 81240: train loss 2.3622, val loss 2.3265\n",
      "step 81250: train loss 2.3658, val loss 2.3722\n",
      "step 81260: train loss 2.4203, val loss 2.3928\n",
      "step 81270: train loss 2.3208, val loss 2.4011\n",
      "step 81280: train loss 2.3435, val loss 2.4416\n",
      "step 81290: train loss 2.3252, val loss 2.3914\n",
      "step 81300: train loss 2.4775, val loss 2.3417\n",
      "step 81310: train loss 2.4001, val loss 2.5154\n",
      "step 81320: train loss 2.3961, val loss 2.3503\n",
      "step 81330: train loss 2.3569, val loss 2.3809\n",
      "step 81340: train loss 2.3506, val loss 2.3299\n",
      "step 81350: train loss 2.4689, val loss 2.3006\n",
      "step 81360: train loss 2.3581, val loss 2.3486\n",
      "step 81370: train loss 2.3435, val loss 2.3718\n",
      "step 81380: train loss 2.3555, val loss 2.4712\n",
      "step 81390: train loss 2.3628, val loss 2.3988\n",
      "step 81400: train loss 2.4431, val loss 2.2199\n",
      "step 81410: train loss 2.3679, val loss 2.4012\n",
      "step 81420: train loss 2.3951, val loss 2.3221\n",
      "step 81430: train loss 2.3154, val loss 2.4183\n",
      "step 81440: train loss 2.3409, val loss 2.3030\n",
      "step 81450: train loss 2.3494, val loss 2.3115\n",
      "step 81460: train loss 2.3544, val loss 2.3246\n",
      "step 81470: train loss 2.3391, val loss 2.3849\n",
      "step 81480: train loss 2.4141, val loss 2.3650\n",
      "step 81490: train loss 2.3925, val loss 2.3253\n",
      "step 81500: train loss 2.2802, val loss 2.4321\n",
      "step 81510: train loss 2.3819, val loss 2.3399\n",
      "step 81520: train loss 2.3633, val loss 2.4340\n",
      "step 81530: train loss 2.3930, val loss 2.3590\n",
      "step 81540: train loss 2.3932, val loss 2.3770\n",
      "step 81550: train loss 2.3425, val loss 2.3577\n",
      "step 81560: train loss 2.4534, val loss 2.5813\n",
      "step 81570: train loss 2.3780, val loss 2.3438\n",
      "step 81580: train loss 2.3624, val loss 2.3773\n",
      "step 81590: train loss 2.3973, val loss 2.3957\n",
      "step 81600: train loss 2.3990, val loss 2.3822\n",
      "step 81610: train loss 2.4153, val loss 2.3711\n",
      "step 81620: train loss 2.2820, val loss 2.4618\n",
      "step 81630: train loss 2.3844, val loss 2.4176\n",
      "step 81640: train loss 2.3705, val loss 2.2859\n",
      "step 81650: train loss 2.4332, val loss 2.3413\n",
      "step 81660: train loss 2.3681, val loss 2.4539\n",
      "step 81670: train loss 2.4322, val loss 2.3207\n",
      "step 81680: train loss 2.4137, val loss 2.3465\n",
      "step 81690: train loss 2.4407, val loss 2.3928\n",
      "step 81700: train loss 2.3891, val loss 2.3777\n",
      "step 81710: train loss 2.3101, val loss 2.3231\n",
      "step 81720: train loss 2.3172, val loss 2.4181\n",
      "step 81730: train loss 2.3602, val loss 2.4078\n",
      "step 81740: train loss 2.3178, val loss 2.3075\n",
      "step 81750: train loss 2.3604, val loss 2.2943\n",
      "step 81760: train loss 2.4215, val loss 2.2887\n",
      "step 81770: train loss 2.4842, val loss 2.3598\n",
      "step 81780: train loss 2.3878, val loss 2.3437\n",
      "step 81790: train loss 2.3599, val loss 2.3462\n",
      "step 81800: train loss 2.3330, val loss 2.3926\n",
      "step 81810: train loss 2.3487, val loss 2.4012\n",
      "step 81820: train loss 2.3247, val loss 2.3983\n",
      "step 81830: train loss 2.3392, val loss 2.4227\n",
      "step 81840: train loss 2.3910, val loss 2.3135\n",
      "step 81850: train loss 2.3918, val loss 2.4272\n",
      "step 81860: train loss 2.3693, val loss 2.3809\n",
      "step 81870: train loss 2.3014, val loss 2.3623\n",
      "step 81880: train loss 2.5105, val loss 2.3142\n",
      "step 81890: train loss 2.3892, val loss 2.3697\n",
      "step 81900: train loss 2.3558, val loss 2.3879\n",
      "step 81910: train loss 2.4453, val loss 2.3796\n",
      "step 81920: train loss 2.3325, val loss 2.4002\n",
      "step 81930: train loss 2.3682, val loss 2.2487\n",
      "step 81940: train loss 2.3132, val loss 2.3745\n",
      "step 81950: train loss 2.4250, val loss 2.4139\n",
      "step 81960: train loss 2.2605, val loss 2.3102\n",
      "step 81970: train loss 2.2857, val loss 2.3744\n",
      "step 81980: train loss 2.3338, val loss 2.3406\n",
      "step 81990: train loss 2.4329, val loss 2.4457\n",
      "step 82000: train loss 2.5017, val loss 2.3745\n",
      "step 82010: train loss 2.3782, val loss 2.2895\n",
      "step 82020: train loss 2.3616, val loss 2.3409\n",
      "step 82030: train loss 2.3930, val loss 2.3592\n",
      "step 82040: train loss 2.5256, val loss 2.3714\n",
      "step 82050: train loss 2.4121, val loss 2.3936\n",
      "step 82060: train loss 2.3937, val loss 2.3116\n",
      "step 82070: train loss 2.3610, val loss 2.3849\n",
      "step 82080: train loss 2.3327, val loss 2.3002\n",
      "step 82090: train loss 2.3778, val loss 2.2755\n",
      "step 82100: train loss 2.4468, val loss 2.3605\n",
      "step 82110: train loss 2.3837, val loss 2.3662\n",
      "step 82120: train loss 2.4128, val loss 2.4222\n",
      "step 82130: train loss 2.3779, val loss 2.3507\n",
      "step 82140: train loss 2.4001, val loss 2.3729\n",
      "step 82150: train loss 2.3742, val loss 2.3623\n",
      "step 82160: train loss 2.3260, val loss 2.3915\n",
      "step 82170: train loss 2.3975, val loss 2.3887\n",
      "step 82180: train loss 2.3261, val loss 2.3933\n",
      "step 82190: train loss 2.3974, val loss 2.3519\n",
      "step 82200: train loss 2.4047, val loss 2.3506\n",
      "step 82210: train loss 2.4052, val loss 2.4091\n",
      "step 82220: train loss 2.3388, val loss 2.3606\n",
      "step 82230: train loss 2.3087, val loss 2.3710\n",
      "step 82240: train loss 2.3900, val loss 2.3465\n",
      "step 82250: train loss 2.3751, val loss 2.3858\n",
      "step 82260: train loss 2.3875, val loss 2.2798\n",
      "step 82270: train loss 2.3950, val loss 2.3645\n",
      "step 82280: train loss 2.4025, val loss 2.3372\n",
      "step 82290: train loss 2.4548, val loss 2.3949\n",
      "step 82300: train loss 2.3767, val loss 2.3417\n",
      "step 82310: train loss 2.3514, val loss 2.3348\n",
      "step 82320: train loss 2.3930, val loss 2.3646\n",
      "step 82330: train loss 2.4319, val loss 2.3838\n",
      "step 82340: train loss 2.3870, val loss 2.3995\n",
      "step 82350: train loss 2.4115, val loss 2.3447\n",
      "step 82360: train loss 2.3706, val loss 2.3714\n",
      "step 82370: train loss 2.4005, val loss 2.3274\n",
      "step 82380: train loss 2.3666, val loss 2.4132\n",
      "step 82390: train loss 2.3758, val loss 2.4085\n",
      "step 82400: train loss 2.3905, val loss 2.3766\n",
      "step 82410: train loss 2.3848, val loss 2.3250\n",
      "step 82420: train loss 2.3802, val loss 2.2983\n",
      "step 82430: train loss 2.3898, val loss 2.3613\n",
      "step 82440: train loss 2.4502, val loss 2.4148\n",
      "step 82450: train loss 2.3736, val loss 2.3468\n",
      "step 82460: train loss 2.3805, val loss 2.3635\n",
      "step 82470: train loss 2.3217, val loss 2.4244\n",
      "step 82480: train loss 2.3381, val loss 2.3805\n",
      "step 82490: train loss 2.2752, val loss 2.4209\n",
      "step 82500: train loss 2.3252, val loss 2.4438\n",
      "step 82510: train loss 2.3007, val loss 2.3577\n",
      "step 82520: train loss 2.4407, val loss 2.3592\n",
      "step 82530: train loss 2.3903, val loss 2.3367\n",
      "step 82540: train loss 2.4127, val loss 2.2941\n",
      "step 82550: train loss 2.3453, val loss 2.3622\n",
      "step 82560: train loss 2.3506, val loss 2.4095\n",
      "step 82570: train loss 2.3807, val loss 2.4401\n",
      "step 82580: train loss 2.3523, val loss 2.3866\n",
      "step 82590: train loss 2.3572, val loss 2.2689\n",
      "step 82600: train loss 2.4414, val loss 2.3432\n",
      "step 82610: train loss 2.4403, val loss 2.3057\n",
      "step 82620: train loss 2.4034, val loss 2.3970\n",
      "step 82630: train loss 2.4358, val loss 2.2828\n",
      "step 82640: train loss 2.4271, val loss 2.3369\n",
      "step 82650: train loss 2.3987, val loss 2.4111\n",
      "step 82660: train loss 2.4266, val loss 2.3970\n",
      "step 82670: train loss 2.3978, val loss 2.3001\n",
      "step 82680: train loss 2.3006, val loss 2.3818\n",
      "step 82690: train loss 2.3327, val loss 2.4588\n",
      "step 82700: train loss 2.3279, val loss 2.3879\n",
      "step 82710: train loss 2.4371, val loss 2.3822\n",
      "step 82720: train loss 2.3595, val loss 2.3573\n",
      "step 82730: train loss 2.3538, val loss 2.3291\n",
      "step 82740: train loss 2.3322, val loss 2.3761\n",
      "step 82750: train loss 2.3347, val loss 2.4046\n",
      "step 82760: train loss 2.3490, val loss 2.3648\n",
      "step 82770: train loss 2.3569, val loss 2.4691\n",
      "step 82780: train loss 2.4453, val loss 2.3977\n",
      "step 82790: train loss 2.3552, val loss 2.3703\n",
      "step 82800: train loss 2.3903, val loss 2.3798\n",
      "step 82810: train loss 2.3660, val loss 2.4167\n",
      "step 82820: train loss 2.4496, val loss 2.4488\n",
      "step 82830: train loss 2.3124, val loss 2.3416\n",
      "step 82840: train loss 2.4022, val loss 2.4005\n",
      "step 82850: train loss 2.3539, val loss 2.3877\n",
      "step 82860: train loss 2.3991, val loss 2.3323\n",
      "step 82870: train loss 2.4183, val loss 2.3284\n",
      "step 82880: train loss 2.3306, val loss 2.4265\n",
      "step 82890: train loss 2.3752, val loss 2.2961\n",
      "step 82900: train loss 2.3381, val loss 2.3522\n",
      "step 82910: train loss 2.3180, val loss 2.3731\n",
      "step 82920: train loss 2.4139, val loss 2.3796\n",
      "step 82930: train loss 2.3513, val loss 2.3591\n",
      "step 82940: train loss 2.3757, val loss 2.3235\n",
      "step 82950: train loss 2.3199, val loss 2.4359\n",
      "step 82960: train loss 2.4372, val loss 2.3907\n",
      "step 82970: train loss 2.3749, val loss 2.3657\n",
      "step 82980: train loss 2.3986, val loss 2.3836\n",
      "step 82990: train loss 2.3808, val loss 2.3635\n",
      "step 83000: train loss 2.3891, val loss 2.3416\n",
      "step 83010: train loss 2.4315, val loss 2.4298\n",
      "step 83020: train loss 2.3632, val loss 2.3665\n",
      "step 83030: train loss 2.3153, val loss 2.3501\n",
      "step 83040: train loss 2.3318, val loss 2.3457\n",
      "step 83050: train loss 2.4041, val loss 2.2998\n",
      "step 83060: train loss 2.4086, val loss 2.3205\n",
      "step 83070: train loss 2.3879, val loss 2.3579\n",
      "step 83080: train loss 2.3891, val loss 2.3564\n",
      "step 83090: train loss 2.3226, val loss 2.3997\n",
      "step 83100: train loss 2.3630, val loss 2.3538\n",
      "step 83110: train loss 2.3360, val loss 2.3658\n",
      "step 83120: train loss 2.4107, val loss 2.3543\n",
      "step 83130: train loss 2.3655, val loss 2.2493\n",
      "step 83140: train loss 2.3062, val loss 2.3958\n",
      "step 83150: train loss 2.3701, val loss 2.3678\n",
      "step 83160: train loss 2.3976, val loss 2.4318\n",
      "step 83170: train loss 2.3821, val loss 2.3633\n",
      "step 83180: train loss 2.3095, val loss 2.3326\n",
      "step 83190: train loss 2.4114, val loss 2.3481\n",
      "step 83200: train loss 2.3777, val loss 2.3196\n",
      "step 83210: train loss 2.4437, val loss 2.3022\n",
      "step 83220: train loss 2.4065, val loss 2.3604\n",
      "step 83230: train loss 2.3768, val loss 2.2595\n",
      "step 83240: train loss 2.3990, val loss 2.3565\n",
      "step 83250: train loss 2.3411, val loss 2.4061\n",
      "step 83260: train loss 2.3602, val loss 2.3936\n",
      "step 83270: train loss 2.3591, val loss 2.3109\n",
      "step 83280: train loss 2.3808, val loss 2.2936\n",
      "step 83290: train loss 2.3041, val loss 2.4063\n",
      "step 83300: train loss 2.3773, val loss 2.3713\n",
      "step 83310: train loss 2.3553, val loss 2.4137\n",
      "step 83320: train loss 2.3171, val loss 2.3048\n",
      "step 83330: train loss 2.3373, val loss 2.3342\n",
      "step 83340: train loss 2.3864, val loss 2.3829\n",
      "step 83350: train loss 2.3901, val loss 2.3990\n",
      "step 83360: train loss 2.3936, val loss 2.3424\n",
      "step 83370: train loss 2.3263, val loss 2.3095\n",
      "step 83380: train loss 2.3064, val loss 2.3247\n",
      "step 83390: train loss 2.2839, val loss 2.3681\n",
      "step 83400: train loss 2.3209, val loss 2.4064\n",
      "step 83410: train loss 2.3767, val loss 2.3691\n",
      "step 83420: train loss 2.3789, val loss 2.3535\n",
      "step 83430: train loss 2.4030, val loss 2.3068\n",
      "step 83440: train loss 2.3864, val loss 2.3528\n",
      "step 83450: train loss 2.4262, val loss 2.4403\n",
      "step 83460: train loss 2.3479, val loss 2.3953\n",
      "step 83470: train loss 2.4199, val loss 2.4102\n",
      "step 83480: train loss 2.3473, val loss 2.4245\n",
      "step 83490: train loss 2.3691, val loss 2.3192\n",
      "step 83500: train loss 2.3638, val loss 2.3824\n",
      "step 83510: train loss 2.2972, val loss 2.4050\n",
      "step 83520: train loss 2.4205, val loss 2.3515\n",
      "step 83530: train loss 2.3718, val loss 2.3279\n",
      "step 83540: train loss 2.3451, val loss 2.4010\n",
      "step 83550: train loss 2.3029, val loss 2.3728\n",
      "step 83560: train loss 2.4616, val loss 2.3675\n",
      "step 83570: train loss 2.3489, val loss 2.3930\n",
      "step 83580: train loss 2.3995, val loss 2.3436\n",
      "step 83590: train loss 2.4053, val loss 2.3405\n",
      "step 83600: train loss 2.3320, val loss 2.3570\n",
      "step 83610: train loss 2.4217, val loss 2.3218\n",
      "step 83620: train loss 2.3947, val loss 2.3958\n",
      "step 83630: train loss 2.4047, val loss 2.3194\n",
      "step 83640: train loss 2.4301, val loss 2.3791\n",
      "step 83650: train loss 2.2429, val loss 2.4412\n",
      "step 83660: train loss 2.3290, val loss 2.3658\n",
      "step 83670: train loss 2.3230, val loss 2.4287\n",
      "step 83680: train loss 2.3763, val loss 2.3913\n",
      "step 83690: train loss 2.3670, val loss 2.3206\n",
      "step 83700: train loss 2.2835, val loss 2.3459\n",
      "step 83710: train loss 2.4410, val loss 2.3749\n",
      "step 83720: train loss 2.3870, val loss 2.3581\n",
      "step 83730: train loss 2.3393, val loss 2.3712\n",
      "step 83740: train loss 2.4076, val loss 2.3218\n",
      "step 83750: train loss 2.4163, val loss 2.3483\n",
      "step 83760: train loss 2.4018, val loss 2.3973\n",
      "step 83770: train loss 2.3741, val loss 2.3789\n",
      "step 83780: train loss 2.3864, val loss 2.3428\n",
      "step 83790: train loss 2.3799, val loss 2.3498\n",
      "step 83800: train loss 2.4207, val loss 2.3516\n",
      "step 83810: train loss 2.4235, val loss 2.3759\n",
      "step 83820: train loss 2.2656, val loss 2.3284\n",
      "step 83830: train loss 2.4832, val loss 2.2779\n",
      "step 83840: train loss 2.3563, val loss 2.3423\n",
      "step 83850: train loss 2.4079, val loss 2.4083\n",
      "step 83860: train loss 2.3723, val loss 2.3519\n",
      "step 83870: train loss 2.3820, val loss 2.2852\n",
      "step 83880: train loss 2.3472, val loss 2.3434\n",
      "step 83890: train loss 2.3398, val loss 2.3740\n",
      "step 83900: train loss 2.3676, val loss 2.3696\n",
      "step 83910: train loss 2.3508, val loss 2.3220\n",
      "step 83920: train loss 2.3817, val loss 2.3630\n",
      "step 83930: train loss 2.2861, val loss 2.3071\n",
      "step 83940: train loss 2.3848, val loss 2.3611\n",
      "step 83950: train loss 2.4148, val loss 2.3771\n",
      "step 83960: train loss 2.3326, val loss 2.3973\n",
      "step 83970: train loss 2.3838, val loss 2.3052\n",
      "step 83980: train loss 2.3974, val loss 2.3217\n",
      "step 83990: train loss 2.4407, val loss 2.3278\n",
      "step 84000: train loss 2.3127, val loss 2.4373\n",
      "step 84010: train loss 2.3631, val loss 2.3578\n",
      "step 84020: train loss 2.3929, val loss 2.3840\n",
      "step 84030: train loss 2.3938, val loss 2.3439\n",
      "step 84040: train loss 2.3519, val loss 2.3539\n",
      "step 84050: train loss 2.3426, val loss 2.4325\n",
      "step 84060: train loss 2.4044, val loss 2.4246\n",
      "step 84070: train loss 2.4036, val loss 2.4199\n",
      "step 84080: train loss 2.3898, val loss 2.3596\n",
      "step 84090: train loss 2.3967, val loss 2.3342\n",
      "step 84100: train loss 2.4022, val loss 2.3463\n",
      "step 84110: train loss 2.4140, val loss 2.3598\n",
      "step 84120: train loss 2.4671, val loss 2.5085\n",
      "step 84130: train loss 2.3998, val loss 2.3920\n",
      "step 84140: train loss 2.3375, val loss 2.4146\n",
      "step 84150: train loss 2.2955, val loss 2.4053\n",
      "step 84160: train loss 2.4045, val loss 2.3950\n",
      "step 84170: train loss 2.3876, val loss 2.2984\n",
      "step 84180: train loss 2.3951, val loss 2.3737\n",
      "step 84190: train loss 2.3116, val loss 2.3491\n",
      "step 84200: train loss 2.3910, val loss 2.3353\n",
      "step 84210: train loss 2.4214, val loss 2.3617\n",
      "step 84220: train loss 2.4121, val loss 2.4353\n",
      "step 84230: train loss 2.3576, val loss 2.3194\n",
      "step 84240: train loss 2.3595, val loss 2.3876\n",
      "step 84250: train loss 2.4026, val loss 2.4129\n",
      "step 84260: train loss 2.3493, val loss 2.4016\n",
      "step 84270: train loss 2.3900, val loss 2.3722\n",
      "step 84280: train loss 2.3840, val loss 2.3192\n",
      "step 84290: train loss 2.3625, val loss 2.3512\n",
      "step 84300: train loss 2.3070, val loss 2.4523\n",
      "step 84310: train loss 2.3535, val loss 2.3379\n",
      "step 84320: train loss 2.4034, val loss 2.3471\n",
      "step 84330: train loss 2.4132, val loss 2.4123\n",
      "step 84340: train loss 2.3861, val loss 2.3198\n",
      "step 84350: train loss 2.3776, val loss 2.3836\n",
      "step 84360: train loss 2.3819, val loss 2.3391\n",
      "step 84370: train loss 2.4418, val loss 2.3455\n",
      "step 84380: train loss 2.3534, val loss 2.3713\n",
      "step 84390: train loss 2.3771, val loss 2.2920\n",
      "step 84400: train loss 2.4742, val loss 2.3228\n",
      "step 84410: train loss 2.3558, val loss 2.2850\n",
      "step 84420: train loss 2.3276, val loss 2.3720\n",
      "step 84430: train loss 2.4378, val loss 2.3428\n",
      "step 84440: train loss 2.3163, val loss 2.3573\n",
      "step 84450: train loss 2.3888, val loss 2.4042\n",
      "step 84460: train loss 2.3630, val loss 2.3833\n",
      "step 84470: train loss 2.3235, val loss 2.3420\n",
      "step 84480: train loss 2.3221, val loss 2.4426\n",
      "step 84490: train loss 2.3237, val loss 2.4569\n",
      "step 84500: train loss 2.3768, val loss 2.4105\n",
      "step 84510: train loss 2.3671, val loss 2.4431\n",
      "step 84520: train loss 2.3523, val loss 2.3704\n",
      "step 84530: train loss 2.3364, val loss 2.3562\n",
      "step 84540: train loss 2.3596, val loss 2.3782\n",
      "step 84550: train loss 2.3979, val loss 2.4549\n",
      "step 84560: train loss 2.4699, val loss 2.4011\n",
      "step 84570: train loss 2.4731, val loss 2.3837\n",
      "step 84580: train loss 2.2908, val loss 2.3374\n",
      "step 84590: train loss 2.3839, val loss 2.4750\n",
      "step 84600: train loss 2.3786, val loss 2.3522\n",
      "step 84610: train loss 2.4253, val loss 2.4036\n",
      "step 84620: train loss 2.3979, val loss 2.3799\n",
      "step 84630: train loss 2.3793, val loss 2.4393\n",
      "step 84640: train loss 2.3540, val loss 2.3332\n",
      "step 84650: train loss 2.3331, val loss 2.3109\n",
      "step 84660: train loss 2.3492, val loss 2.3840\n",
      "step 84670: train loss 2.3860, val loss 2.3984\n",
      "step 84680: train loss 2.4318, val loss 2.3028\n",
      "step 84690: train loss 2.2889, val loss 2.3952\n",
      "step 84700: train loss 2.4744, val loss 2.3318\n",
      "step 84710: train loss 2.3715, val loss 2.3344\n",
      "step 84720: train loss 2.3611, val loss 2.4046\n",
      "step 84730: train loss 2.3475, val loss 2.3275\n",
      "step 84740: train loss 2.3372, val loss 2.3787\n",
      "step 84750: train loss 2.4348, val loss 2.4587\n",
      "step 84760: train loss 2.3392, val loss 2.3778\n",
      "step 84770: train loss 2.3698, val loss 2.3659\n",
      "step 84780: train loss 2.3664, val loss 2.3640\n",
      "step 84790: train loss 2.3772, val loss 2.3746\n",
      "step 84800: train loss 2.4025, val loss 2.4094\n",
      "step 84810: train loss 2.3821, val loss 2.3177\n",
      "step 84820: train loss 2.3319, val loss 2.3701\n",
      "step 84830: train loss 2.4236, val loss 2.3161\n",
      "step 84840: train loss 2.3529, val loss 2.3667\n",
      "step 84850: train loss 2.3155, val loss 2.3604\n",
      "step 84860: train loss 2.2964, val loss 2.3333\n",
      "step 84870: train loss 2.4729, val loss 2.3798\n",
      "step 84880: train loss 2.4279, val loss 2.3485\n",
      "step 84890: train loss 2.3088, val loss 2.3272\n",
      "step 84900: train loss 2.3637, val loss 2.3873\n",
      "step 84910: train loss 2.2881, val loss 2.3686\n",
      "step 84920: train loss 2.3984, val loss 2.4266\n",
      "step 84930: train loss 2.3492, val loss 2.4359\n",
      "step 84940: train loss 2.3885, val loss 2.3247\n",
      "step 84950: train loss 2.4165, val loss 2.3889\n",
      "step 84960: train loss 2.3511, val loss 2.3448\n",
      "step 84970: train loss 2.3225, val loss 2.3438\n",
      "step 84980: train loss 2.3701, val loss 2.2987\n",
      "step 84990: train loss 2.4269, val loss 2.3996\n",
      "step 85000: train loss 2.2932, val loss 2.4022\n",
      "step 85010: train loss 2.3167, val loss 2.3454\n",
      "step 85020: train loss 2.3508, val loss 2.3644\n",
      "step 85030: train loss 2.3620, val loss 2.3160\n",
      "step 85040: train loss 2.4269, val loss 2.4024\n",
      "step 85050: train loss 2.3143, val loss 2.3778\n",
      "step 85060: train loss 2.4050, val loss 2.3470\n",
      "step 85070: train loss 2.3911, val loss 2.4660\n",
      "step 85080: train loss 2.4241, val loss 2.2612\n",
      "step 85090: train loss 2.4324, val loss 2.4154\n",
      "step 85100: train loss 2.4307, val loss 2.4207\n",
      "step 85110: train loss 2.3526, val loss 2.3683\n",
      "step 85120: train loss 2.3941, val loss 2.3814\n",
      "step 85130: train loss 2.3634, val loss 2.3837\n",
      "step 85140: train loss 2.4039, val loss 2.3802\n",
      "step 85150: train loss 2.3921, val loss 2.3902\n",
      "step 85160: train loss 2.3758, val loss 2.3729\n",
      "step 85170: train loss 2.4039, val loss 2.3616\n",
      "step 85180: train loss 2.3415, val loss 2.3759\n",
      "step 85190: train loss 2.3888, val loss 2.3570\n",
      "step 85200: train loss 2.4234, val loss 2.3884\n",
      "step 85210: train loss 2.2913, val loss 2.3518\n",
      "step 85220: train loss 2.3711, val loss 2.4053\n",
      "step 85230: train loss 2.4156, val loss 2.3381\n",
      "step 85240: train loss 2.3319, val loss 2.3021\n",
      "step 85250: train loss 2.3039, val loss 2.4359\n",
      "step 85260: train loss 2.3246, val loss 2.3736\n",
      "step 85270: train loss 2.3347, val loss 2.3510\n",
      "step 85280: train loss 2.3468, val loss 2.3884\n",
      "step 85290: train loss 2.3035, val loss 2.3598\n",
      "step 85300: train loss 2.4132, val loss 2.4165\n",
      "step 85310: train loss 2.3375, val loss 2.4434\n",
      "step 85320: train loss 2.4363, val loss 2.3612\n",
      "step 85330: train loss 2.3820, val loss 2.2611\n",
      "step 85340: train loss 2.4229, val loss 2.3264\n",
      "step 85350: train loss 2.3742, val loss 2.4004\n",
      "step 85360: train loss 2.3751, val loss 2.4210\n",
      "step 85370: train loss 2.3515, val loss 2.3149\n",
      "step 85380: train loss 2.4578, val loss 2.3785\n",
      "step 85390: train loss 2.3962, val loss 2.4663\n",
      "step 85400: train loss 2.3896, val loss 2.3990\n",
      "step 85410: train loss 2.4279, val loss 2.3104\n",
      "step 85420: train loss 2.3355, val loss 2.3106\n",
      "step 85430: train loss 2.4540, val loss 2.4117\n",
      "step 85440: train loss 2.3389, val loss 2.4153\n",
      "step 85450: train loss 2.3247, val loss 2.3590\n",
      "step 85460: train loss 2.3244, val loss 2.3374\n",
      "step 85470: train loss 2.4130, val loss 2.2688\n",
      "step 85480: train loss 2.3539, val loss 2.3141\n",
      "step 85490: train loss 2.3640, val loss 2.3510\n",
      "step 85500: train loss 2.4119, val loss 2.3817\n",
      "step 85510: train loss 2.3521, val loss 2.3933\n",
      "step 85520: train loss 2.3753, val loss 2.2845\n",
      "step 85530: train loss 2.3614, val loss 2.3884\n",
      "step 85540: train loss 2.4310, val loss 2.2596\n",
      "step 85550: train loss 2.3670, val loss 2.3599\n",
      "step 85560: train loss 2.3681, val loss 2.3952\n",
      "step 85570: train loss 2.3585, val loss 2.3002\n",
      "step 85580: train loss 2.4147, val loss 2.3982\n",
      "step 85590: train loss 2.4214, val loss 2.3569\n",
      "step 85600: train loss 2.3893, val loss 2.4342\n",
      "step 85610: train loss 2.4006, val loss 2.3592\n",
      "step 85620: train loss 2.4399, val loss 2.3655\n",
      "step 85630: train loss 2.4092, val loss 2.3857\n",
      "step 85640: train loss 2.3289, val loss 2.3970\n",
      "step 85650: train loss 2.3789, val loss 2.4464\n",
      "step 85660: train loss 2.3535, val loss 2.3023\n",
      "step 85670: train loss 2.3559, val loss 2.3857\n",
      "step 85680: train loss 2.3960, val loss 2.3973\n",
      "step 85690: train loss 2.4075, val loss 2.4090\n",
      "step 85700: train loss 2.4002, val loss 2.3332\n",
      "step 85710: train loss 2.3978, val loss 2.3590\n",
      "step 85720: train loss 2.3769, val loss 2.3583\n",
      "step 85730: train loss 2.3558, val loss 2.3270\n",
      "step 85740: train loss 2.3820, val loss 2.4280\n",
      "step 85750: train loss 2.4186, val loss 2.3592\n",
      "step 85760: train loss 2.3912, val loss 2.4088\n",
      "step 85770: train loss 2.3451, val loss 2.3599\n",
      "step 85780: train loss 2.4418, val loss 2.3661\n",
      "step 85790: train loss 2.3680, val loss 2.4653\n",
      "step 85800: train loss 2.4251, val loss 2.4998\n",
      "step 85810: train loss 2.3913, val loss 2.3840\n",
      "step 85820: train loss 2.3552, val loss 2.3875\n",
      "step 85830: train loss 2.3597, val loss 2.3314\n",
      "step 85840: train loss 2.2912, val loss 2.3692\n",
      "step 85850: train loss 2.4236, val loss 2.3339\n",
      "step 85860: train loss 2.4539, val loss 2.3654\n",
      "step 85870: train loss 2.4417, val loss 2.4110\n",
      "step 85880: train loss 2.3775, val loss 2.3851\n",
      "step 85890: train loss 2.4149, val loss 2.3050\n",
      "step 85900: train loss 2.3757, val loss 2.3118\n",
      "step 85910: train loss 2.3540, val loss 2.3970\n",
      "step 85920: train loss 2.3679, val loss 2.4379\n",
      "step 85930: train loss 2.3848, val loss 2.3702\n",
      "step 85940: train loss 2.3444, val loss 2.3766\n",
      "step 85950: train loss 2.2885, val loss 2.3797\n",
      "step 85960: train loss 2.3754, val loss 2.3305\n",
      "step 85970: train loss 2.4369, val loss 2.3868\n",
      "step 85980: train loss 2.3544, val loss 2.3256\n",
      "step 85990: train loss 2.3167, val loss 2.3491\n",
      "step 86000: train loss 2.3737, val loss 2.3877\n",
      "step 86010: train loss 2.3241, val loss 2.3399\n",
      "step 86020: train loss 2.3547, val loss 2.3614\n",
      "step 86030: train loss 2.3075, val loss 2.4030\n",
      "step 86040: train loss 2.3434, val loss 2.3068\n",
      "step 86050: train loss 2.3505, val loss 2.3059\n",
      "step 86060: train loss 2.3092, val loss 2.2989\n",
      "step 86070: train loss 2.4135, val loss 2.3392\n",
      "step 86080: train loss 2.4012, val loss 2.3646\n",
      "step 86090: train loss 2.3380, val loss 2.3203\n",
      "step 86100: train loss 2.3591, val loss 2.3769\n",
      "step 86110: train loss 2.3062, val loss 2.4012\n",
      "step 86120: train loss 2.4491, val loss 2.3723\n",
      "step 86130: train loss 2.3823, val loss 2.4444\n",
      "step 86140: train loss 2.3987, val loss 2.4392\n",
      "step 86150: train loss 2.3683, val loss 2.4299\n",
      "step 86160: train loss 2.3862, val loss 2.3862\n",
      "step 86170: train loss 2.3396, val loss 2.3849\n",
      "step 86180: train loss 2.2766, val loss 2.3951\n",
      "step 86190: train loss 2.4253, val loss 2.4174\n",
      "step 86200: train loss 2.4266, val loss 2.3346\n",
      "step 86210: train loss 2.3928, val loss 2.3224\n",
      "step 86220: train loss 2.3666, val loss 2.3103\n",
      "step 86230: train loss 2.4028, val loss 2.3917\n",
      "step 86240: train loss 2.3746, val loss 2.4450\n",
      "step 86250: train loss 2.3162, val loss 2.3232\n",
      "step 86260: train loss 2.4246, val loss 2.3855\n",
      "step 86270: train loss 2.3890, val loss 2.3770\n",
      "step 86280: train loss 2.3731, val loss 2.3428\n",
      "step 86290: train loss 2.3372, val loss 2.3445\n",
      "step 86300: train loss 2.3937, val loss 2.3849\n",
      "step 86310: train loss 2.4626, val loss 2.3313\n",
      "step 86320: train loss 2.3068, val loss 2.3470\n",
      "step 86330: train loss 2.3562, val loss 2.4020\n",
      "step 86340: train loss 2.3806, val loss 2.4009\n",
      "step 86350: train loss 2.4037, val loss 2.3672\n",
      "step 86360: train loss 2.3427, val loss 2.4101\n",
      "step 86370: train loss 2.4305, val loss 2.3879\n",
      "step 86380: train loss 2.3920, val loss 2.3558\n",
      "step 86390: train loss 2.3881, val loss 2.3717\n",
      "step 86400: train loss 2.3826, val loss 2.3920\n",
      "step 86410: train loss 2.3055, val loss 2.3682\n",
      "step 86420: train loss 2.3953, val loss 2.3731\n",
      "step 86430: train loss 2.3588, val loss 2.3411\n",
      "step 86440: train loss 2.4209, val loss 2.3671\n",
      "step 86450: train loss 2.3890, val loss 2.3221\n",
      "step 86460: train loss 2.3846, val loss 2.3108\n",
      "step 86470: train loss 2.3667, val loss 2.4036\n",
      "step 86480: train loss 2.4497, val loss 2.3804\n",
      "step 86490: train loss 2.3374, val loss 2.3879\n",
      "step 86500: train loss 2.3425, val loss 2.3163\n",
      "step 86510: train loss 2.4079, val loss 2.4088\n",
      "step 86520: train loss 2.3776, val loss 2.4284\n",
      "step 86530: train loss 2.3930, val loss 2.3474\n",
      "step 86540: train loss 2.3711, val loss 2.3612\n",
      "step 86550: train loss 2.4770, val loss 2.3079\n",
      "step 86560: train loss 2.4158, val loss 2.3846\n",
      "step 86570: train loss 2.4251, val loss 2.3445\n",
      "step 86580: train loss 2.3256, val loss 2.3438\n",
      "step 86590: train loss 2.3281, val loss 2.3554\n",
      "step 86600: train loss 2.4019, val loss 2.3695\n",
      "step 86610: train loss 2.4086, val loss 2.3781\n",
      "step 86620: train loss 2.3622, val loss 2.3740\n",
      "step 86630: train loss 2.4244, val loss 2.4024\n",
      "step 86640: train loss 2.3672, val loss 2.3982\n",
      "step 86650: train loss 2.4013, val loss 2.3889\n",
      "step 86660: train loss 2.3244, val loss 2.3433\n",
      "step 86670: train loss 2.3298, val loss 2.3572\n",
      "step 86680: train loss 2.4147, val loss 2.3750\n",
      "step 86690: train loss 2.3856, val loss 2.3795\n",
      "step 86700: train loss 2.4372, val loss 2.4454\n",
      "step 86710: train loss 2.4019, val loss 2.3136\n",
      "step 86720: train loss 2.3992, val loss 2.3513\n",
      "step 86730: train loss 2.3946, val loss 2.3192\n",
      "step 86740: train loss 2.3504, val loss 2.4116\n",
      "step 86750: train loss 2.3736, val loss 2.3582\n",
      "step 86760: train loss 2.4110, val loss 2.3839\n",
      "step 86770: train loss 2.4494, val loss 2.4322\n",
      "step 86780: train loss 2.3701, val loss 2.3742\n",
      "step 86790: train loss 2.3709, val loss 2.3158\n",
      "step 86800: train loss 2.4042, val loss 2.2778\n",
      "step 86810: train loss 2.3086, val loss 2.3676\n",
      "step 86820: train loss 2.3826, val loss 2.4813\n",
      "step 86830: train loss 2.4085, val loss 2.3520\n",
      "step 86840: train loss 2.3579, val loss 2.3533\n",
      "step 86850: train loss 2.3949, val loss 2.3315\n",
      "step 86860: train loss 2.3942, val loss 2.3885\n",
      "step 86870: train loss 2.3744, val loss 2.2701\n",
      "step 86880: train loss 2.4230, val loss 2.3550\n",
      "step 86890: train loss 2.3522, val loss 2.3490\n",
      "step 86900: train loss 2.3231, val loss 2.4013\n",
      "step 86910: train loss 2.3197, val loss 2.3659\n",
      "step 86920: train loss 2.3704, val loss 2.4423\n",
      "step 86930: train loss 2.3065, val loss 2.3429\n",
      "step 86940: train loss 2.3725, val loss 2.3954\n",
      "step 86950: train loss 2.4783, val loss 2.4159\n",
      "step 86960: train loss 2.4151, val loss 2.4060\n",
      "step 86970: train loss 2.2856, val loss 2.3626\n",
      "step 86980: train loss 2.3892, val loss 2.4792\n",
      "step 86990: train loss 2.4198, val loss 2.3614\n",
      "step 87000: train loss 2.3387, val loss 2.3424\n",
      "step 87010: train loss 2.4467, val loss 2.3215\n",
      "step 87020: train loss 2.3542, val loss 2.3422\n",
      "step 87030: train loss 2.3750, val loss 2.2986\n",
      "step 87040: train loss 2.4023, val loss 2.4064\n",
      "step 87050: train loss 2.3728, val loss 2.3743\n",
      "step 87060: train loss 2.4145, val loss 2.2852\n",
      "step 87070: train loss 2.3625, val loss 2.3397\n",
      "step 87080: train loss 2.3664, val loss 2.3967\n",
      "step 87090: train loss 2.4102, val loss 2.3785\n",
      "step 87100: train loss 2.4174, val loss 2.3875\n",
      "step 87110: train loss 2.3948, val loss 2.3737\n",
      "step 87120: train loss 2.4431, val loss 2.3710\n",
      "step 87130: train loss 2.3407, val loss 2.4098\n",
      "step 87140: train loss 2.3938, val loss 2.2587\n",
      "step 87150: train loss 2.3733, val loss 2.3432\n",
      "step 87160: train loss 2.3424, val loss 2.3709\n",
      "step 87170: train loss 2.3132, val loss 2.3867\n",
      "step 87180: train loss 2.3814, val loss 2.4968\n",
      "step 87190: train loss 2.4429, val loss 2.3860\n",
      "step 87200: train loss 2.3467, val loss 2.4204\n",
      "step 87210: train loss 2.3831, val loss 2.2916\n",
      "step 87220: train loss 2.4233, val loss 2.3199\n",
      "step 87230: train loss 2.3169, val loss 2.4381\n",
      "step 87240: train loss 2.4160, val loss 2.3511\n",
      "step 87250: train loss 2.4122, val loss 2.3588\n",
      "step 87260: train loss 2.3635, val loss 2.3297\n",
      "step 87270: train loss 2.4155, val loss 2.4053\n",
      "step 87280: train loss 2.3518, val loss 2.3369\n",
      "step 87290: train loss 2.3650, val loss 2.4037\n",
      "step 87300: train loss 2.4255, val loss 2.3066\n",
      "step 87310: train loss 2.4011, val loss 2.3265\n",
      "step 87320: train loss 2.4035, val loss 2.4179\n",
      "step 87330: train loss 2.3730, val loss 2.3996\n",
      "step 87340: train loss 2.4326, val loss 2.3496\n",
      "step 87350: train loss 2.3971, val loss 2.3768\n",
      "step 87360: train loss 2.3362, val loss 2.4038\n",
      "step 87370: train loss 2.3610, val loss 2.3655\n",
      "step 87380: train loss 2.4188, val loss 2.3972\n",
      "step 87390: train loss 2.3464, val loss 2.4064\n",
      "step 87400: train loss 2.3572, val loss 2.3373\n",
      "step 87410: train loss 2.4172, val loss 2.3844\n",
      "step 87420: train loss 2.3732, val loss 2.4787\n",
      "step 87430: train loss 2.3709, val loss 2.3812\n",
      "step 87440: train loss 2.3408, val loss 2.2921\n",
      "step 87450: train loss 2.4106, val loss 2.3249\n",
      "step 87460: train loss 2.3817, val loss 2.3604\n",
      "step 87470: train loss 2.3642, val loss 2.3320\n",
      "step 87480: train loss 2.3435, val loss 2.3599\n",
      "step 87490: train loss 2.3696, val loss 2.3545\n",
      "step 87500: train loss 2.4308, val loss 2.3853\n",
      "step 87510: train loss 2.3723, val loss 2.3771\n",
      "step 87520: train loss 2.3662, val loss 2.4054\n",
      "step 87530: train loss 2.3645, val loss 2.3697\n",
      "step 87540: train loss 2.3675, val loss 2.3429\n",
      "step 87550: train loss 2.3456, val loss 2.3737\n",
      "step 87560: train loss 2.4829, val loss 2.3692\n",
      "step 87570: train loss 2.3583, val loss 2.3097\n",
      "step 87580: train loss 2.4128, val loss 2.3908\n",
      "step 87590: train loss 2.3798, val loss 2.3599\n",
      "step 87600: train loss 2.3216, val loss 2.3131\n",
      "step 87610: train loss 2.3729, val loss 2.3708\n",
      "step 87620: train loss 2.3349, val loss 2.4030\n",
      "step 87630: train loss 2.3616, val loss 2.3733\n",
      "step 87640: train loss 2.3887, val loss 2.3750\n",
      "step 87650: train loss 2.3643, val loss 2.3429\n",
      "step 87660: train loss 2.4023, val loss 2.3864\n",
      "step 87670: train loss 2.4243, val loss 2.3563\n",
      "step 87680: train loss 2.3454, val loss 2.4087\n",
      "step 87690: train loss 2.3532, val loss 2.2987\n",
      "step 87700: train loss 2.3712, val loss 2.3735\n",
      "step 87710: train loss 2.3840, val loss 2.3997\n",
      "step 87720: train loss 2.3821, val loss 2.3969\n",
      "step 87730: train loss 2.3638, val loss 2.3952\n",
      "step 87740: train loss 2.3437, val loss 2.3035\n",
      "step 87750: train loss 2.3655, val loss 2.3657\n",
      "step 87760: train loss 2.3214, val loss 2.4254\n",
      "step 87770: train loss 2.3499, val loss 2.3779\n",
      "step 87780: train loss 2.3731, val loss 2.4074\n",
      "step 87790: train loss 2.3855, val loss 2.3176\n",
      "step 87800: train loss 2.4015, val loss 2.3179\n",
      "step 87810: train loss 2.4447, val loss 2.3771\n",
      "step 87820: train loss 2.4737, val loss 2.3526\n",
      "step 87830: train loss 2.4193, val loss 2.3602\n",
      "step 87840: train loss 2.3042, val loss 2.3108\n",
      "step 87850: train loss 2.4400, val loss 2.2927\n",
      "step 87860: train loss 2.3434, val loss 2.3556\n",
      "step 87870: train loss 2.4149, val loss 2.3695\n",
      "step 87880: train loss 2.4474, val loss 2.3301\n",
      "step 87890: train loss 2.4138, val loss 2.4324\n",
      "step 87900: train loss 2.2979, val loss 2.3511\n",
      "step 87910: train loss 2.3733, val loss 2.3280\n",
      "step 87920: train loss 2.3429, val loss 2.3938\n",
      "step 87930: train loss 2.3149, val loss 2.3612\n",
      "step 87940: train loss 2.3363, val loss 2.3851\n",
      "step 87950: train loss 2.3244, val loss 2.5168\n",
      "step 87960: train loss 2.3505, val loss 2.2918\n",
      "step 87970: train loss 2.3739, val loss 2.3280\n",
      "step 87980: train loss 2.4952, val loss 2.3828\n",
      "step 87990: train loss 2.3252, val loss 2.3784\n",
      "step 88000: train loss 2.4112, val loss 2.3812\n",
      "step 88010: train loss 2.3805, val loss 2.3069\n",
      "step 88020: train loss 2.3963, val loss 2.3666\n",
      "step 88030: train loss 2.3852, val loss 2.4115\n",
      "step 88040: train loss 2.3390, val loss 2.3348\n",
      "step 88050: train loss 2.3874, val loss 2.3087\n",
      "step 88060: train loss 2.4671, val loss 2.4431\n",
      "step 88070: train loss 2.3767, val loss 2.3025\n",
      "step 88080: train loss 2.3174, val loss 2.4104\n",
      "step 88090: train loss 2.3106, val loss 2.3040\n",
      "step 88100: train loss 2.3963, val loss 2.3480\n",
      "step 88110: train loss 2.4197, val loss 2.4068\n",
      "step 88120: train loss 2.3937, val loss 2.4014\n",
      "step 88130: train loss 2.3814, val loss 2.3988\n",
      "step 88140: train loss 2.4180, val loss 2.3496\n",
      "step 88150: train loss 2.3194, val loss 2.3815\n",
      "step 88160: train loss 2.3461, val loss 2.2911\n",
      "step 88170: train loss 2.2940, val loss 2.3087\n",
      "step 88180: train loss 2.4291, val loss 2.3329\n",
      "step 88190: train loss 2.4181, val loss 2.2569\n",
      "step 88200: train loss 2.3512, val loss 2.3873\n",
      "step 88210: train loss 2.3736, val loss 2.3554\n",
      "step 88220: train loss 2.3674, val loss 2.3042\n",
      "step 88230: train loss 2.3358, val loss 2.3985\n",
      "step 88240: train loss 2.3467, val loss 2.3951\n",
      "step 88250: train loss 2.3538, val loss 2.3946\n",
      "step 88260: train loss 2.3631, val loss 2.4992\n",
      "step 88270: train loss 2.3831, val loss 2.3571\n",
      "step 88280: train loss 2.3882, val loss 2.3415\n",
      "step 88290: train loss 2.3618, val loss 2.3705\n",
      "step 88300: train loss 2.3292, val loss 2.3482\n",
      "step 88310: train loss 2.3488, val loss 2.3466\n",
      "step 88320: train loss 2.4339, val loss 2.2715\n",
      "step 88330: train loss 2.3311, val loss 2.3208\n",
      "step 88340: train loss 2.3226, val loss 2.3811\n",
      "step 88350: train loss 2.3416, val loss 2.3742\n",
      "step 88360: train loss 2.3537, val loss 2.4192\n",
      "step 88370: train loss 2.3838, val loss 2.3669\n",
      "step 88380: train loss 2.3711, val loss 2.3154\n",
      "step 88390: train loss 2.3712, val loss 2.2860\n",
      "step 88400: train loss 2.3510, val loss 2.3214\n",
      "step 88410: train loss 2.3591, val loss 2.3770\n",
      "step 88420: train loss 2.3257, val loss 2.3389\n",
      "step 88430: train loss 2.3795, val loss 2.3281\n",
      "step 88440: train loss 2.3549, val loss 2.3228\n",
      "step 88450: train loss 2.3187, val loss 2.4116\n",
      "step 88460: train loss 2.3783, val loss 2.3569\n",
      "step 88470: train loss 2.3813, val loss 2.3943\n",
      "step 88480: train loss 2.3797, val loss 2.3550\n",
      "step 88490: train loss 2.3565, val loss 2.3591\n",
      "step 88500: train loss 2.4143, val loss 2.3598\n",
      "step 88510: train loss 2.4848, val loss 2.3896\n",
      "step 88520: train loss 2.3989, val loss 2.3681\n",
      "step 88530: train loss 2.3779, val loss 2.3124\n",
      "step 88540: train loss 2.4984, val loss 2.3853\n",
      "step 88550: train loss 2.3355, val loss 2.3239\n",
      "step 88560: train loss 2.4295, val loss 2.4043\n",
      "step 88570: train loss 2.3519, val loss 2.3844\n",
      "step 88580: train loss 2.4826, val loss 2.3753\n",
      "step 88590: train loss 2.4023, val loss 2.3528\n",
      "step 88600: train loss 2.4098, val loss 2.4044\n",
      "step 88610: train loss 2.3611, val loss 2.4737\n",
      "step 88620: train loss 2.4218, val loss 2.3806\n",
      "step 88630: train loss 2.3865, val loss 2.2911\n",
      "step 88640: train loss 2.3638, val loss 2.4077\n",
      "step 88650: train loss 2.3429, val loss 2.3162\n",
      "step 88660: train loss 2.4029, val loss 2.3781\n",
      "step 88670: train loss 2.3670, val loss 2.4265\n",
      "step 88680: train loss 2.4491, val loss 2.4035\n",
      "step 88690: train loss 2.3760, val loss 2.3064\n",
      "step 88700: train loss 2.3433, val loss 2.4561\n",
      "step 88710: train loss 2.3820, val loss 2.3574\n",
      "step 88720: train loss 2.4077, val loss 2.3793\n",
      "step 88730: train loss 2.3145, val loss 2.3589\n",
      "step 88740: train loss 2.4613, val loss 2.3578\n",
      "step 88750: train loss 2.3409, val loss 2.3672\n",
      "step 88760: train loss 2.4440, val loss 2.3515\n",
      "step 88770: train loss 2.3312, val loss 2.4613\n",
      "step 88780: train loss 2.3897, val loss 2.3811\n",
      "step 88790: train loss 2.3224, val loss 2.4037\n",
      "step 88800: train loss 2.3368, val loss 2.4403\n",
      "step 88810: train loss 2.3237, val loss 2.4094\n",
      "step 88820: train loss 2.4722, val loss 2.3228\n",
      "step 88830: train loss 2.3313, val loss 2.3034\n",
      "step 88840: train loss 2.3816, val loss 2.2961\n",
      "step 88850: train loss 2.3040, val loss 2.3602\n",
      "step 88860: train loss 2.2689, val loss 2.3684\n",
      "step 88870: train loss 2.3499, val loss 2.4179\n",
      "step 88880: train loss 2.4741, val loss 2.3577\n",
      "step 88890: train loss 2.4104, val loss 2.4322\n",
      "step 88900: train loss 2.3714, val loss 2.3960\n",
      "step 88910: train loss 2.4094, val loss 2.3614\n",
      "step 88920: train loss 2.4491, val loss 2.3456\n",
      "step 88930: train loss 2.4502, val loss 2.3042\n",
      "step 88940: train loss 2.3498, val loss 2.4663\n",
      "step 88950: train loss 2.3749, val loss 2.3754\n",
      "step 88960: train loss 2.3871, val loss 2.3412\n",
      "step 88970: train loss 2.3539, val loss 2.3768\n",
      "step 88980: train loss 2.4139, val loss 2.3486\n",
      "step 88990: train loss 2.3797, val loss 2.3900\n",
      "step 89000: train loss 2.4119, val loss 2.3336\n",
      "step 89010: train loss 2.3690, val loss 2.2915\n",
      "step 89020: train loss 2.3550, val loss 2.3558\n",
      "step 89030: train loss 2.3510, val loss 2.2916\n",
      "step 89040: train loss 2.3263, val loss 2.4065\n",
      "step 89050: train loss 2.3891, val loss 2.3342\n",
      "step 89060: train loss 2.4219, val loss 2.4034\n",
      "step 89070: train loss 2.4003, val loss 2.3762\n",
      "step 89080: train loss 2.3577, val loss 2.3264\n",
      "step 89090: train loss 2.3331, val loss 2.3591\n",
      "step 89100: train loss 2.4288, val loss 2.4188\n",
      "step 89110: train loss 2.3467, val loss 2.3463\n",
      "step 89120: train loss 2.4364, val loss 2.3719\n",
      "step 89130: train loss 2.3086, val loss 2.3446\n",
      "step 89140: train loss 2.4669, val loss 2.3159\n",
      "step 89150: train loss 2.4332, val loss 2.3292\n",
      "step 89160: train loss 2.3326, val loss 2.3583\n",
      "step 89170: train loss 2.3543, val loss 2.3572\n",
      "step 89180: train loss 2.3747, val loss 2.3026\n",
      "step 89190: train loss 2.3838, val loss 2.3731\n",
      "step 89200: train loss 2.3415, val loss 2.3907\n",
      "step 89210: train loss 2.3789, val loss 2.3639\n",
      "step 89220: train loss 2.3918, val loss 2.3345\n",
      "step 89230: train loss 2.3820, val loss 2.4488\n",
      "step 89240: train loss 2.3510, val loss 2.4110\n",
      "step 89250: train loss 2.4064, val loss 2.4531\n",
      "step 89260: train loss 2.3794, val loss 2.3558\n",
      "step 89270: train loss 2.3575, val loss 2.3848\n",
      "step 89280: train loss 2.3562, val loss 2.4116\n",
      "step 89290: train loss 2.2994, val loss 2.4003\n",
      "step 89300: train loss 2.4029, val loss 2.3636\n",
      "step 89310: train loss 2.3500, val loss 2.3975\n",
      "step 89320: train loss 2.3513, val loss 2.3342\n",
      "step 89330: train loss 2.3147, val loss 2.3871\n",
      "step 89340: train loss 2.3445, val loss 2.3015\n",
      "step 89350: train loss 2.4112, val loss 2.3788\n",
      "step 89360: train loss 2.4878, val loss 2.3686\n",
      "step 89370: train loss 2.4185, val loss 2.3514\n",
      "step 89380: train loss 2.3939, val loss 2.4618\n",
      "step 89390: train loss 2.3200, val loss 2.3271\n",
      "step 89400: train loss 2.4705, val loss 2.4797\n",
      "step 89410: train loss 2.4185, val loss 2.3931\n",
      "step 89420: train loss 2.4029, val loss 2.3695\n",
      "step 89430: train loss 2.3364, val loss 2.3661\n",
      "step 89440: train loss 2.3572, val loss 2.3551\n",
      "step 89450: train loss 2.3586, val loss 2.3525\n",
      "step 89460: train loss 2.3745, val loss 2.4307\n",
      "step 89470: train loss 2.3783, val loss 2.4216\n",
      "step 89480: train loss 2.4279, val loss 2.2590\n",
      "step 89490: train loss 2.3881, val loss 2.4063\n",
      "step 89500: train loss 2.3510, val loss 2.3404\n",
      "step 89510: train loss 2.4285, val loss 2.3983\n",
      "step 89520: train loss 2.2855, val loss 2.4127\n",
      "step 89530: train loss 2.4066, val loss 2.4920\n",
      "step 89540: train loss 2.3312, val loss 2.3874\n",
      "step 89550: train loss 2.4315, val loss 2.4035\n",
      "step 89560: train loss 2.3645, val loss 2.3825\n",
      "step 89570: train loss 2.2923, val loss 2.3659\n",
      "step 89580: train loss 2.3457, val loss 2.3017\n",
      "step 89590: train loss 2.4094, val loss 2.3502\n",
      "step 89600: train loss 2.3629, val loss 2.3569\n",
      "step 89610: train loss 2.4397, val loss 2.3802\n",
      "step 89620: train loss 2.3475, val loss 2.4415\n",
      "step 89630: train loss 2.3623, val loss 2.4505\n",
      "step 89640: train loss 2.4196, val loss 2.3845\n",
      "step 89650: train loss 2.3057, val loss 2.2775\n",
      "step 89660: train loss 2.3753, val loss 2.3704\n",
      "step 89670: train loss 2.3106, val loss 2.4642\n",
      "step 89680: train loss 2.3803, val loss 2.3551\n",
      "step 89690: train loss 2.3860, val loss 2.4232\n",
      "step 89700: train loss 2.3654, val loss 2.3785\n",
      "step 89710: train loss 2.3305, val loss 2.3614\n",
      "step 89720: train loss 2.4418, val loss 2.4165\n",
      "step 89730: train loss 2.4160, val loss 2.4301\n",
      "step 89740: train loss 2.3976, val loss 2.4376\n",
      "step 89750: train loss 2.3483, val loss 2.3703\n",
      "step 89760: train loss 2.3399, val loss 2.3531\n",
      "step 89770: train loss 2.3774, val loss 2.3904\n",
      "step 89780: train loss 2.3540, val loss 2.3257\n",
      "step 89790: train loss 2.4348, val loss 2.3244\n",
      "step 89800: train loss 2.4111, val loss 2.3593\n",
      "step 89810: train loss 2.3316, val loss 2.4298\n",
      "step 89820: train loss 2.3310, val loss 2.3404\n",
      "step 89830: train loss 2.4239, val loss 2.3587\n",
      "step 89840: train loss 2.4112, val loss 2.5025\n",
      "step 89850: train loss 2.3774, val loss 2.4371\n",
      "step 89860: train loss 2.4273, val loss 2.3598\n",
      "step 89870: train loss 2.3631, val loss 2.3730\n",
      "step 89880: train loss 2.4570, val loss 2.3926\n",
      "step 89890: train loss 2.3734, val loss 2.3067\n",
      "step 89900: train loss 2.3860, val loss 2.3503\n",
      "step 89910: train loss 2.3284, val loss 2.3698\n",
      "step 89920: train loss 2.3547, val loss 2.4001\n",
      "step 89930: train loss 2.4116, val loss 2.3521\n",
      "step 89940: train loss 2.3624, val loss 2.4020\n",
      "step 89950: train loss 2.3682, val loss 2.3986\n",
      "step 89960: train loss 2.3540, val loss 2.3852\n",
      "step 89970: train loss 2.2976, val loss 2.3385\n",
      "step 89980: train loss 2.3323, val loss 2.3242\n",
      "step 89990: train loss 2.4791, val loss 2.3215\n",
      "step 90000: train loss 2.4727, val loss 2.3328\n",
      "step 90010: train loss 2.3442, val loss 2.3128\n",
      "step 90020: train loss 2.3790, val loss 2.4129\n",
      "step 90030: train loss 2.3541, val loss 2.4082\n",
      "step 90040: train loss 2.3577, val loss 2.3818\n",
      "step 90050: train loss 2.3701, val loss 2.3090\n",
      "step 90060: train loss 2.3933, val loss 2.3735\n",
      "step 90070: train loss 2.3670, val loss 2.3049\n",
      "step 90080: train loss 2.4184, val loss 2.3333\n",
      "step 90090: train loss 2.4115, val loss 2.3204\n",
      "step 90100: train loss 2.3806, val loss 2.3883\n",
      "step 90110: train loss 2.3885, val loss 2.4277\n",
      "step 90120: train loss 2.4678, val loss 2.3448\n",
      "step 90130: train loss 2.4874, val loss 2.3545\n",
      "step 90140: train loss 2.2679, val loss 2.3383\n",
      "step 90150: train loss 2.4147, val loss 2.4059\n",
      "step 90160: train loss 2.4085, val loss 2.3711\n",
      "step 90170: train loss 2.3633, val loss 2.3332\n",
      "step 90180: train loss 2.3923, val loss 2.4065\n",
      "step 90190: train loss 2.3558, val loss 2.3605\n",
      "step 90200: train loss 2.3630, val loss 2.4007\n",
      "step 90210: train loss 2.3453, val loss 2.3639\n",
      "step 90220: train loss 2.4174, val loss 2.3271\n",
      "step 90230: train loss 2.4115, val loss 2.3084\n",
      "step 90240: train loss 2.4014, val loss 2.3519\n",
      "step 90250: train loss 2.3391, val loss 2.3728\n",
      "step 90260: train loss 2.3860, val loss 2.3634\n",
      "step 90270: train loss 2.4188, val loss 2.3710\n",
      "step 90280: train loss 2.3581, val loss 2.4700\n",
      "step 90290: train loss 2.3922, val loss 2.3412\n",
      "step 90300: train loss 2.3901, val loss 2.3984\n",
      "step 90310: train loss 2.3486, val loss 2.4205\n",
      "step 90320: train loss 2.3839, val loss 2.3254\n",
      "step 90330: train loss 2.3932, val loss 2.3813\n",
      "step 90340: train loss 2.3916, val loss 2.3250\n",
      "step 90350: train loss 2.4124, val loss 2.3294\n",
      "step 90360: train loss 2.3527, val loss 2.3928\n",
      "step 90370: train loss 2.4594, val loss 2.3841\n",
      "step 90380: train loss 2.4269, val loss 2.3312\n",
      "step 90390: train loss 2.4180, val loss 2.4335\n",
      "step 90400: train loss 2.4797, val loss 2.3964\n",
      "step 90410: train loss 2.3723, val loss 2.3859\n",
      "step 90420: train loss 2.3464, val loss 2.3623\n",
      "step 90430: train loss 2.3621, val loss 2.4518\n",
      "step 90440: train loss 2.3900, val loss 2.3758\n",
      "step 90450: train loss 2.4087, val loss 2.3123\n",
      "step 90460: train loss 2.3631, val loss 2.2984\n",
      "step 90470: train loss 2.4140, val loss 2.3859\n",
      "step 90480: train loss 2.3450, val loss 2.3655\n",
      "step 90490: train loss 2.3864, val loss 2.4260\n",
      "step 90500: train loss 2.3846, val loss 2.3791\n",
      "step 90510: train loss 2.4490, val loss 2.3748\n",
      "step 90520: train loss 2.4109, val loss 2.4479\n",
      "step 90530: train loss 2.3827, val loss 2.2918\n",
      "step 90540: train loss 2.3422, val loss 2.2699\n",
      "step 90550: train loss 2.3882, val loss 2.3087\n",
      "step 90560: train loss 2.3780, val loss 2.3776\n",
      "step 90570: train loss 2.3635, val loss 2.3767\n",
      "step 90580: train loss 2.3999, val loss 2.3580\n",
      "step 90590: train loss 2.3557, val loss 2.3850\n",
      "step 90600: train loss 2.3874, val loss 2.3055\n",
      "step 90610: train loss 2.4195, val loss 2.4343\n",
      "step 90620: train loss 2.3462, val loss 2.3621\n",
      "step 90630: train loss 2.3597, val loss 2.4708\n",
      "step 90640: train loss 2.3773, val loss 2.3412\n",
      "step 90650: train loss 2.4387, val loss 2.3348\n",
      "step 90660: train loss 2.3495, val loss 2.4311\n",
      "step 90670: train loss 2.4014, val loss 2.3661\n",
      "step 90680: train loss 2.3520, val loss 2.3569\n",
      "step 90690: train loss 2.3586, val loss 2.4342\n",
      "step 90700: train loss 2.3889, val loss 2.3822\n",
      "step 90710: train loss 2.3881, val loss 2.3206\n",
      "step 90720: train loss 2.4134, val loss 2.4005\n",
      "step 90730: train loss 2.4460, val loss 2.4246\n",
      "step 90740: train loss 2.4090, val loss 2.4117\n",
      "step 90750: train loss 2.3130, val loss 2.2986\n",
      "step 90760: train loss 2.4056, val loss 2.3334\n",
      "step 90770: train loss 2.3598, val loss 2.4052\n",
      "step 90780: train loss 2.4526, val loss 2.3857\n",
      "step 90790: train loss 2.4696, val loss 2.3732\n",
      "step 90800: train loss 2.3932, val loss 2.3231\n",
      "step 90810: train loss 2.3787, val loss 2.3993\n",
      "step 90820: train loss 2.4216, val loss 2.3830\n",
      "step 90830: train loss 2.3417, val loss 2.3787\n",
      "step 90840: train loss 2.3196, val loss 2.4366\n",
      "step 90850: train loss 2.4379, val loss 2.3331\n",
      "step 90860: train loss 2.3688, val loss 2.3770\n",
      "step 90870: train loss 2.3983, val loss 2.4343\n",
      "step 90880: train loss 2.4187, val loss 2.3704\n",
      "step 90890: train loss 2.4254, val loss 2.4252\n",
      "step 90900: train loss 2.4674, val loss 2.3584\n",
      "step 90910: train loss 2.3016, val loss 2.3242\n",
      "step 90920: train loss 2.3797, val loss 2.3643\n",
      "step 90930: train loss 2.4183, val loss 2.3695\n",
      "step 90940: train loss 2.4253, val loss 2.3566\n",
      "step 90950: train loss 2.3722, val loss 2.3062\n",
      "step 90960: train loss 2.4137, val loss 2.3856\n",
      "step 90970: train loss 2.4299, val loss 2.3223\n",
      "step 90980: train loss 2.3356, val loss 2.4347\n",
      "step 90990: train loss 2.3668, val loss 2.3573\n",
      "step 91000: train loss 2.3805, val loss 2.3959\n",
      "step 91010: train loss 2.3711, val loss 2.3224\n",
      "step 91020: train loss 2.4321, val loss 2.3122\n",
      "step 91030: train loss 2.3556, val loss 2.3446\n",
      "step 91040: train loss 2.4244, val loss 2.3719\n",
      "step 91050: train loss 2.3186, val loss 2.3294\n",
      "step 91060: train loss 2.3892, val loss 2.3774\n",
      "step 91070: train loss 2.3583, val loss 2.3674\n",
      "step 91080: train loss 2.4051, val loss 2.4214\n",
      "step 91090: train loss 2.4474, val loss 2.3528\n",
      "step 91100: train loss 2.4151, val loss 2.3178\n",
      "step 91110: train loss 2.4088, val loss 2.3038\n",
      "step 91120: train loss 2.3387, val loss 2.3052\n",
      "step 91130: train loss 2.4009, val loss 2.3514\n",
      "step 91140: train loss 2.4059, val loss 2.3871\n",
      "step 91150: train loss 2.3939, val loss 2.4367\n",
      "step 91160: train loss 2.3831, val loss 2.4417\n",
      "step 91170: train loss 2.3520, val loss 2.4232\n",
      "step 91180: train loss 2.4255, val loss 2.3034\n",
      "step 91190: train loss 2.4259, val loss 2.3076\n",
      "step 91200: train loss 2.3888, val loss 2.3421\n",
      "step 91210: train loss 2.3434, val loss 2.4196\n",
      "step 91220: train loss 2.4216, val loss 2.3394\n",
      "step 91230: train loss 2.4043, val loss 2.3640\n",
      "step 91240: train loss 2.4767, val loss 2.3290\n",
      "step 91250: train loss 2.3885, val loss 2.3886\n",
      "step 91260: train loss 2.3270, val loss 2.3217\n",
      "step 91270: train loss 2.3810, val loss 2.4477\n",
      "step 91280: train loss 2.4114, val loss 2.3314\n",
      "step 91290: train loss 2.5071, val loss 2.4086\n",
      "step 91300: train loss 2.3723, val loss 2.3319\n",
      "step 91310: train loss 2.4265, val loss 2.3195\n",
      "step 91320: train loss 2.4521, val loss 2.2418\n",
      "step 91330: train loss 2.3935, val loss 2.3438\n",
      "step 91340: train loss 2.3947, val loss 2.4441\n",
      "step 91350: train loss 2.3671, val loss 2.3178\n",
      "step 91360: train loss 2.5121, val loss 2.2827\n",
      "step 91370: train loss 2.4704, val loss 2.4020\n",
      "step 91380: train loss 2.3072, val loss 2.4428\n",
      "step 91390: train loss 2.3674, val loss 2.3613\n",
      "step 91400: train loss 2.4534, val loss 2.3448\n",
      "step 91410: train loss 2.3674, val loss 2.3277\n",
      "step 91420: train loss 2.3777, val loss 2.4155\n",
      "step 91430: train loss 2.3611, val loss 2.3453\n",
      "step 91440: train loss 2.3003, val loss 2.3009\n",
      "step 91450: train loss 2.3385, val loss 2.4200\n",
      "step 91460: train loss 2.3464, val loss 2.3038\n",
      "step 91470: train loss 2.3758, val loss 2.3626\n",
      "step 91480: train loss 2.2796, val loss 2.3833\n",
      "step 91490: train loss 2.3871, val loss 2.3579\n",
      "step 91500: train loss 2.4561, val loss 2.3540\n",
      "step 91510: train loss 2.3555, val loss 2.3238\n",
      "step 91520: train loss 2.2788, val loss 2.4036\n",
      "step 91530: train loss 2.3939, val loss 2.3321\n",
      "step 91540: train loss 2.3853, val loss 2.3424\n",
      "step 91550: train loss 2.4096, val loss 2.3635\n",
      "step 91560: train loss 2.4053, val loss 2.3249\n",
      "step 91570: train loss 2.2919, val loss 2.3977\n",
      "step 91580: train loss 2.4167, val loss 2.3806\n",
      "step 91590: train loss 2.3560, val loss 2.3426\n",
      "step 91600: train loss 2.3251, val loss 2.3283\n",
      "step 91610: train loss 2.3857, val loss 2.4267\n",
      "step 91620: train loss 2.3443, val loss 2.3355\n",
      "step 91630: train loss 2.3816, val loss 2.3913\n",
      "step 91640: train loss 2.3444, val loss 2.3737\n",
      "step 91650: train loss 2.3957, val loss 2.4025\n",
      "step 91660: train loss 2.3295, val loss 2.3877\n",
      "step 91670: train loss 2.4730, val loss 2.4114\n",
      "step 91680: train loss 2.3550, val loss 2.4054\n",
      "step 91690: train loss 2.3259, val loss 2.4049\n",
      "step 91700: train loss 2.3948, val loss 2.3458\n",
      "step 91710: train loss 2.3828, val loss 2.3165\n",
      "step 91720: train loss 2.3851, val loss 2.4058\n",
      "step 91730: train loss 2.4166, val loss 2.3419\n",
      "step 91740: train loss 2.4229, val loss 2.3356\n",
      "step 91750: train loss 2.3492, val loss 2.4015\n",
      "step 91760: train loss 2.3972, val loss 2.3314\n",
      "step 91770: train loss 2.3600, val loss 2.3128\n",
      "step 91780: train loss 2.3920, val loss 2.4225\n",
      "step 91790: train loss 2.4858, val loss 2.3551\n",
      "step 91800: train loss 2.3301, val loss 2.4019\n",
      "step 91810: train loss 2.4220, val loss 2.3649\n",
      "step 91820: train loss 2.4752, val loss 2.3350\n",
      "step 91830: train loss 2.3151, val loss 2.4238\n",
      "step 91840: train loss 2.4054, val loss 2.3465\n",
      "step 91850: train loss 2.3368, val loss 2.3501\n",
      "step 91860: train loss 2.3036, val loss 2.4880\n",
      "step 91870: train loss 2.4082, val loss 2.3757\n",
      "step 91880: train loss 2.4395, val loss 2.4043\n",
      "step 91890: train loss 2.4210, val loss 2.3898\n",
      "step 91900: train loss 2.3799, val loss 2.4138\n",
      "step 91910: train loss 2.4097, val loss 2.4409\n",
      "step 91920: train loss 2.3380, val loss 2.3445\n",
      "step 91930: train loss 2.2623, val loss 2.3485\n",
      "step 91940: train loss 2.3562, val loss 2.3642\n",
      "step 91950: train loss 2.3755, val loss 2.3003\n",
      "step 91960: train loss 2.4316, val loss 2.3189\n",
      "step 91970: train loss 2.3643, val loss 2.4477\n",
      "step 91980: train loss 2.3781, val loss 2.3385\n",
      "step 91990: train loss 2.3991, val loss 2.3683\n",
      "step 92000: train loss 2.4494, val loss 2.3704\n",
      "step 92010: train loss 2.4048, val loss 2.3928\n",
      "step 92020: train loss 2.3382, val loss 2.4249\n",
      "step 92030: train loss 2.3638, val loss 2.4067\n",
      "step 92040: train loss 2.3644, val loss 2.3007\n",
      "step 92050: train loss 2.3683, val loss 2.3320\n",
      "step 92060: train loss 2.4174, val loss 2.3647\n",
      "step 92070: train loss 2.4408, val loss 2.3194\n",
      "step 92080: train loss 2.4344, val loss 2.3662\n",
      "step 92090: train loss 2.3651, val loss 2.3868\n",
      "step 92100: train loss 2.3689, val loss 2.3523\n",
      "step 92110: train loss 2.3706, val loss 2.2895\n",
      "step 92120: train loss 2.2365, val loss 2.4403\n",
      "step 92130: train loss 2.4727, val loss 2.4284\n",
      "step 92140: train loss 2.3178, val loss 2.4014\n",
      "step 92150: train loss 2.3457, val loss 2.3781\n",
      "step 92160: train loss 2.3410, val loss 2.3549\n",
      "step 92170: train loss 2.4024, val loss 2.3470\n",
      "step 92180: train loss 2.3614, val loss 2.3614\n",
      "step 92190: train loss 2.3858, val loss 2.3499\n",
      "step 92200: train loss 2.3740, val loss 2.3462\n",
      "step 92210: train loss 2.3547, val loss 2.3831\n",
      "step 92220: train loss 2.4367, val loss 2.3128\n",
      "step 92230: train loss 2.3676, val loss 2.4267\n",
      "step 92240: train loss 2.3327, val loss 2.4080\n",
      "step 92250: train loss 2.4519, val loss 2.3569\n",
      "step 92260: train loss 2.3254, val loss 2.4088\n",
      "step 92270: train loss 2.3906, val loss 2.3469\n",
      "step 92280: train loss 2.4185, val loss 2.3826\n",
      "step 92290: train loss 2.3867, val loss 2.3894\n",
      "step 92300: train loss 2.3700, val loss 2.4040\n",
      "step 92310: train loss 2.3919, val loss 2.3721\n",
      "step 92320: train loss 2.3510, val loss 2.3454\n",
      "step 92330: train loss 2.3702, val loss 2.4127\n",
      "step 92340: train loss 2.4270, val loss 2.3368\n",
      "step 92350: train loss 2.3817, val loss 2.3202\n",
      "step 92360: train loss 2.3461, val loss 2.3721\n",
      "step 92370: train loss 2.3693, val loss 2.3356\n",
      "step 92380: train loss 2.4078, val loss 2.3279\n",
      "step 92390: train loss 2.3961, val loss 2.2976\n",
      "step 92400: train loss 2.3370, val loss 2.4249\n",
      "step 92410: train loss 2.4343, val loss 2.3694\n",
      "step 92420: train loss 2.3480, val loss 2.4335\n",
      "step 92430: train loss 2.3362, val loss 2.2920\n",
      "step 92440: train loss 2.4551, val loss 2.3072\n",
      "step 92450: train loss 2.3831, val loss 2.3940\n",
      "step 92460: train loss 2.4362, val loss 2.3273\n",
      "step 92470: train loss 2.3410, val loss 2.2094\n",
      "step 92480: train loss 2.3711, val loss 2.3446\n",
      "step 92490: train loss 2.4151, val loss 2.3387\n",
      "step 92500: train loss 2.4377, val loss 2.3336\n",
      "step 92510: train loss 2.3959, val loss 2.3595\n",
      "step 92520: train loss 2.3763, val loss 2.3113\n",
      "step 92530: train loss 2.3809, val loss 2.3701\n",
      "step 92540: train loss 2.4610, val loss 2.4524\n",
      "step 92550: train loss 2.3893, val loss 2.5274\n",
      "step 92560: train loss 2.3061, val loss 2.4371\n",
      "step 92570: train loss 2.3452, val loss 2.4292\n",
      "step 92580: train loss 2.4368, val loss 2.4239\n",
      "step 92590: train loss 2.3173, val loss 2.3424\n",
      "step 92600: train loss 2.3824, val loss 2.3199\n",
      "step 92610: train loss 2.3373, val loss 2.4322\n",
      "step 92620: train loss 2.3893, val loss 2.3611\n",
      "step 92630: train loss 2.4352, val loss 2.3221\n",
      "step 92640: train loss 2.3571, val loss 2.3318\n",
      "step 92650: train loss 2.3851, val loss 2.4005\n",
      "step 92660: train loss 2.3478, val loss 2.4177\n",
      "step 92670: train loss 2.3671, val loss 2.3926\n",
      "step 92680: train loss 2.3369, val loss 2.4230\n",
      "step 92690: train loss 2.3696, val loss 2.3463\n",
      "step 92700: train loss 2.3929, val loss 2.4379\n",
      "step 92710: train loss 2.4033, val loss 2.3102\n",
      "step 92720: train loss 2.4248, val loss 2.3313\n",
      "step 92730: train loss 2.3913, val loss 2.3276\n",
      "step 92740: train loss 2.3399, val loss 2.4569\n",
      "step 92750: train loss 2.3608, val loss 2.4124\n",
      "step 92760: train loss 2.3106, val loss 2.3929\n",
      "step 92770: train loss 2.3924, val loss 2.4166\n",
      "step 92780: train loss 2.4351, val loss 2.3480\n",
      "step 92790: train loss 2.3592, val loss 2.3877\n",
      "step 92800: train loss 2.4107, val loss 2.3996\n",
      "step 92810: train loss 2.3731, val loss 2.3635\n",
      "step 92820: train loss 2.3984, val loss 2.4409\n",
      "step 92830: train loss 2.3514, val loss 2.3789\n",
      "step 92840: train loss 2.3799, val loss 2.4225\n",
      "step 92850: train loss 2.3154, val loss 2.4074\n",
      "step 92860: train loss 2.2942, val loss 2.4046\n",
      "step 92870: train loss 2.3692, val loss 2.3858\n",
      "step 92880: train loss 2.3708, val loss 2.3729\n",
      "step 92890: train loss 2.4037, val loss 2.3451\n",
      "step 92900: train loss 2.3666, val loss 2.3592\n",
      "step 92910: train loss 2.3878, val loss 2.3778\n",
      "step 92920: train loss 2.3192, val loss 2.3951\n",
      "step 92930: train loss 2.3718, val loss 2.2593\n",
      "step 92940: train loss 2.4578, val loss 2.3825\n",
      "step 92950: train loss 2.3535, val loss 2.3475\n",
      "step 92960: train loss 2.3779, val loss 2.3642\n",
      "step 92970: train loss 2.2957, val loss 2.3892\n",
      "step 92980: train loss 2.4043, val loss 2.3287\n",
      "step 92990: train loss 2.3453, val loss 2.4340\n",
      "step 93000: train loss 2.3853, val loss 2.3601\n",
      "step 93010: train loss 2.4130, val loss 2.3251\n",
      "step 93020: train loss 2.3595, val loss 2.3900\n",
      "step 93030: train loss 2.4421, val loss 2.3632\n",
      "step 93040: train loss 2.3256, val loss 2.3644\n",
      "step 93050: train loss 2.2965, val loss 2.3297\n",
      "step 93060: train loss 2.3799, val loss 2.4324\n",
      "step 93070: train loss 2.3576, val loss 2.3469\n",
      "step 93080: train loss 2.3563, val loss 2.4606\n",
      "step 93090: train loss 2.3311, val loss 2.4776\n",
      "step 93100: train loss 2.4151, val loss 2.3659\n",
      "step 93110: train loss 2.3919, val loss 2.3551\n",
      "step 93120: train loss 2.4401, val loss 2.3031\n",
      "step 93130: train loss 2.4469, val loss 2.4245\n",
      "step 93140: train loss 2.3556, val loss 2.4252\n",
      "step 93150: train loss 2.3761, val loss 2.3802\n",
      "step 93160: train loss 2.4261, val loss 2.3058\n",
      "step 93170: train loss 2.2851, val loss 2.3456\n",
      "step 93180: train loss 2.3440, val loss 2.4291\n",
      "step 93190: train loss 2.3535, val loss 2.3399\n",
      "step 93200: train loss 2.3475, val loss 2.4125\n",
      "step 93210: train loss 2.3232, val loss 2.3124\n",
      "step 93220: train loss 2.3698, val loss 2.4023\n",
      "step 93230: train loss 2.4107, val loss 2.3589\n",
      "step 93240: train loss 2.4237, val loss 2.4063\n",
      "step 93250: train loss 2.3185, val loss 2.3300\n",
      "step 93260: train loss 2.4133, val loss 2.2727\n",
      "step 93270: train loss 2.3744, val loss 2.3216\n",
      "step 93280: train loss 2.4219, val loss 2.3548\n",
      "step 93290: train loss 2.3637, val loss 2.3305\n",
      "step 93300: train loss 2.4340, val loss 2.2974\n",
      "step 93310: train loss 2.3154, val loss 2.4047\n",
      "step 93320: train loss 2.3381, val loss 2.3772\n",
      "step 93330: train loss 2.3779, val loss 2.3986\n",
      "step 93340: train loss 2.3309, val loss 2.3957\n",
      "step 93350: train loss 2.3732, val loss 2.3913\n",
      "step 93360: train loss 2.3855, val loss 2.3901\n",
      "step 93370: train loss 2.3428, val loss 2.3824\n",
      "step 93380: train loss 2.4064, val loss 2.3296\n",
      "step 93390: train loss 2.3993, val loss 2.4176\n",
      "step 93400: train loss 2.3034, val loss 2.3996\n",
      "step 93410: train loss 2.4123, val loss 2.4492\n",
      "step 93420: train loss 2.3837, val loss 2.3467\n",
      "step 93430: train loss 2.4635, val loss 2.4666\n",
      "step 93440: train loss 2.3566, val loss 2.2685\n",
      "step 93450: train loss 2.2961, val loss 2.3658\n",
      "step 93460: train loss 2.3190, val loss 2.3954\n",
      "step 93470: train loss 2.3510, val loss 2.4265\n",
      "step 93480: train loss 2.4707, val loss 2.3493\n",
      "step 93490: train loss 2.3341, val loss 2.3568\n",
      "step 93500: train loss 2.3633, val loss 2.3751\n",
      "step 93510: train loss 2.4227, val loss 2.2914\n",
      "step 93520: train loss 2.3772, val loss 2.3560\n",
      "step 93530: train loss 2.3398, val loss 2.3857\n",
      "step 93540: train loss 2.4500, val loss 2.3548\n",
      "step 93550: train loss 2.3909, val loss 2.4049\n",
      "step 93560: train loss 2.3033, val loss 2.3408\n",
      "step 93570: train loss 2.3253, val loss 2.3237\n",
      "step 93580: train loss 2.3606, val loss 2.3178\n",
      "step 93590: train loss 2.3983, val loss 2.3856\n",
      "step 93600: train loss 2.3293, val loss 2.3550\n",
      "step 93610: train loss 2.3833, val loss 2.2564\n",
      "step 93620: train loss 2.3011, val loss 2.3350\n",
      "step 93630: train loss 2.4123, val loss 2.3861\n",
      "step 93640: train loss 2.3675, val loss 2.4134\n",
      "step 93650: train loss 2.3177, val loss 2.3615\n",
      "step 93660: train loss 2.3621, val loss 2.2438\n",
      "step 93670: train loss 2.3847, val loss 2.3891\n",
      "step 93680: train loss 2.3491, val loss 2.3977\n",
      "step 93690: train loss 2.3407, val loss 2.3396\n",
      "step 93700: train loss 2.3688, val loss 2.3910\n",
      "step 93710: train loss 2.3697, val loss 2.4484\n",
      "step 93720: train loss 2.3119, val loss 2.3249\n",
      "step 93730: train loss 2.3866, val loss 2.3121\n",
      "step 93740: train loss 2.3985, val loss 2.4032\n",
      "step 93750: train loss 2.3567, val loss 2.3599\n",
      "step 93760: train loss 2.3861, val loss 2.4316\n",
      "step 93770: train loss 2.4039, val loss 2.3758\n",
      "step 93780: train loss 2.5105, val loss 2.3123\n",
      "step 93790: train loss 2.3615, val loss 2.3670\n",
      "step 93800: train loss 2.3320, val loss 2.3901\n",
      "step 93810: train loss 2.3883, val loss 2.3891\n",
      "step 93820: train loss 2.3750, val loss 2.3623\n",
      "step 93830: train loss 2.3850, val loss 2.3904\n",
      "step 93840: train loss 2.3266, val loss 2.3582\n",
      "step 93850: train loss 2.3913, val loss 2.3340\n",
      "step 93860: train loss 2.3392, val loss 2.3733\n",
      "step 93870: train loss 2.3675, val loss 2.3203\n",
      "step 93880: train loss 2.3060, val loss 2.3533\n",
      "step 93890: train loss 2.3624, val loss 2.4072\n",
      "step 93900: train loss 2.2894, val loss 2.2776\n",
      "step 93910: train loss 2.3831, val loss 2.3602\n",
      "step 93920: train loss 2.4071, val loss 2.3118\n",
      "step 93930: train loss 2.3204, val loss 2.3723\n",
      "step 93940: train loss 2.3773, val loss 2.3809\n",
      "step 93950: train loss 2.3656, val loss 2.4098\n",
      "step 93960: train loss 2.3689, val loss 2.3727\n",
      "step 93970: train loss 2.3580, val loss 2.3454\n",
      "step 93980: train loss 2.3542, val loss 2.3589\n",
      "step 93990: train loss 2.3670, val loss 2.3415\n",
      "step 94000: train loss 2.4111, val loss 2.3639\n",
      "step 94010: train loss 2.3311, val loss 2.3415\n",
      "step 94020: train loss 2.4831, val loss 2.3037\n",
      "step 94030: train loss 2.3009, val loss 2.4128\n",
      "step 94040: train loss 2.3887, val loss 2.4126\n",
      "step 94050: train loss 2.3856, val loss 2.3501\n",
      "step 94060: train loss 2.3925, val loss 2.4153\n",
      "step 94070: train loss 2.3643, val loss 2.3634\n",
      "step 94080: train loss 2.4156, val loss 2.4487\n",
      "step 94090: train loss 2.3774, val loss 2.3658\n",
      "step 94100: train loss 2.3174, val loss 2.4013\n",
      "step 94110: train loss 2.3490, val loss 2.3692\n",
      "step 94120: train loss 2.3943, val loss 2.3997\n",
      "step 94130: train loss 2.3825, val loss 2.3657\n",
      "step 94140: train loss 2.3155, val loss 2.3148\n",
      "step 94150: train loss 2.3267, val loss 2.4022\n",
      "step 94160: train loss 2.3658, val loss 2.3642\n",
      "step 94170: train loss 2.4058, val loss 2.3822\n",
      "step 94180: train loss 2.3812, val loss 2.3494\n",
      "step 94190: train loss 2.4109, val loss 2.3836\n",
      "step 94200: train loss 2.3908, val loss 2.3884\n",
      "step 94210: train loss 2.4103, val loss 2.4364\n",
      "step 94220: train loss 2.3999, val loss 2.3842\n",
      "step 94230: train loss 2.4477, val loss 2.3298\n",
      "step 94240: train loss 2.4368, val loss 2.3291\n",
      "step 94250: train loss 2.3279, val loss 2.3598\n",
      "step 94260: train loss 2.3533, val loss 2.3645\n",
      "step 94270: train loss 2.3874, val loss 2.3614\n",
      "step 94280: train loss 2.4415, val loss 2.3315\n",
      "step 94290: train loss 2.4174, val loss 2.3141\n",
      "step 94300: train loss 2.3704, val loss 2.3766\n",
      "step 94310: train loss 2.4462, val loss 2.3099\n",
      "step 94320: train loss 2.2863, val loss 2.3287\n",
      "step 94330: train loss 2.3800, val loss 2.4176\n",
      "step 94340: train loss 2.4182, val loss 2.4161\n",
      "step 94350: train loss 2.3340, val loss 2.4397\n",
      "step 94360: train loss 2.3814, val loss 2.3670\n",
      "step 94370: train loss 2.3775, val loss 2.4101\n",
      "step 94380: train loss 2.3692, val loss 2.4082\n",
      "step 94390: train loss 2.3826, val loss 2.4015\n",
      "step 94400: train loss 2.4258, val loss 2.3852\n",
      "step 94410: train loss 2.3692, val loss 2.3683\n",
      "step 94420: train loss 2.4005, val loss 2.3736\n",
      "step 94430: train loss 2.3207, val loss 2.3442\n",
      "step 94440: train loss 2.3744, val loss 2.4036\n",
      "step 94450: train loss 2.4086, val loss 2.3412\n",
      "step 94460: train loss 2.4171, val loss 2.4173\n",
      "step 94470: train loss 2.3769, val loss 2.3454\n",
      "step 94480: train loss 2.3904, val loss 2.4665\n",
      "step 94490: train loss 2.3528, val loss 2.3718\n",
      "step 94500: train loss 2.3575, val loss 2.3802\n",
      "step 94510: train loss 2.3134, val loss 2.3861\n",
      "step 94520: train loss 2.2626, val loss 2.4216\n",
      "step 94530: train loss 2.3235, val loss 2.3865\n",
      "step 94540: train loss 2.3861, val loss 2.3729\n",
      "step 94550: train loss 2.3992, val loss 2.4356\n",
      "step 94560: train loss 2.3493, val loss 2.3640\n",
      "step 94570: train loss 2.3078, val loss 2.4985\n",
      "step 94580: train loss 2.4195, val loss 2.3763\n",
      "step 94590: train loss 2.4621, val loss 2.4450\n",
      "step 94600: train loss 2.4028, val loss 2.3562\n",
      "step 94610: train loss 2.4028, val loss 2.4008\n",
      "step 94620: train loss 2.3655, val loss 2.3155\n",
      "step 94630: train loss 2.3648, val loss 2.3636\n",
      "step 94640: train loss 2.3260, val loss 2.2496\n",
      "step 94650: train loss 2.4394, val loss 2.3235\n",
      "step 94660: train loss 2.3217, val loss 2.3275\n",
      "step 94670: train loss 2.3754, val loss 2.3605\n",
      "step 94680: train loss 2.4463, val loss 2.3104\n",
      "step 94690: train loss 2.3151, val loss 2.4832\n",
      "step 94700: train loss 2.3826, val loss 2.4138\n",
      "step 94710: train loss 2.3905, val loss 2.4000\n",
      "step 94720: train loss 2.3724, val loss 2.3547\n",
      "step 94730: train loss 2.3809, val loss 2.3358\n",
      "step 94740: train loss 2.4207, val loss 2.4227\n",
      "step 94750: train loss 2.3651, val loss 2.3841\n",
      "step 94760: train loss 2.3599, val loss 2.3836\n",
      "step 94770: train loss 2.3665, val loss 2.4354\n",
      "step 94780: train loss 2.4537, val loss 2.3677\n",
      "step 94790: train loss 2.3034, val loss 2.3811\n",
      "step 94800: train loss 2.3517, val loss 2.4265\n",
      "step 94810: train loss 2.3506, val loss 2.2556\n",
      "step 94820: train loss 2.3862, val loss 2.4355\n",
      "step 94830: train loss 2.3213, val loss 2.3318\n",
      "step 94840: train loss 2.4338, val loss 2.3023\n",
      "step 94850: train loss 2.3912, val loss 2.3020\n",
      "step 94860: train loss 2.3587, val loss 2.3332\n",
      "step 94870: train loss 2.3868, val loss 2.3588\n",
      "step 94880: train loss 2.4883, val loss 2.3081\n",
      "step 94890: train loss 2.4077, val loss 2.3932\n",
      "step 94900: train loss 2.3751, val loss 2.3696\n",
      "step 94910: train loss 2.2571, val loss 2.3090\n",
      "step 94920: train loss 2.4444, val loss 2.4058\n",
      "step 94930: train loss 2.3967, val loss 2.3746\n",
      "step 94940: train loss 2.3899, val loss 2.3988\n",
      "step 94950: train loss 2.4129, val loss 2.3946\n",
      "step 94960: train loss 2.3203, val loss 2.3538\n",
      "step 94970: train loss 2.3373, val loss 2.3350\n",
      "step 94980: train loss 2.4244, val loss 2.3373\n",
      "step 94990: train loss 2.3893, val loss 2.3975\n",
      "step 95000: train loss 2.3745, val loss 2.3736\n",
      "step 95010: train loss 2.4378, val loss 2.3251\n",
      "step 95020: train loss 2.3924, val loss 2.3350\n",
      "step 95030: train loss 2.3227, val loss 2.3988\n",
      "step 95040: train loss 2.4487, val loss 2.3566\n",
      "step 95050: train loss 2.3967, val loss 2.4059\n",
      "step 95060: train loss 2.3509, val loss 2.3209\n",
      "step 95070: train loss 2.3624, val loss 2.2991\n",
      "step 95080: train loss 2.4745, val loss 2.3291\n",
      "step 95090: train loss 2.3558, val loss 2.3376\n",
      "step 95100: train loss 2.3359, val loss 2.3624\n",
      "step 95110: train loss 2.3985, val loss 2.4041\n",
      "step 95120: train loss 2.4295, val loss 2.3454\n",
      "step 95130: train loss 2.3672, val loss 2.4260\n",
      "step 95140: train loss 2.3964, val loss 2.3985\n",
      "step 95150: train loss 2.3332, val loss 2.4084\n",
      "step 95160: train loss 2.3007, val loss 2.3476\n",
      "step 95170: train loss 2.4005, val loss 2.3793\n",
      "step 95180: train loss 2.4050, val loss 2.4264\n",
      "step 95190: train loss 2.3970, val loss 2.3982\n",
      "step 95200: train loss 2.3590, val loss 2.3606\n",
      "step 95210: train loss 2.4173, val loss 2.3714\n",
      "step 95220: train loss 2.3337, val loss 2.4053\n",
      "step 95230: train loss 2.3871, val loss 2.4067\n",
      "step 95240: train loss 2.4293, val loss 2.3564\n",
      "step 95250: train loss 2.4252, val loss 2.4186\n",
      "step 95260: train loss 2.3715, val loss 2.3932\n",
      "step 95270: train loss 2.3128, val loss 2.3648\n",
      "step 95280: train loss 2.4120, val loss 2.2685\n",
      "step 95290: train loss 2.3169, val loss 2.3578\n",
      "step 95300: train loss 2.3429, val loss 2.3759\n",
      "step 95310: train loss 2.4002, val loss 2.3384\n",
      "step 95320: train loss 2.4118, val loss 2.3708\n",
      "step 95330: train loss 2.4122, val loss 2.3258\n",
      "step 95340: train loss 2.3506, val loss 2.2997\n",
      "step 95350: train loss 2.4511, val loss 2.4075\n",
      "step 95360: train loss 2.3789, val loss 2.3613\n",
      "step 95370: train loss 2.3254, val loss 2.3163\n",
      "step 95380: train loss 2.3784, val loss 2.3856\n",
      "step 95390: train loss 2.3748, val loss 2.3087\n",
      "step 95400: train loss 2.3738, val loss 2.3510\n",
      "step 95410: train loss 2.4712, val loss 2.4154\n",
      "step 95420: train loss 2.3833, val loss 2.3839\n",
      "step 95430: train loss 2.4131, val loss 2.2874\n",
      "step 95440: train loss 2.4170, val loss 2.3829\n",
      "step 95450: train loss 2.4220, val loss 2.3741\n",
      "step 95460: train loss 2.4012, val loss 2.3712\n",
      "step 95470: train loss 2.3775, val loss 2.4301\n",
      "step 95480: train loss 2.3521, val loss 2.3466\n",
      "step 95490: train loss 2.4319, val loss 2.2801\n",
      "step 95500: train loss 2.4054, val loss 2.2919\n",
      "step 95510: train loss 2.3622, val loss 2.3642\n",
      "step 95520: train loss 2.3639, val loss 2.4090\n",
      "step 95530: train loss 2.3227, val loss 2.3508\n",
      "step 95540: train loss 2.3348, val loss 2.4129\n",
      "step 95550: train loss 2.3750, val loss 2.2874\n",
      "step 95560: train loss 2.4292, val loss 2.3806\n",
      "step 95570: train loss 2.3525, val loss 2.2837\n",
      "step 95580: train loss 2.3671, val loss 2.3687\n",
      "step 95590: train loss 2.3600, val loss 2.4002\n",
      "step 95600: train loss 2.4370, val loss 2.4061\n",
      "step 95610: train loss 2.3649, val loss 2.3856\n",
      "step 95620: train loss 2.4258, val loss 2.3090\n",
      "step 95630: train loss 2.4447, val loss 2.3669\n",
      "step 95640: train loss 2.3285, val loss 2.3529\n",
      "step 95650: train loss 2.2757, val loss 2.4353\n",
      "step 95660: train loss 2.3605, val loss 2.3254\n",
      "step 95670: train loss 2.3521, val loss 2.4228\n",
      "step 95680: train loss 2.3947, val loss 2.3192\n",
      "step 95690: train loss 2.3463, val loss 2.3531\n",
      "step 95700: train loss 2.3897, val loss 2.3213\n",
      "step 95710: train loss 2.3289, val loss 2.4143\n",
      "step 95720: train loss 2.4075, val loss 2.4819\n",
      "step 95730: train loss 2.3762, val loss 2.4141\n",
      "step 95740: train loss 2.4148, val loss 2.3741\n",
      "step 95750: train loss 2.4586, val loss 2.4062\n",
      "step 95760: train loss 2.4488, val loss 2.3615\n",
      "step 95770: train loss 2.3560, val loss 2.3532\n",
      "step 95780: train loss 2.3776, val loss 2.4549\n",
      "step 95790: train loss 2.3963, val loss 2.3645\n",
      "step 95800: train loss 2.3719, val loss 2.3386\n",
      "step 95810: train loss 2.3572, val loss 2.2976\n",
      "step 95820: train loss 2.4238, val loss 2.3583\n",
      "step 95830: train loss 2.4040, val loss 2.4651\n",
      "step 95840: train loss 2.3749, val loss 2.3908\n",
      "step 95850: train loss 2.3479, val loss 2.3423\n",
      "step 95860: train loss 2.3550, val loss 2.3612\n",
      "step 95870: train loss 2.3729, val loss 2.3885\n",
      "step 95880: train loss 2.3704, val loss 2.3351\n",
      "step 95890: train loss 2.3564, val loss 2.3392\n",
      "step 95900: train loss 2.3875, val loss 2.2863\n",
      "step 95910: train loss 2.4629, val loss 2.3443\n",
      "step 95920: train loss 2.3753, val loss 2.3333\n",
      "step 95930: train loss 2.3537, val loss 2.3946\n",
      "step 95940: train loss 2.3967, val loss 2.3268\n",
      "step 95950: train loss 2.4161, val loss 2.3605\n",
      "step 95960: train loss 2.3382, val loss 2.3467\n",
      "step 95970: train loss 2.3496, val loss 2.3896\n",
      "step 95980: train loss 2.3584, val loss 2.3620\n",
      "step 95990: train loss 2.4173, val loss 2.3155\n",
      "step 96000: train loss 2.4064, val loss 2.3129\n",
      "step 96010: train loss 2.4185, val loss 2.3159\n",
      "step 96020: train loss 2.3367, val loss 2.3823\n",
      "step 96030: train loss 2.3103, val loss 2.3282\n",
      "step 96040: train loss 2.3848, val loss 2.3777\n",
      "step 96050: train loss 2.3898, val loss 2.4697\n",
      "step 96060: train loss 2.3956, val loss 2.2914\n",
      "step 96070: train loss 2.4005, val loss 2.3881\n",
      "step 96080: train loss 2.4166, val loss 2.3606\n",
      "step 96090: train loss 2.4721, val loss 2.3948\n",
      "step 96100: train loss 2.3960, val loss 2.3215\n",
      "step 96110: train loss 2.3573, val loss 2.4027\n",
      "step 96120: train loss 2.4016, val loss 2.3603\n",
      "step 96130: train loss 2.3601, val loss 2.3303\n",
      "step 96140: train loss 2.3605, val loss 2.3716\n",
      "step 96150: train loss 2.4686, val loss 2.3775\n",
      "step 96160: train loss 2.4495, val loss 2.4170\n",
      "step 96170: train loss 2.3889, val loss 2.2866\n",
      "step 96180: train loss 2.3774, val loss 2.4582\n",
      "step 96190: train loss 2.3102, val loss 2.4116\n",
      "step 96200: train loss 2.3084, val loss 2.4569\n",
      "step 96210: train loss 2.4048, val loss 2.3520\n",
      "step 96220: train loss 2.4456, val loss 2.3350\n",
      "step 96230: train loss 2.4211, val loss 2.4242\n",
      "step 96240: train loss 2.3578, val loss 2.3757\n",
      "step 96250: train loss 2.4041, val loss 2.2862\n",
      "step 96260: train loss 2.4255, val loss 2.3856\n",
      "step 96270: train loss 2.4061, val loss 2.3637\n",
      "step 96280: train loss 2.4380, val loss 2.3532\n",
      "step 96290: train loss 2.4396, val loss 2.3733\n",
      "step 96300: train loss 2.3741, val loss 2.2594\n",
      "step 96310: train loss 2.4025, val loss 2.3160\n",
      "step 96320: train loss 2.3666, val loss 2.3780\n",
      "step 96330: train loss 2.3551, val loss 2.3957\n",
      "step 96340: train loss 2.4568, val loss 2.4882\n",
      "step 96350: train loss 2.3408, val loss 2.4275\n",
      "step 96360: train loss 2.3930, val loss 2.2796\n",
      "step 96370: train loss 2.4079, val loss 2.4366\n",
      "step 96380: train loss 2.3742, val loss 2.4110\n",
      "step 96390: train loss 2.3253, val loss 2.3401\n",
      "step 96400: train loss 2.3324, val loss 2.2947\n",
      "step 96410: train loss 2.3538, val loss 2.3556\n",
      "step 96420: train loss 2.3566, val loss 2.3670\n",
      "step 96430: train loss 2.4028, val loss 2.4181\n",
      "step 96440: train loss 2.3521, val loss 2.4525\n",
      "step 96450: train loss 2.3840, val loss 2.3585\n",
      "step 96460: train loss 2.4587, val loss 2.3557\n",
      "step 96470: train loss 2.4053, val loss 2.3136\n",
      "step 96480: train loss 2.2913, val loss 2.4129\n",
      "step 96490: train loss 2.3289, val loss 2.3844\n",
      "step 96500: train loss 2.4873, val loss 2.3513\n",
      "step 96510: train loss 2.3601, val loss 2.4226\n",
      "step 96520: train loss 2.3929, val loss 2.3526\n",
      "step 96530: train loss 2.3690, val loss 2.4084\n",
      "step 96540: train loss 2.3511, val loss 2.3442\n",
      "step 96550: train loss 2.4036, val loss 2.4298\n",
      "step 96560: train loss 2.3866, val loss 2.3791\n",
      "step 96570: train loss 2.4212, val loss 2.3731\n",
      "step 96580: train loss 2.3305, val loss 2.3309\n",
      "step 96590: train loss 2.3274, val loss 2.3302\n",
      "step 96600: train loss 2.3649, val loss 2.3173\n",
      "step 96610: train loss 2.3819, val loss 2.3584\n",
      "step 96620: train loss 2.3652, val loss 2.4366\n",
      "step 96630: train loss 2.3875, val loss 2.3978\n",
      "step 96640: train loss 2.3807, val loss 2.3433\n",
      "step 96650: train loss 2.3291, val loss 2.3768\n",
      "step 96660: train loss 2.3197, val loss 2.3024\n",
      "step 96670: train loss 2.3189, val loss 2.4242\n",
      "step 96680: train loss 2.3840, val loss 2.3192\n",
      "step 96690: train loss 2.3708, val loss 2.3753\n",
      "step 96700: train loss 2.3572, val loss 2.3500\n",
      "step 96710: train loss 2.4002, val loss 2.4091\n",
      "step 96720: train loss 2.4025, val loss 2.3580\n",
      "step 96730: train loss 2.3553, val loss 2.4310\n",
      "step 96740: train loss 2.3935, val loss 2.3465\n",
      "step 96750: train loss 2.4027, val loss 2.3077\n",
      "step 96760: train loss 2.3250, val loss 2.3716\n",
      "step 96770: train loss 2.3609, val loss 2.3761\n",
      "step 96780: train loss 2.3409, val loss 2.3795\n",
      "step 96790: train loss 2.3172, val loss 2.3693\n",
      "step 96800: train loss 2.4091, val loss 2.3866\n",
      "step 96810: train loss 2.3537, val loss 2.3033\n",
      "step 96820: train loss 2.3145, val loss 2.3485\n",
      "step 96830: train loss 2.4457, val loss 2.3511\n",
      "step 96840: train loss 2.3683, val loss 2.3176\n",
      "step 96850: train loss 2.4205, val loss 2.4350\n",
      "step 96860: train loss 2.3609, val loss 2.4376\n",
      "step 96870: train loss 2.3321, val loss 2.4560\n",
      "step 96880: train loss 2.3940, val loss 2.3662\n",
      "step 96890: train loss 2.4508, val loss 2.3566\n",
      "step 96900: train loss 2.3514, val loss 2.3619\n",
      "step 96910: train loss 2.3411, val loss 2.3533\n",
      "step 96920: train loss 2.3169, val loss 2.3349\n",
      "step 96930: train loss 2.3408, val loss 2.4026\n",
      "step 96940: train loss 2.3223, val loss 2.4247\n",
      "step 96950: train loss 2.3584, val loss 2.3609\n",
      "step 96960: train loss 2.3246, val loss 2.3301\n",
      "step 96970: train loss 2.3380, val loss 2.4071\n",
      "step 96980: train loss 2.3383, val loss 2.3660\n",
      "step 96990: train loss 2.3356, val loss 2.3933\n",
      "step 97000: train loss 2.3590, val loss 2.4243\n",
      "step 97010: train loss 2.4041, val loss 2.3886\n",
      "step 97020: train loss 2.3695, val loss 2.3802\n",
      "step 97030: train loss 2.3480, val loss 2.3884\n",
      "step 97040: train loss 2.3750, val loss 2.3170\n",
      "step 97050: train loss 2.3648, val loss 2.4580\n",
      "step 97060: train loss 2.4863, val loss 2.3741\n",
      "step 97070: train loss 2.3616, val loss 2.3388\n",
      "step 97080: train loss 2.3570, val loss 2.3290\n",
      "step 97090: train loss 2.3564, val loss 2.3482\n",
      "step 97100: train loss 2.3560, val loss 2.3059\n",
      "step 97110: train loss 2.4038, val loss 2.3288\n",
      "step 97120: train loss 2.3241, val loss 2.3396\n",
      "step 97130: train loss 2.3205, val loss 2.3954\n",
      "step 97140: train loss 2.3510, val loss 2.3916\n",
      "step 97150: train loss 2.3675, val loss 2.3894\n",
      "step 97160: train loss 2.3596, val loss 2.2719\n",
      "step 97170: train loss 2.3621, val loss 2.4183\n",
      "step 97180: train loss 2.3250, val loss 2.3640\n",
      "step 97190: train loss 2.4526, val loss 2.3580\n",
      "step 97200: train loss 2.4201, val loss 2.4320\n",
      "step 97210: train loss 2.3902, val loss 2.2876\n",
      "step 97220: train loss 2.3617, val loss 2.3342\n",
      "step 97230: train loss 2.3365, val loss 2.4758\n",
      "step 97240: train loss 2.3484, val loss 2.4102\n",
      "step 97250: train loss 2.3032, val loss 2.3931\n",
      "step 97260: train loss 2.3818, val loss 2.3268\n",
      "step 97270: train loss 2.3142, val loss 2.2909\n",
      "step 97280: train loss 2.4620, val loss 2.3892\n",
      "step 97290: train loss 2.3823, val loss 2.3181\n",
      "step 97300: train loss 2.3715, val loss 2.3185\n",
      "step 97310: train loss 2.4292, val loss 2.4284\n",
      "step 97320: train loss 2.3215, val loss 2.3560\n",
      "step 97330: train loss 2.3997, val loss 2.3533\n",
      "step 97340: train loss 2.4058, val loss 2.4121\n",
      "step 97350: train loss 2.3668, val loss 2.3365\n",
      "step 97360: train loss 2.4260, val loss 2.4069\n",
      "step 97370: train loss 2.3935, val loss 2.3758\n",
      "step 97380: train loss 2.3486, val loss 2.3543\n",
      "step 97390: train loss 2.3829, val loss 2.3190\n",
      "step 97400: train loss 2.3744, val loss 2.3626\n",
      "step 97410: train loss 2.3574, val loss 2.3814\n",
      "step 97420: train loss 2.4394, val loss 2.2478\n",
      "step 97430: train loss 2.3773, val loss 2.3458\n",
      "step 97440: train loss 2.3024, val loss 2.2886\n",
      "step 97450: train loss 2.3774, val loss 2.3216\n",
      "step 97460: train loss 2.4450, val loss 2.3659\n",
      "step 97470: train loss 2.3401, val loss 2.3742\n",
      "step 97480: train loss 2.4499, val loss 2.3540\n",
      "step 97490: train loss 2.3928, val loss 2.3968\n",
      "step 97500: train loss 2.4463, val loss 2.3017\n",
      "step 97510: train loss 2.3277, val loss 2.3751\n",
      "step 97520: train loss 2.3793, val loss 2.5297\n",
      "step 97530: train loss 2.3261, val loss 2.4116\n",
      "step 97540: train loss 2.3909, val loss 2.4019\n",
      "step 97550: train loss 2.3667, val loss 2.4401\n",
      "step 97560: train loss 2.3729, val loss 2.3002\n",
      "step 97570: train loss 2.3977, val loss 2.4160\n",
      "step 97580: train loss 2.3665, val loss 2.3929\n",
      "step 97590: train loss 2.4217, val loss 2.3404\n",
      "step 97600: train loss 2.4334, val loss 2.4045\n",
      "step 97610: train loss 2.4114, val loss 2.3835\n",
      "step 97620: train loss 2.3846, val loss 2.3991\n",
      "step 97630: train loss 2.3614, val loss 2.4392\n",
      "step 97640: train loss 2.3882, val loss 2.4246\n",
      "step 97650: train loss 2.3423, val loss 2.2848\n",
      "step 97660: train loss 2.4233, val loss 2.3113\n",
      "step 97670: train loss 2.4831, val loss 2.3722\n",
      "step 97680: train loss 2.3540, val loss 2.3859\n",
      "step 97690: train loss 2.3660, val loss 2.3211\n",
      "step 97700: train loss 2.3425, val loss 2.4034\n",
      "step 97710: train loss 2.3658, val loss 2.3614\n",
      "step 97720: train loss 2.3167, val loss 2.3444\n",
      "step 97730: train loss 2.3662, val loss 2.3606\n",
      "step 97740: train loss 2.4006, val loss 2.3944\n",
      "step 97750: train loss 2.4089, val loss 2.3611\n",
      "step 97760: train loss 2.3148, val loss 2.3388\n",
      "step 97770: train loss 2.3370, val loss 2.3399\n",
      "step 97780: train loss 2.3323, val loss 2.3125\n",
      "step 97790: train loss 2.3725, val loss 2.3739\n",
      "step 97800: train loss 2.4040, val loss 2.3486\n",
      "step 97810: train loss 2.3973, val loss 2.3526\n",
      "step 97820: train loss 2.3625, val loss 2.2838\n",
      "step 97830: train loss 2.3987, val loss 2.3287\n",
      "step 97840: train loss 2.3730, val loss 2.3854\n",
      "step 97850: train loss 2.3796, val loss 2.3439\n",
      "step 97860: train loss 2.3705, val loss 2.3112\n",
      "step 97870: train loss 2.3506, val loss 2.3904\n",
      "step 97880: train loss 2.4190, val loss 2.3377\n",
      "step 97890: train loss 2.4527, val loss 2.3285\n",
      "step 97900: train loss 2.3989, val loss 2.5324\n",
      "step 97910: train loss 2.4352, val loss 2.3284\n",
      "step 97920: train loss 2.4125, val loss 2.4409\n",
      "step 97930: train loss 2.4134, val loss 2.3259\n",
      "step 97940: train loss 2.3277, val loss 2.2756\n",
      "step 97950: train loss 2.3916, val loss 2.3515\n",
      "step 97960: train loss 2.3771, val loss 2.3258\n",
      "step 97970: train loss 2.4223, val loss 2.3884\n",
      "step 97980: train loss 2.3897, val loss 2.3516\n",
      "step 97990: train loss 2.3598, val loss 2.4387\n",
      "step 98000: train loss 2.4038, val loss 2.2908\n",
      "step 98010: train loss 2.3496, val loss 2.3996\n",
      "step 98020: train loss 2.4869, val loss 2.3014\n",
      "step 98030: train loss 2.3812, val loss 2.3981\n",
      "step 98040: train loss 2.3961, val loss 2.3938\n",
      "step 98050: train loss 2.3630, val loss 2.3350\n",
      "step 98060: train loss 2.4250, val loss 2.3618\n",
      "step 98070: train loss 2.3839, val loss 2.3329\n",
      "step 98080: train loss 2.4773, val loss 2.3846\n",
      "step 98090: train loss 2.3375, val loss 2.3738\n",
      "step 98100: train loss 2.3603, val loss 2.3627\n",
      "step 98110: train loss 2.3443, val loss 2.3478\n",
      "step 98120: train loss 2.3800, val loss 2.4154\n",
      "step 98130: train loss 2.4065, val loss 2.3553\n",
      "step 98140: train loss 2.3940, val loss 2.3643\n",
      "step 98150: train loss 2.2769, val loss 2.4090\n",
      "step 98160: train loss 2.4692, val loss 2.3807\n",
      "step 98170: train loss 2.3586, val loss 2.3558\n",
      "step 98180: train loss 2.3994, val loss 2.4157\n",
      "step 98190: train loss 2.4329, val loss 2.3330\n",
      "step 98200: train loss 2.3918, val loss 2.3458\n",
      "step 98210: train loss 2.4405, val loss 2.4005\n",
      "step 98220: train loss 2.4243, val loss 2.3235\n",
      "step 98230: train loss 2.3474, val loss 2.4131\n",
      "step 98240: train loss 2.4074, val loss 2.3329\n",
      "step 98250: train loss 2.3917, val loss 2.3037\n",
      "step 98260: train loss 2.3801, val loss 2.4245\n",
      "step 98270: train loss 2.3961, val loss 2.3050\n",
      "step 98280: train loss 2.4100, val loss 2.4315\n",
      "step 98290: train loss 2.2793, val loss 2.3848\n",
      "step 98300: train loss 2.3857, val loss 2.3847\n",
      "step 98310: train loss 2.3436, val loss 2.3374\n",
      "step 98320: train loss 2.3713, val loss 2.4091\n",
      "step 98330: train loss 2.3694, val loss 2.2862\n",
      "step 98340: train loss 2.3371, val loss 2.3721\n",
      "step 98350: train loss 2.4790, val loss 2.3430\n",
      "step 98360: train loss 2.3966, val loss 2.3748\n",
      "step 98370: train loss 2.4168, val loss 2.3313\n",
      "step 98380: train loss 2.3669, val loss 2.4102\n",
      "step 98390: train loss 2.4916, val loss 2.3472\n",
      "step 98400: train loss 2.3656, val loss 2.3535\n",
      "step 98410: train loss 2.3834, val loss 2.3365\n",
      "step 98420: train loss 2.4747, val loss 2.3630\n",
      "step 98430: train loss 2.3785, val loss 2.3872\n",
      "step 98440: train loss 2.4312, val loss 2.3776\n",
      "step 98450: train loss 2.4102, val loss 2.4334\n",
      "step 98460: train loss 2.4142, val loss 2.3886\n",
      "step 98470: train loss 2.4047, val loss 2.3729\n",
      "step 98480: train loss 2.3724, val loss 2.5221\n",
      "step 98490: train loss 2.3926, val loss 2.3913\n",
      "step 98500: train loss 2.3676, val loss 2.3019\n",
      "step 98510: train loss 2.4278, val loss 2.4142\n",
      "step 98520: train loss 2.4350, val loss 2.3903\n",
      "step 98530: train loss 2.3026, val loss 2.3794\n",
      "step 98540: train loss 2.4288, val loss 2.4405\n",
      "step 98550: train loss 2.4018, val loss 2.4074\n",
      "step 98560: train loss 2.4682, val loss 2.3121\n",
      "step 98570: train loss 2.3587, val loss 2.4500\n",
      "step 98580: train loss 2.4417, val loss 2.3447\n",
      "step 98590: train loss 2.3527, val loss 2.3756\n",
      "step 98600: train loss 2.4338, val loss 2.4507\n",
      "step 98610: train loss 2.3779, val loss 2.3912\n",
      "step 98620: train loss 2.4245, val loss 2.3599\n",
      "step 98630: train loss 2.3966, val loss 2.3816\n",
      "step 98640: train loss 2.3402, val loss 2.3823\n",
      "step 98650: train loss 2.3641, val loss 2.3489\n",
      "step 98660: train loss 2.3619, val loss 2.3560\n",
      "step 98670: train loss 2.3901, val loss 2.3464\n",
      "step 98680: train loss 2.4212, val loss 2.2771\n",
      "step 98690: train loss 2.3984, val loss 2.3015\n",
      "step 98700: train loss 2.3205, val loss 2.3769\n",
      "step 98710: train loss 2.4043, val loss 2.4107\n",
      "step 98720: train loss 2.4364, val loss 2.4815\n",
      "step 98730: train loss 2.3826, val loss 2.3724\n",
      "step 98740: train loss 2.3225, val loss 2.4210\n",
      "step 98750: train loss 2.3810, val loss 2.2626\n",
      "step 98760: train loss 2.4398, val loss 2.3193\n",
      "step 98770: train loss 2.4036, val loss 2.3716\n",
      "step 98780: train loss 2.4237, val loss 2.4492\n",
      "step 98790: train loss 2.3781, val loss 2.3146\n",
      "step 98800: train loss 2.3855, val loss 2.2963\n",
      "step 98810: train loss 2.4017, val loss 2.3019\n",
      "step 98820: train loss 2.3350, val loss 2.3736\n",
      "step 98830: train loss 2.4121, val loss 2.3892\n",
      "step 98840: train loss 2.3500, val loss 2.4296\n",
      "step 98850: train loss 2.3769, val loss 2.4174\n",
      "step 98860: train loss 2.3568, val loss 2.4032\n",
      "step 98870: train loss 2.3054, val loss 2.3711\n",
      "step 98880: train loss 2.3197, val loss 2.3788\n",
      "step 98890: train loss 2.3741, val loss 2.3738\n",
      "step 98900: train loss 2.3258, val loss 2.3176\n",
      "step 98910: train loss 2.4403, val loss 2.3726\n",
      "step 98920: train loss 2.3331, val loss 2.3820\n",
      "step 98930: train loss 2.4296, val loss 2.3610\n",
      "step 98940: train loss 2.3747, val loss 2.3790\n",
      "step 98950: train loss 2.3930, val loss 2.3721\n",
      "step 98960: train loss 2.4351, val loss 2.4160\n",
      "step 98970: train loss 2.4033, val loss 2.3984\n",
      "step 98980: train loss 2.3570, val loss 2.3439\n",
      "step 98990: train loss 2.4424, val loss 2.3924\n",
      "step 99000: train loss 2.4538, val loss 2.3518\n",
      "step 99010: train loss 2.2877, val loss 2.4717\n",
      "step 99020: train loss 2.4132, val loss 2.3107\n",
      "step 99030: train loss 2.3632, val loss 2.2519\n",
      "step 99040: train loss 2.3279, val loss 2.3658\n",
      "step 99050: train loss 2.3473, val loss 2.4793\n",
      "step 99060: train loss 2.3607, val loss 2.4232\n",
      "step 99070: train loss 2.3971, val loss 2.3636\n",
      "step 99080: train loss 2.2873, val loss 2.3182\n",
      "step 99090: train loss 2.4585, val loss 2.3160\n",
      "step 99100: train loss 2.3112, val loss 2.3264\n",
      "step 99110: train loss 2.3975, val loss 2.3308\n",
      "step 99120: train loss 2.4288, val loss 2.4640\n",
      "step 99130: train loss 2.3267, val loss 2.2924\n",
      "step 99140: train loss 2.4134, val loss 2.3961\n",
      "step 99150: train loss 2.3638, val loss 2.3965\n",
      "step 99160: train loss 2.3331, val loss 2.3293\n",
      "step 99170: train loss 2.3339, val loss 2.3620\n",
      "step 99180: train loss 2.3621, val loss 2.3845\n",
      "step 99190: train loss 2.3841, val loss 2.3125\n",
      "step 99200: train loss 2.3946, val loss 2.3641\n",
      "step 99210: train loss 2.3514, val loss 2.3760\n",
      "step 99220: train loss 2.3817, val loss 2.3960\n",
      "step 99230: train loss 2.3690, val loss 2.4368\n",
      "step 99240: train loss 2.3789, val loss 2.4067\n",
      "step 99250: train loss 2.3977, val loss 2.3925\n",
      "step 99260: train loss 2.3363, val loss 2.2524\n",
      "step 99270: train loss 2.3734, val loss 2.4025\n",
      "step 99280: train loss 2.3287, val loss 2.3280\n",
      "step 99290: train loss 2.3859, val loss 2.3349\n",
      "step 99300: train loss 2.3902, val loss 2.4581\n",
      "step 99310: train loss 2.3716, val loss 2.3215\n",
      "step 99320: train loss 2.4290, val loss 2.4340\n",
      "step 99330: train loss 2.3306, val loss 2.3599\n",
      "step 99340: train loss 2.3648, val loss 2.3916\n",
      "step 99350: train loss 2.4523, val loss 2.4164\n",
      "step 99360: train loss 2.3127, val loss 2.4072\n",
      "step 99370: train loss 2.3312, val loss 2.4218\n",
      "step 99380: train loss 2.4163, val loss 2.3877\n",
      "step 99390: train loss 2.3790, val loss 2.3934\n",
      "step 99400: train loss 2.3027, val loss 2.4259\n",
      "step 99410: train loss 2.4220, val loss 2.4497\n",
      "step 99420: train loss 2.3379, val loss 2.3319\n",
      "step 99430: train loss 2.3103, val loss 2.3966\n",
      "step 99440: train loss 2.3407, val loss 2.4003\n",
      "step 99450: train loss 2.4245, val loss 2.4237\n",
      "step 99460: train loss 2.4179, val loss 2.3667\n",
      "step 99470: train loss 2.4164, val loss 2.3095\n",
      "step 99480: train loss 2.4113, val loss 2.3917\n",
      "step 99490: train loss 2.3846, val loss 2.4278\n",
      "step 99500: train loss 2.4343, val loss 2.3964\n",
      "step 99510: train loss 2.3801, val loss 2.4058\n",
      "step 99520: train loss 2.3809, val loss 2.3712\n",
      "step 99530: train loss 2.3533, val loss 2.4262\n",
      "step 99540: train loss 2.3712, val loss 2.3476\n",
      "step 99550: train loss 2.2926, val loss 2.3364\n",
      "step 99560: train loss 2.3131, val loss 2.3307\n",
      "step 99570: train loss 2.4271, val loss 2.3581\n",
      "step 99580: train loss 2.4384, val loss 2.3960\n",
      "step 99590: train loss 2.3346, val loss 2.3776\n",
      "step 99600: train loss 2.4087, val loss 2.3657\n",
      "step 99610: train loss 2.3976, val loss 2.4478\n",
      "step 99620: train loss 2.3116, val loss 2.3729\n",
      "step 99630: train loss 2.3777, val loss 2.4120\n",
      "step 99640: train loss 2.3594, val loss 2.4386\n",
      "step 99650: train loss 2.3257, val loss 2.3613\n",
      "step 99660: train loss 2.4051, val loss 2.3365\n",
      "step 99670: train loss 2.3462, val loss 2.4288\n",
      "step 99680: train loss 2.3007, val loss 2.3010\n",
      "step 99690: train loss 2.4565, val loss 2.3668\n",
      "step 99700: train loss 2.4003, val loss 2.3498\n",
      "step 99710: train loss 2.4127, val loss 2.3590\n",
      "step 99720: train loss 2.3959, val loss 2.4074\n",
      "step 99730: train loss 2.3481, val loss 2.3631\n",
      "step 99740: train loss 2.4504, val loss 2.4071\n",
      "step 99750: train loss 2.3379, val loss 2.3716\n",
      "step 99760: train loss 2.3408, val loss 2.3624\n",
      "step 99770: train loss 2.3854, val loss 2.3194\n",
      "step 99780: train loss 2.3582, val loss 2.3502\n",
      "step 99790: train loss 2.3724, val loss 2.3488\n",
      "step 99800: train loss 2.4513, val loss 2.5182\n",
      "step 99810: train loss 2.3011, val loss 2.3453\n",
      "step 99820: train loss 2.4138, val loss 2.4309\n",
      "step 99830: train loss 2.3640, val loss 2.3940\n",
      "step 99840: train loss 2.3803, val loss 2.3019\n",
      "step 99850: train loss 2.3734, val loss 2.3034\n",
      "step 99860: train loss 2.3483, val loss 2.3830\n",
      "step 99870: train loss 2.3932, val loss 2.3098\n",
      "step 99880: train loss 2.4628, val loss 2.4156\n",
      "step 99890: train loss 2.3275, val loss 2.4632\n",
      "step 99900: train loss 2.3640, val loss 2.3833\n",
      "step 99910: train loss 2.3672, val loss 2.3404\n",
      "step 99920: train loss 2.3255, val loss 2.2918\n",
      "step 99930: train loss 2.4771, val loss 2.3462\n",
      "step 99940: train loss 2.4332, val loss 2.4008\n",
      "step 99950: train loss 2.4767, val loss 2.3763\n",
      "step 99960: train loss 2.3290, val loss 2.4113\n",
      "step 99970: train loss 2.3964, val loss 2.3586\n",
      "step 99980: train loss 2.3438, val loss 2.3605\n",
      "step 99990: train loss 2.3806, val loss 2.3385\n"
     ]
    }
   ],
   "source": [
    "max_iters = 100000\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 20\n",
    "\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512395a0",
   "metadata": {},
   "source": [
    "Une fois le réseau entrainé pendant 100 itérations, on peut générer une séquence de caractères.\n",
    "\n",
    "Questions :\n",
    "> * Quel est l'effet de l'entraînement ?\n",
    "> * Augmenter le nombre d'itérations à 1000 puis à 10,000, noter la loss obtenue et la phrase générée. Qu'observez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4071b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mêmbrauere le pos, parefréd'érs a e-ve, ffl'ounte.\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "print (decode(m.generate(idx, max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd33b1",
   "metadata": {},
   "source": [
    "## Single Head Attention\n",
    "\n",
    "Nous allons maintenant implémenter le mécanisme de base de l'attention. Pour chaque couple de mots de la séquence, ce mécanisme combine Q une *query* (l'information recherchée), K une *key* (l'information obtenue) et calcule  V une *value*, un vecteur de résultat. \n",
    "\n",
    "![single head attention](images/single_head_attention.png)\n",
    "\n",
    "### Masquage\n",
    "Cependant, comme nous utilisons le modèle pour générer des séquences, on ne doit pas utiliser les caractères situés après le caractère courant, car ce sont justement ces caractères que l'on cherche à prédire lors de l'apprentissage : *le futur n'est pas utilisé pour prédire (le futur).* \n",
    "\n",
    "On va donc intégrer une matrice de masquage dans le processus. Cette matrice indique que pour le premier caractère de la séquence, on ne peux utiliser que ce caractère pour prédire (pas de contexte). Pour le second caractère, on peut utiliser le premier caractère et le second. Pour le troisième caractère, on peut utiliser le premier, le second et le troisième et ainsi de suite. Cette matrice est donc une matrice triangulaire inférieure. Cette matrice est normalisée par ligne (les lignes somment à 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d15fbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "T = 8\n",
    "\n",
    "# first version of the contraints with matrix multiplication\n",
    "# create a lower triangular matrix\n",
    "weights0 = torch.tril(torch.ones(T,T))\n",
    "# normalize each row\n",
    "weights0 = weights0 / weights0.sum(1, keepdim=True) \n",
    "print (weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eb4a7",
   "metadata": {},
   "source": [
    "La couche [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) est une autre manière de réaliser la normalisation :\n",
    "\n",
    "Question :\n",
    "> * Vérifier qu'on obtient bien la même matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "75455f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a323f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "print(weights0==weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76c8b",
   "metadata": {},
   "source": [
    "### Implémentation\n",
    "\n",
    "Nous pouvons maintenant implémenter la couche d'attention :\n",
    "\n",
    "![attention_formula](images/attention_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681533",
   "metadata": {},
   "source": [
    "Questions :\n",
    "\n",
    "> * Créer les couches key, query et value comme des couches linéaires de dimension `C` x `head_size`.\n",
    "> * Appliquer les couches à `x`.\n",
    "> * `weights = query x key` (transposer les deuxième et troisième dimensions de key pour pouvoir faire le produit).\n",
    "> * Appliquer le facteur de normalisation.\n",
    "> * Appliquer le masque triangulaire et la softmax à `weights`.\n",
    "> * Appliquer value à `x`.\n",
    "> * Le résultat `out` est la multiplication de `weights` par `value(x)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "129fe994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1700,  0.6180, -0.3346,  0.3648,  0.4960,  0.6652, -0.0907,  0.1670,\n",
       "         -0.3092,  0.5187,  0.9016,  0.5854, -0.9934, -0.3828, -0.3957, -0.3004],\n",
       "        [ 0.8558,  0.3197, -0.4484,  0.5498, -0.0115,  0.5081,  0.0692,  0.1967,\n",
       "         -0.3770,  0.0998,  0.4115,  0.2646, -0.5755, -0.1244, -0.2725, -0.1273],\n",
       "        [ 0.7903,  0.6937, -0.0745,  0.3096, -0.3811,  0.1664,  0.2032, -0.3123,\n",
       "         -0.5529, -0.0430,  0.5985,  0.0796, -0.8585, -0.3135, -0.3559, -0.1375],\n",
       "        [ 0.4363,  0.8063,  0.1963,  0.2345, -1.2100, -0.3381,  0.4827, -0.7812,\n",
       "         -0.7646, -0.5728,  0.3017, -0.4243, -0.7025, -0.2552, -0.3343,  0.0323],\n",
       "        [ 0.9304,  0.7963, -0.2754,  0.2896,  0.2706,  0.0134,  0.0091,  0.3992,\n",
       "          0.3031,  0.1499, -0.2323, -0.2666,  0.6590,  0.0090, -0.5403,  0.3311],\n",
       "        [ 0.3503,  0.3494, -0.3037,  0.1735, -0.2752,  0.0696, -0.0173, -0.0533,\n",
       "          0.1532, -0.5034, -0.3019, -0.1942,  0.3556, -0.0896,  0.0627,  0.2054],\n",
       "        [ 0.8668,  0.7955, -0.1764,  0.1293,  0.2841,  0.0181, -0.0420,  0.2207,\n",
       "          0.2807,  0.1648, -0.0470, -0.1074,  0.4900, -0.0863, -0.4959,  0.2012],\n",
       "        [ 0.7180,  0.6326, -0.2418,  0.2277, -0.2531,  0.0119,  0.0405,  0.2562,\n",
       "          0.0635, -0.0617, -0.1469, -0.2962,  0.4735,  0.1514, -0.3803,  0.1157]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_size = 16\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "## YOUR CODE HERE\n",
    "# define the Key layer  \n",
    "key = nn.Linear(C,head_size)\n",
    "# define the Query layer\n",
    "query = nn.Linear(C,head_size)\n",
    "# define the Value layer\n",
    "value = nn.Linear(C,head_size)\n",
    "# apply each layer to the input\n",
    "k = key(x) # (B, T, head_size)\n",
    "q =  query(x) # (B, T, head_size)\n",
    "v =  value(x) # (B, T, head_size)\n",
    "# compute the normalize product between Q and K \n",
    "weights=q@k.transpose(-2,-1)\n",
    " # (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\n",
    "# apply the mask (lower triangular matrix)\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "# apply the softmax\n",
    "weights = nn.functional.softmax(weights,dim=-1)\n",
    "###\n",
    "out  = weights @ value(x) # (B, T, head_size)\n",
    "\n",
    "# print the result\n",
    "weights[0]\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db591771",
   "metadata": {},
   "source": [
    "Questions :\n",
    "\n",
    "> * Copier votre code dans `gpt_single_head.py` : la définition des couches dans le constructeur de la classe `Head` et les calculs dans la fonction `forward`.\n",
    "> * Faire un entrainement.\n",
    "> * Quelle loss en train et val obtenez vous ? Le texte vous parait-il meilleur ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2eab27",
   "metadata": {},
   "source": [
    "step 4999: train loss 2.3395, val loss 2.4377\n",
    "\n",
    "Lan s itendin dutaisoprrs bongez brit,\n",
    " cesiai;\n",
    "Degaie ce ves treus ue\n",
    " pesr combr, eis s es'ou veux ommont;\n",
    "PA uquuentomu quoire:--garne, mmort bouve t penndu pinemurei an ives defaus\n",
    "Etess-t  du fémobire à epc, quontent Ot oirdantr, où l'à  mis? g'évas, plisoillaur, lerchotrênd ffine l'az de denobie flal vouireve less proiloumu  pisste aêvommenarsontorutosurau bl'anant rait farireut esali l'ues;\n",
    "Su, me  Nonelusileut  âtre, plre soiracent;\n",
    "Que Uzu  cet, vi,  soma l'orucre plavecue det  à aut,\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043812e",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "La *multi-head attention* est simplement le calcul en parallèle de plusieurs *single head attention*. Chacune des single head attention est concaténée pour créer la sortie de la multi-head attention. Dans la figure issue de l'article original, le nombre de *heads* dans le *multi-head* est `h`. Afin d'opérer des combinaisons pondérées sur la sortie de chacune des single head, une couche de calcul linéaire est ajoutée.\n",
    "\n",
    "![multi head attention](images/multi_head_attention.png)\n",
    "\n",
    "Le code ci-dessous crée un module de multi-head attention.\n",
    "Questions :\n",
    "> * Dans le constructeur, créer une liste contenant `num_heads` module `Head` en utilisant la fonction [ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) de pytorch.\n",
    "> * Dans la fonction `forward`, appliquer chaque single head à l'input et concaténer le résultat en utilisant la fonction [cat](https://pytorch.org/docs/stable/generated/torch.cat.html) de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        ## list of num_heads modules of type Head\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE HERE\n",
    "        ## apply each head in self.heads to x and concat the results \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39989",
   "metadata": {},
   "source": [
    "Questions :\n",
    "> * Copier le fichier `gpt_single_head.py` en `gpt_multi_head.py`.\n",
    "> * Ajouter le module MultiHeadAttention dans `gpt_multi_head.py`.\n",
    "> * En tête de fichier, ajouter un paramètre  `n_head = 4`.\n",
    "> * Dans le module BigramLanguageModel, remplacer le module Head par un module MultiHeadAttention avec les paramètres `num_heads = n_head` et `head_size = n_embd // n_head` pour garder le même nombre de paramètres.\n",
    "> Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
    "\n",
    "0.009893 M parameters\n",
    "step 4999: train loss 2.1570, val loss 2.1802"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d3f27",
   "metadata": {},
   "source": [
    "## Ajout d'une couche de calcul FeedForward\n",
    "\n",
    "\n",
    "Après les couches d'attention qui collectent l'information dans la séquence, une couche de calcul est ajoutée pour combiner toutes les informations de la séquence. Cette couche est un simple Multi-Layer-Perceptron avec une couche cachée et une non linéarité de type [RELU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n",
    "![multi feedfoward](images/multi_ff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple MLP with RELU \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5ef7",
   "metadata": {},
   "source": [
    "Questions :\n",
    "> * Ajouter le module `FeedForward` dans votre fichier `gpt_multi_head.py`.\n",
    "> * Ajouter cette couche `FeedForward` après la *multi-head attention*.\n",
    "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942f7be",
   "metadata": {},
   "source": [
    "0.010949 M parameters\n",
    "step 4999: train loss 2.1290, val loss 2.1216"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16dfb3",
   "metadata": {},
   "source": [
    "## Empiler les blocs\n",
    "\n",
    "Le réseau construit jusqu'à présent n'est en fait qu'un bloc du réseau final. Il est maintenant possible d'empiler les blocs de *multi-head attention* pour créer un réseau profond. \n",
    "\n",
    "![multi feedfoward](images/multi_bloc.png)\n",
    "\n",
    "\n",
    "Le code suivant crée un bloc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" A single bloc of multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144fff7",
   "metadata": {},
   "source": [
    "Questions :\n",
    "> * Ajouter le module `Block` dans `gpt_multi_head.py`.\n",
    "> * Modifier le code de `BigramLanguageModel` pour ajouter 3 `Block(n_embd, n_head=4)` avec un container [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) à la place de `MultiHeadAttention`et `FeedForward`.\n",
    "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
    "\n",
    "0.009989 M parameters\n",
    "step 0: train loss 4.6645, val loss 4.6758\n",
    "step 4999: train loss 2.1482, val loss 2.1700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d77a",
   "metadata": {},
   "source": [
    "## Amélioration de l'entraînement\n",
    "\n",
    "Si on veut continuer à augmenter la taille du réseau, il est nécessaire d'utiliser des couches permettant d'améliorer l'entraînement et ses capacités de généralisation (réduire le sur-apprentissage). Ces couches sont :\n",
    "- *skip connections* ou *residual connections*\n",
    "- les couches de normalisation\n",
    "- le dropout.\n",
    "\n",
    "\n",
    "![multi feedfoward](images/multi_skip_norm.png)\n",
    "\n",
    "\n",
    "Questions :\n",
    "> * Dans le module Block, ajouter une skip connection en ajoutant l'input dans chaque connexion :\n",
    "```\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "```\n",
    "> * Dans le module Block, ajouter 2 couches de [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) de taille `n_embd` avant la couche de `Multi-Head attention` et avant la `FeedForward`.\n",
    "> * Après la série de 3 blocs, ajouter une couche de LayerNorm de taille `n_embd`.\n",
    "> * Définir une variable `dropout = 0.2` en début de fichier et ajouter une couche de [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) :\n",
    ">    * Après la couche RELU dans FeedForward\n",
    ">    * Après la couche de MultiHead dans `MultiHeadAttention`\n",
    ">    * Après la softmax dans la single head attention `Head`.\n",
    "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
    "\n",
    "0.019941 M parameters\n",
    "step 0: train loss 5.1058, val loss 5.1539\n",
    "step 4999: train loss 1.9860, val loss 1.9973\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733a17",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Les principaux éléments de GPT2 sont en place, il faut maintenant faire passer le modèle à l'échelle et l'entraîner sur une base de données beaucoup plus grande. Pour comparaison, les paramètres de [GPT2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html) sont : \n",
    "\n",
    "* `vocab_size = 50257` : GPT2 modélise des tokens (subwords) alors que nous modélisons des caractères. Pour nous, `vocab_size = 100`.\n",
    "* `n_positions = 1024` : la taille maximale du contexte. Pour nous, c'est `block_size = 8`.\n",
    "* `n_embd = 768`:  la dimension des embeddings. Pour nous c'est `n_embd = 32`.\n",
    "* `n_layer = 12`: le nombre de block. Pour nous c'est 3.\n",
    "* `n_head = 12`: le nombre de multi-head attention. Pour nous c'est 4.\n",
    "\n",
    "Au total, GPT2 est composé de 1,500 millions de paramètres et a été entrainé sur 8M de pages web, soit 40 Gb de texte.\n",
    "\n",
    "\n",
    "```\n",
    "10.816613 M parameters\n",
    "step 0: train loss 4.7847, val loss 4.7701\n",
    "step 4999: train loss 0.2683, val loss 2.1161\n",
    "time: 31m47.910s   \n",
    "    \n",
    "Le pêcheur où l'homme en peu de Carevante\n",
    "Sa conter des chosses qu'en ses yoitn!\n",
    "\n",
    "Ils sont là-hauts parler à leurs ténèbres\n",
    "A ceux qu'on rêve aux oiseaux des cheveux,\n",
    "Et celus qu'on tourna jamais sous le front;\n",
    "Ils se disent tu mêle aux univers.\n",
    "J'ai vu Jean vu France, potte; petits contempler,\n",
    "Et petié calme au milibre et versait,\n",
    "M'éblouissant, emportant, écoute, ingorancessible,\n",
    "On meurt s'efferayait.....--Pas cont âme parle en Apparia!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
